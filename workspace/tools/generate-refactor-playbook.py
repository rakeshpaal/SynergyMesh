#!/usr/bin/env python3
"""
AI Refactor Playbook Generator
AI é‡æ§‹ Playbook ç”Ÿæˆå™¨

Generates actionable refactor playbooks for each directory cluster based on
language governance data, security scans, and hotspot analysis.

å°ˆé–€ç‚ºå¤§å‹é›²åŸç”Ÿå¹³å°è¨­è¨ˆé‡æ§‹è¨ˆç•«çš„ã€ŒAI Refactor Playbook Generatorã€
"""

import argparse
import hashlib
import json
import sys
from datetime import datetime, timedelta
from pathlib import Path

import yaml


class RefactorPlaybookGenerator:
    """Generate refactor playbooks for directory clusters"""
    
    # Configuration constants
    MAX_HOTSPOTS_DISPLAY = 10
    MAX_SEMGREP_DISPLAY = 10
    AI_SUGGESTIONS_EXCERPT_LENGTH = 500
    
    # System prompt for LLM
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹å°ˆé–€ç‚ºå¤§å‹é›²åŸç”Ÿå¹³å°è¨­è¨ˆé‡æ§‹è¨ˆç•«çš„ã€ŒAI Refactor Playbook Generatorã€ã€‚

ä½ åŒæ™‚æ‰®æ¼”ä¸‰å€‹è§’è‰²ï¼š
- é¦–å¸­è»Ÿé«”æ¶æ§‹å¸«ï¼ˆè² è²¬æ•´é«”æ¶æ§‹èˆ‡æ¨¡çµ„é‚Šç•Œï¼‰
- èªè¨€æ²»ç†è² è²¬äººï¼ˆLanguage Governance Ownerï¼‰
- å®‰å…¨èˆ‡å“è³ªé¡§å•ï¼ˆæ•´åˆ Semgrep / éœæ…‹åˆ†æçµæœï¼‰

å°ˆæ¡ˆèƒŒæ™¯ï¼š
- å°ˆæ¡ˆåç¨±ï¼šUnmanned Island System
- èªè¨€ç­–ç•¥ï¼š
  - TypeScript + Python ç‚ºæœ€é«˜éšèªè¨€ï¼ˆHigh-Level / æ¥­å‹™é‚è¼¯å±¤ï¼‰
  - Go / C++ / ROS 2 ç”¨æ–¼åº•å±¤é«˜æ€§èƒ½èˆ‡è‡ªä¸»ç³»çµ±
- å·²å­˜åœ¨çš„æ²»ç†ç³»çµ±ï¼š
  - Language Governance Pipeline
  - Hotspot Heatmap
  - Cluster Heatmap
  - Migration Flow Model
  - AI Auto-Fix Bot

ä½ çš„å·¥ä½œç›®æ¨™ï¼š
- å°ã€Œæ¯ä¸€å€‹ç›®éŒ„ç¾¤é›†ï¼ˆclusterï¼‰ã€ç”¢ç”Ÿä¸€ä»½å¯åŸ·è¡Œçš„ **Refactor Playbook**ï¼ˆMarkdownï¼‰
- Playbook è¦èƒ½ç›´æ¥çµ¦å·¥ç¨‹å¸« / æ¶æ§‹å¸« / è‡ªå‹•åŒ– Bot ä½¿ç”¨
- æ‰€æœ‰å»ºè­°å¿…é ˆï¼š
  - ç¬¦åˆæ—¢æœ‰èªè¨€æ”¿ç­–èˆ‡æ¶æ§‹éª¨æ¶
  - å…·é«”ã€å¯è½åœ°ã€æœ‰æ˜ç¢ºå„ªå…ˆé †åºï¼ˆP0 / P1 / P2ï¼‰
  - æ˜ç¢ºå€åˆ†ã€Œé©åˆäº¤çµ¦ Auto-Fix Botã€èˆ‡ã€Œå¿…é ˆäººå·¥å¯©æŸ¥ã€çš„ç¯„åœ
"""

    USER_PROMPT_TEMPLATE = """æˆ‘æœƒæä¾›ä½ ä¸€å€‹ç›®éŒ„ç¾¤é›†ï¼ˆclusterï¼‰çš„æ‰€æœ‰æ²»ç†æ•¸æ“šï¼Œè«‹ä½ ç”¢å‡ºè©² cluster å°ˆå±¬çš„ã€ŒRefactor Playbookã€ã€‚

è«‹æ ¹æ“šä»¥ä¸‹è¼¸å…¥é€²è¡Œåˆ†æèˆ‡è¦åŠƒï¼š

---

[1] Cluster åŸºæœ¬è³‡è¨Š
- Cluster åç¨±ï¼š{cluster_name}
- Cluster Scoreï¼š{cluster_score}

---

[2] èªè¨€æ²»ç†é•è¦ï¼ˆå¾ governance/language-governance-report.md æ“·å–ï¼‰

ç›®å‰å±¬æ–¼è©² cluster çš„é•è¦æª”æ¡ˆèˆ‡åŸå› å¦‚ä¸‹ï¼š

{violations_text}

---

[3] Hotspot æª”æ¡ˆï¼ˆapps/web/public/data/hotspot.jsonï¼‰

è©² cluster ä¸‹é•è¦èˆ‡é¢¨éšªåˆ†æ•¸æœ€é«˜çš„æª”æ¡ˆå¦‚ä¸‹ï¼š

{hotspot_text}

---

[4] Semgrep å®‰å…¨å•é¡Œï¼ˆgovernance/semgrep-report.jsonï¼‰

è©² cluster ç›¸é—œçš„ Semgrep çµæœå¦‚ä¸‹ï¼š

{semgrep_text}

---

[5] Migration Flow Modelï¼ˆapps/web/public/data/migration-flow.jsonï¼‰

åœ¨èªè¨€é·ç§»æµæ¨¡å‹ä¸­ï¼Œé€™å€‹ cluster æ‰®æ¼”çš„è§’è‰²ï¼š

- Incoming Flowsï¼ˆå…¶ä»– cluster â†’ æœ¬ clusterï¼‰ï¼š
{incoming_text}

- Outgoing Flowsï¼ˆæœ¬ cluster â†’ å…¶ä»– cluster æˆ– removedï¼‰ï¼š
{outgoing_text}

---

[6] å…¨å±€ AI å»ºè­°ï¼ˆgovernance/ai-refactor-suggestions.mdï¼‰

ä»¥ä¸‹æ˜¯å°æ•´å€‹å„²å­˜åº«çš„å…¨å±€ AI å»ºè­°æ‘˜è¦ï¼Œä¾›ä½ åœ¨åˆ¶å®šæœ¬ cluster è¨ˆç•«æ™‚åƒè€ƒï¼š

{global_ai_suggestions_excerpt}

---

è«‹ä¾ç…§ä»¥ä¸‹ã€Œå›ºå®šè¼¸å‡ºæ ¼å¼ã€ç”¢ç”Ÿ **Markdown** çµæœï¼ˆéå¸¸é‡è¦ï¼‰ï¼š

## 1. Cluster æ¦‚è¦½
- é€™å€‹ cluster åœ¨æ•´å€‹ Unmanned Island System ä¸­çš„è§’è‰²ï¼ˆè«‹æ ¹æ“šè·¯å¾‘èˆ‡æª”æ¡ˆé¡å‹åˆç†æ¨æ–·ï¼‰
- ç›®å‰ä¸»è¦èªè¨€çµ„æˆèˆ‡å¥åº·ç‹€æ…‹ï¼ˆé«˜å±¤ vs ä½å±¤èªè¨€æ˜¯å¦åˆç†ï¼‰

## 2. å•é¡Œç›¤é»
- èªè¨€æ²»ç†å•é¡Œåˆ†é¡å½™ç¸½ï¼ˆä¾‹å¦‚ï¼šforbidden language / layer violation / low-level leakï¼‰
- Hotspot æª”æ¡ˆåˆ—è¡¨ï¼ˆä¾ score é«˜â†’ä½ï¼Œé™„ä¸Šç°¡çŸ­é¢¨éšªèªªæ˜ï¼‰
- Semgrep å®‰å…¨å•é¡Œæ‘˜è¦ï¼ˆä¾ severity é«˜â†’ä½ï¼‰
- Migration Flow è§€å¯Ÿï¼ˆæœ¬ cluster æ˜¯èªè¨€é•è¦çš„ã€Œæºé ­ã€é‚„æ˜¯ã€Œåƒåœ¾å ´ã€ï¼Ÿï¼‰

## 3. èªè¨€èˆ‡çµæ§‹é‡æ§‹ç­–ç•¥
- èªè¨€å±¤ç´šç­–ç•¥ï¼š
  - æ‡‰ä¿ç•™å“ªäº›èªè¨€ï¼Ÿï¼ˆä¾‹å¦‚ï¼šåªå…è¨± TS + Pythonï¼‰
  - æ‡‰é·å‡º/åˆªé™¤å“ªäº›èªè¨€ï¼Ÿï¼ˆä¾‹å¦‚ï¼šPHP / Lua / é›œæ•£ C++ï¼‰
- ç›®éŒ„çµæ§‹ç­–ç•¥ï¼š
  - æ˜¯å¦æ‡‰æ‹†åˆ†å­æ¨¡çµ„ï¼Ÿ
  - æ˜¯å¦æœ‰æ‡‰ä¸Šç§»/ä¸‹æ²‰åˆ° core/ã€services/ã€autonomous/ çš„éƒ¨åˆ†ï¼Ÿ
- èªè¨€é·ç§»å»ºè­°ï¼š
  - é‡å°å¸¸è¦‹ patternï¼Œçµ¦å‡ºå…·é«”è·¯å¾‘å»ºè­°

## 4. åˆ†ç´šé‡æ§‹è¨ˆç•«ï¼ˆP0 / P1 / P2ï¼‰

è«‹ç”¨æ¢åˆ—åˆ—å‡ºå…·é«”ã€Œæª”æ¡ˆå±¤ç´šã€èˆ‡ã€Œå‹•ä½œå±¤ç´šã€å»ºè­°ï¼Œä¾‹å¦‚ï¼š

- P0ï¼ˆ24â€“48 å°æ™‚å…§å¿…é ˆè™•ç†ï¼‰
  - æª”æ¡ˆï¼š...
  - å‹•ä½œï¼šåˆªé™¤ / ç§»å‹•åˆ° X ç›®éŒ„ / æ”¹å¯«æˆ TypeScript / åŠ ä¸Šå®‰å…¨æª¢æŸ¥ ...
- P1ï¼ˆä¸€é€±å…§ï¼‰
  - æª”æ¡ˆï¼š...
  - å‹•ä½œï¼šé‡æ§‹æ¨¡çµ„é‚Šç•Œ / åˆ†é›¢é«˜å±¤ API èˆ‡ä½å±¤æ§åˆ¶ / èª¿æ•´ç›®éŒ„çµæ§‹ ...
- P2ï¼ˆæŒçºŒé‡æ§‹ï¼‰
  - æª”æ¡ˆ / å­ç›®éŒ„ï¼š...
  - å‹•ä½œï¼šæŠ€è¡“å‚µæ¸…ç† / æ¸›å°‘èªè¨€æ··ç”¨ / æ”¹å–„å¯æ¸¬è©¦æ€§ ...

## 5. é©åˆäº¤çµ¦ Auto-Fix Bot çš„é …ç›®

- åˆ—å‡ºå“ªäº›æª”æ¡ˆ / é¡å‹çš„å•é¡Œã€Œå¯ä»¥ç”± Auto-Fix Bot ç›´æ¥ç”¢ç”Ÿ patchã€
- åˆ—å‡ºå“ªäº›é …ç›®ã€Œå¿…é ˆäººå·¥ code reviewã€

## 6. é©—æ”¶æ¢ä»¶èˆ‡æˆåŠŸæŒ‡æ¨™

- å°æœ¬ cluster çš„èªè¨€æ²»ç† CI æœŸæœ›å€¼
- å° Hotspot / Cluster Score çš„é æœŸæ”¹å–„
- å°æœªä¾†é–‹ç™¼æµç¨‹çš„æ”¹å–„æ–¹å‘

## 7. æª”æ¡ˆèˆ‡ç›®éŒ„çµæ§‹ï¼ˆäº¤ä»˜è¦–åœ–ï¼‰

ã€å¼·åˆ¶äº¤ä»˜è¦æ±‚ã€‘

å¿…é ˆåŒ…å«ä»¥ä¸‹å…§å®¹ï¼š

1. **å—å½±éŸ¿ç›®éŒ„æ¸…å–®**
   - æ˜ç¢ºåˆ—å‡ºæœ¬æ¬¡é‡æ§‹æ¶‰åŠçš„æ‰€æœ‰ç›®éŒ„è·¯å¾‘
   - ä¾‹å¦‚ï¼šcore/, services/gateway/, automation/autonomous/

2. **å®Œæ•´äº¤ä»˜æª”çµæ§‹åœ–**
   - ä½¿ç”¨ tree é¢¨æ ¼æ–‡å­—ï¼ˆç¸®æ’åˆ—å‡ºç›®éŒ„èˆ‡æª”æ¡ˆï¼‰
   - æˆ–ä½¿ç”¨ Mermaid åœ–è¡¨
   - åªéœ€æ¶µè“‹æœ¬æ¬¡è®Šæ›´æ¶‰åŠçš„ç›®éŒ„èˆ‡æª”æ¡ˆï¼Œä¸éœ€è¦å…¨ repo
   - å¿…é ˆæ¸…æ¥šå¯è®€ï¼Œå±•ç¤ºç›®éŒ„å±¤ç´šé—œä¿‚

3. **ä¸»è¦æª”æ¡ˆèˆ‡ç›®éŒ„çš„è¨»è§£èªªæ˜**
   - é‡å°æ¯å€‹é‡è¦ç›®éŒ„/æª”æ¡ˆï¼Œç°¡çŸ­æè¿°ç”¨é€”èˆ‡è§’è‰²
   - æ ¼å¼ï¼š`path/to/file.ts` â€” ä¸€è¡Œèªªæ˜è©²æª”æ¡ˆçš„è·è²¬
   - æ ¼å¼ï¼š`dir/` â€” èªªæ˜è©²ç›®éŒ„å­˜æ”¾ä»€éº¼é¡å‹çš„å…§å®¹

ç¯„ä¾‹æ ¼å¼ï¼š

### å—å½±éŸ¿ç›®éŒ„
- services/gateway/
- core/machinenativenops.contracts/
- automation/intelligent/

### çµæ§‹ç¤ºæ„ï¼ˆåƒ…æ¶µè“‹è®Šæ›´å€åŸŸï¼‰

```
services/gateway/
â”œâ”€â”€ router.cpp          # èˆŠç‰ˆ C++ gateway å…¥å£ï¼ˆå»ºè­°é·ç§»ï¼‰
â”œâ”€â”€ router.ts           # æ–°ç‰ˆ TypeScript gatewayï¼ˆç›®æ¨™ï¼‰
â”œâ”€â”€ middleware/
â”‚   â”œâ”€â”€ auth.ts         # èªè­‰ä¸­ä»‹å±¤
â”‚   â””â”€â”€ logging.ts      # æ—¥èªŒä¸­ä»‹å±¤
â””â”€â”€ types/
    â””â”€â”€ request.ts      # è«‹æ±‚é¡å‹å®šç¾©
```

### æª”æ¡ˆèªªæ˜
- `services/gateway/router.cpp` â€” ç¾æœ‰ C++ å¯¦ä½œçš„ gateway è·¯ç”±å™¨ï¼Œèˆ‡èˆŠç‰ˆ core/ æ­é…
- `services/gateway/router.ts` â€” å»ºè­°çš„æ–° TypeScript å¯¦ä½œï¼Œä¾›å‰ç«¯èˆ‡å¾®æœå‹™ä½¿ç”¨
- `services/gateway/middleware/` â€” ä¸­ä»‹å±¤ç›®éŒ„ï¼ŒåŒ…å«èªè­‰ã€æ—¥èªŒç­‰æ©«åˆ‡é—œæ³¨é»

è«‹å‹™å¿…ä¾ç…§ä¸Šè¿°æ®µè½é †åºèˆ‡æ¨™é¡Œè¼¸å‡º Markdownï¼Œä¸”æ‰€æœ‰å»ºè­°éƒ½è¦ä»¥ **å¯åŸ·è¡Œè¡Œå‹•** ç‚ºå°å‘ï¼Œè€Œä¸æ˜¯æŠ½è±¡åŸå‰‡ã€‚
"""
    
    def __init__(self, repo_root: str):
        self.repo_root = Path(repo_root)
        self.clusters = {}
        self.violations = []
        self.hotspots = []
        self.semgrep_results = []
        self.migration_flows = {}
        self.global_suggestions = ""
        
        # Cache settings
        self.cache_enabled, self.cache_ttl_hours = self._load_cache_settings()
        self.cache_dir = self.repo_root / ".cache" / "refactor"
        if self.cache_enabled:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        
    def load_governance_data(self):
        """Load all governance data files"""
        print("ğŸ“‚ Loading governance data...")
        
        # Load language governance report
        gov_report_path = self.repo_root / "governance" / "language-governance-report.md"
        if gov_report_path.exists():
            self._parse_governance_report(gov_report_path)
        else:
            print(f"âš ï¸ Governance report not found: {gov_report_path}")
            
        # Load hotspot data
        hotspot_path = self.repo_root / "apps" / "web" / "public" / "data" / "hotspot.json"
        if hotspot_path.exists():
            with open(hotspot_path) as f:
                self.hotspots = json.load(f)
        else:
            print(f"âš ï¸ Hotspot data not found: {hotspot_path}")
            
        # Load semgrep report
        semgrep_path = self.repo_root / "governance" / "semgrep-report.json"
        if semgrep_path.exists():
            with open(semgrep_path) as f:
                self.semgrep_results = json.load(f)
        else:
            print(f"âš ï¸ Semgrep report not found: {semgrep_path}")
            
        # Load migration flow
        migration_path = self.repo_root / "apps" / "web" / "public" / "data" / "migration-flow.json"
        if migration_path.exists():
            with open(migration_path) as f:
                self.migration_flows = json.load(f)
        else:
            print(f"âš ï¸ Migration flow not found: {migration_path}")
            
        # Load cluster heatmap
        cluster_path = self.repo_root / "apps" / "web" / "public" / "data" / "cluster-heatmap.json"
        if cluster_path.exists():
            with open(cluster_path) as f:
                self.clusters = json.load(f)
        else:
            print(f"âš ï¸ Cluster heatmap not found: {cluster_path}")
            
        # Load global AI suggestions
        ai_suggestions_path = self.repo_root / "governance" / "ai-refactor-suggestions.md"
        if ai_suggestions_path.exists():
            with open(ai_suggestions_path) as f:
                self.global_suggestions = f.read()
        else:
            print(f"âš ï¸ AI suggestions not found: {ai_suggestions_path}")
    
    def _load_cache_settings(self) -> tuple:
        """Load cache settings from sync-refactor-config.yaml
        
        Returns:
            tuple: (enabled: bool, ttl_hours: int)
        """
        config_path = self.repo_root / "config" / "sync-refactor-config.yaml"
        default_enabled = True
        default_ttl = 24
        
        if config_path.exists():
            try:
                with open(config_path) as f:
                    config = yaml.safe_load(f)
                cache_config = config.get('refactor', {}).get('cache', {})
                enabled = cache_config.get('enabled', default_enabled)
                ttl = cache_config.get('ttl_hours', default_ttl)
                return (enabled, ttl)
            except Exception as e:
                print(f"âš ï¸ Could not load cache settings: {e}")
                return (default_enabled, default_ttl)
        return (default_enabled, default_ttl)
    
    def _get_data_hash(self) -> str:
        """Generate hash of all input data sources"""
        hash_content = []
        
        # Hash all data source files
        data_files = [
            self.repo_root / "governance" / "language-governance-report.md",
            self.repo_root / "governance" / "semgrep-report.json",
            self.repo_root / "apps" / "web" / "public" / "data" / "hotspot.json",
            self.repo_root / "apps" / "web" / "public" / "data" / "cluster-heatmap.json",
            self.repo_root / "apps" / "web" / "public" / "data" / "migration-flow.json",
            self.repo_root / "governance" / "ai-refactor-suggestions.md",
        ]
        
        for file_path in data_files:
            if file_path.exists():
                hash_content.append(f"{file_path.name}:{file_path.stat().st_mtime}")
        
        # Create hash using SHA-256 (more secure than MD5)
        hash_str = "|".join(hash_content)
        return hashlib.sha256(hash_str.encode()).hexdigest()
    
    def _is_cache_valid(self, cluster_name: str) -> bool:
        """Check if cached playbook is still valid"""
        if not self.cache_enabled:
            return False
        
        cache_file = self.cache_dir / f"{cluster_name.replace('/', '_')}.cache"
        if not cache_file.exists():
            return False
        
        try:
            with open(cache_file) as f:
                cache_data = json.load(f)
            
            # Check if data hash matches
            current_hash = self._get_data_hash()
            if cache_data.get('data_hash') != current_hash:
                return False
            
            # Check TTL using configured value
            cache_time = datetime.fromisoformat(cache_data.get('timestamp', '2000-01-01'))
            if datetime.now() - cache_time > timedelta(hours=self.cache_ttl_hours):
                return False
            
            return True
        except Exception as e:
            print(f"âš ï¸ Cache validation error for {cluster_name}: {e}")
            return False
    
    def _load_cached_playbook(self, cluster_name: str) -> str | None:
        """Load cached playbook if valid"""
        if not self._is_cache_valid(cluster_name):
            return None
        
        cache_file = self.cache_dir / f"{cluster_name.replace('/', '_')}.cache"
        try:
            with open(cache_file) as f:
                cache_data = json.load(f)
            return cache_data.get('playbook')
        except Exception as e:
            print(f"âš ï¸ Cache load error for {cluster_name}: {e}")
            return None
    
    def _save_to_cache(self, cluster_name: str, playbook: str):
        """Save playbook to cache"""
        if not self.cache_enabled:
            return
        
        cache_file = self.cache_dir / f"{cluster_name.replace('/', '_')}.cache"
        try:
            cache_data = {
                'cluster_name': cluster_name,
                'timestamp': datetime.now().isoformat(),
                'data_hash': self._get_data_hash(),
                'playbook': playbook
            }
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ Cache save error for {cluster_name}: {e}")
            
    def _parse_governance_report(self, report_path: Path):
        """Parse language governance report markdown
        
        Expected format:
        - **path/to/file.ext** â€” Reason for violation
        - **path/to/file.ext** - Reason for violation
        """
        import re
        
        with open(report_path) as f:
            content = f.read()
            
        # Use regex for more robust parsing
        # Matches: - **file/path** â€” reason or - **file/path** - reason
        violation_pattern = r'-\s+\*\*([^*]+)\*\*\s+[â€”-]\s+(.+)'
        
        for match in re.finditer(violation_pattern, content):
            file_path = match.group(1).strip()
            reason = match.group(2).strip()
            self.violations.append({
                'file': file_path,
                'reason': reason
            })
                        
    def _get_cluster_violations(self, cluster_name: str) -> list[dict]:
        """Get violations for a specific cluster"""
        cluster_violations = []
        for v in self.violations:
            if v['file'].startswith(cluster_name):
                cluster_violations.append(v)
        return cluster_violations
        
    def _get_cluster_hotspots(self, cluster_name: str) -> list[dict]:
        """Get hotspot files for a specific cluster"""
        cluster_hotspots = []
        if isinstance(self.hotspots, list):
            for h in self.hotspots:
                if isinstance(h, dict) and h.get('file', '').startswith(cluster_name):
                    cluster_hotspots.append(h)
        return cluster_hotspots
        
    def _get_cluster_semgrep(self, cluster_name: str) -> list[dict]:
        """Get semgrep issues for a specific cluster
        
        Semgrep data can be in two formats:
        1. Dict with 'results' key: {"results": [...], "summary": {...}}
        2. List of results directly: [...]
        """
        cluster_semgrep = []
        if isinstance(self.semgrep_results, dict):
            results = self.semgrep_results.get('results', [])
        elif isinstance(self.semgrep_results, list):
            results = self.semgrep_results
        else:
            results = []
            
        for issue in results:
            if isinstance(issue, dict):
                file_path = issue.get('path', '')
                if file_path.startswith(cluster_name):
                    cluster_semgrep.append(issue)
        return cluster_semgrep
        
    def _get_migration_flows(self, cluster_name: str) -> tuple:
        """Get migration flows for a cluster"""
        incoming = []
        outgoing = []
        
        if isinstance(self.migration_flows, dict):
            flows = self.migration_flows.get('flows', [])
            for flow in flows:
                if isinstance(flow, dict):
                    source = flow.get('source', '')
                    target = flow.get('target', '')
                    
                    if target.startswith(cluster_name):
                        incoming.append(flow)
                    if source.startswith(cluster_name):
                        outgoing.append(flow)
                        
        return incoming, outgoing
        
    def generate_cluster_prompt(self, cluster_name: str, cluster_score: float = 0) -> str:
        """Generate LLM prompt for a specific cluster"""
        
        # Get cluster-specific data
        violations = self._get_cluster_violations(cluster_name)
        hotspots = self._get_cluster_hotspots(cluster_name)
        semgrep = self._get_cluster_semgrep(cluster_name)
        incoming, outgoing = self._get_migration_flows(cluster_name)
        
        # Format violations
        violations_text = "\n".join([
            f"- {v['file']}: {v['reason']}" for v in violations
        ]) if violations else "ç„¡é•è¦"
        
        # Format hotspots (limited to MAX_HOTSPOTS_DISPLAY)
        hotspot_text = "\n".join([
            f"- {h.get('file', 'unknown')} (score={h.get('score', 0)})"
            for h in sorted(hotspots, key=lambda x: x.get('score', 0), reverse=True)[:self.MAX_HOTSPOTS_DISPLAY]
        ]) if hotspots else "ç„¡ hotspot"
        
        # Format semgrep (limited to MAX_SEMGREP_DISPLAY)
        semgrep_text = "\n".join([
            f"- {s.get('path', 'unknown')} [{s.get('severity', 'UNKNOWN')}] {s.get('rule_id', 'unknown')}: {s.get('message', 'no message')}"
            for s in sorted(semgrep, key=lambda x: x.get('severity', 'LOW'), reverse=True)[:self.MAX_SEMGREP_DISPLAY]
        ]) if semgrep else "ç„¡å®‰å…¨å•é¡Œ"
        
        # Format flows
        incoming_text = "\n".join([
            f"- {f.get('source', 'unknown')} â†’ {cluster_name} (count={f.get('count', 0)}, type={f.get('type', 'unknown')})"
            for f in incoming[:5]
        ]) if incoming else "ç„¡ incoming flows"
        
        outgoing_text = "\n".join([
            f"- {cluster_name} â†’ {f.get('target', 'unknown')} (count={f.get('count', 0)}, type={f.get('type', 'unknown')})"
            for f in outgoing[:5]
        ]) if outgoing else "ç„¡ outgoing flows"
        
        # Get AI suggestions excerpt (configurable length)
        excerpt_len = self.AI_SUGGESTIONS_EXCERPT_LENGTH
        global_ai_suggestions_excerpt = self.global_suggestions[:excerpt_len] + "..." if len(self.global_suggestions) > excerpt_len else self.global_suggestions
        if not global_ai_suggestions_excerpt:
            global_ai_suggestions_excerpt = "ç„¡å…¨å±€å»ºè­°"
            
        # Generate prompt
        prompt = self.USER_PROMPT_TEMPLATE.format(
            cluster_name=cluster_name,
            cluster_score=cluster_score,
            violations_text=violations_text,
            hotspot_text=hotspot_text,
            semgrep_text=semgrep_text,
            incoming_text=incoming_text,
            outgoing_text=outgoing_text,
            global_ai_suggestions_excerpt=global_ai_suggestions_excerpt
        )
        
        return prompt
        
    def generate_playbook_stub(self, cluster_name: str, cluster_score: float = 0) -> str:
        """Generate a stub playbook (without LLM)"""
        
        violations = self._get_cluster_violations(cluster_name)
        hotspots = self._get_cluster_hotspots(cluster_name)
        semgrep = self._get_cluster_semgrep(cluster_name)
        
        playbook = f"""# Refactor Playbook: {cluster_name}

**Generated:** {datetime.now().isoformat()}  
**Cluster Score:** {cluster_score}  
**Status:** Draft (LLM generation required for complete playbook)

---

## 1. Cluster æ¦‚è¦½

**Cluster Path:** `{cluster_name}`  
**Current Status:** éœ€è¦é‡æ§‹èˆ‡èªè¨€æ²»ç†æ”¹é€²

é€™å€‹ cluster åœ¨ Unmanned Island System ä¸­çš„è§’è‰²ï¼š
- è·¯å¾‘ä½ç½®ï¼š{cluster_name}
- é•è¦æ•¸é‡ï¼š{len(violations)}
- Hotspot æª”æ¡ˆï¼š{len(hotspots)}
- å®‰å…¨å•é¡Œï¼š{len(semgrep)}

---

## 2. å•é¡Œç›¤é»

### èªè¨€æ²»ç†é•è¦ ({len(violations)})

"""
        
        if violations:
            for v in violations[:10]:
                playbook += f"- **{v['file']}** â€” {v['reason']}\n"
            if len(violations) > 10:
                playbook += f"\n... å’Œ {len(violations) - 10} å€‹å…¶ä»–é•è¦\n"
        else:
            playbook += "âœ… ç„¡èªè¨€æ²»ç†é•è¦\n"
            
        playbook += f"\n### Hotspot æª”æ¡ˆ ({len(hotspots)})\n\n"
        
        if hotspots:
            for h in sorted(hotspots, key=lambda x: x.get('score', 0), reverse=True)[:5]:
                playbook += f"- **{h.get('file', 'unknown')}** (score: {h.get('score', 0)})\n"
        else:
            playbook += "âœ… ç„¡ hotspot æª”æ¡ˆ\n"
            
        playbook += f"\n### Semgrep å®‰å…¨å•é¡Œ ({len(semgrep)})\n\n"
        
        if semgrep:
            for s in sorted(semgrep, key=lambda x: x.get('severity', 'LOW'), reverse=True)[:5]:
                playbook += f"- [{s.get('severity', 'UNKNOWN')}] **{s.get('path', 'unknown')}**: {s.get('message', 'no message')}\n"
        else:
            playbook += "âœ… ç„¡å®‰å…¨å•é¡Œ\n"
            
        playbook += """

---

## 3. èªè¨€èˆ‡çµæ§‹é‡æ§‹ç­–ç•¥

**æ³¨æ„ï¼š** æ­¤éƒ¨åˆ†éœ€è¦ä½¿ç”¨ LLM ç”Ÿæˆå®Œæ•´å»ºè­°ã€‚

é æœŸå…§å®¹ï¼š
- èªè¨€å±¤ç´šç­–ç•¥ï¼ˆä¿ç•™/é·å‡ºèªè¨€ï¼‰
- ç›®éŒ„çµæ§‹å„ªåŒ–å»ºè­°
- èªè¨€é·ç§»è·¯å¾‘

---

## 4. åˆ†ç´šé‡æ§‹è¨ˆç•«ï¼ˆP0 / P1 / P2ï¼‰

**æ³¨æ„ï¼š** æ­¤éƒ¨åˆ†éœ€è¦ä½¿ç”¨ LLM ç”Ÿæˆå…·é«”è¡Œå‹•è¨ˆç•«ã€‚

### P0ï¼ˆ24â€“48 å°æ™‚å…§å¿…é ˆè™•ç†ï¼‰
- å¾… LLM ç”Ÿæˆ

### P1ï¼ˆä¸€é€±å…§ï¼‰
- å¾… LLM ç”Ÿæˆ

### P2ï¼ˆæŒçºŒé‡æ§‹ï¼‰
- å¾… LLM ç”Ÿæˆ

---

## 5. é©åˆäº¤çµ¦ Auto-Fix Bot çš„é …ç›®

**å¯è‡ªå‹•ä¿®å¾©ï¼š**
- å¾… LLM åˆ†æ

**éœ€äººå·¥å¯©æŸ¥ï¼š**
- å¾… LLM åˆ†æ

---

## 6. é©—æ”¶æ¢ä»¶èˆ‡æˆåŠŸæŒ‡æ¨™

**èªè¨€æ²»ç†ç›®æ¨™ï¼š**
- é•è¦æ•¸ < 5
- å®‰å…¨å•é¡Œ HIGH severity = 0
- Cluster score < 20

**æ”¹å–„æ–¹å‘ï¼š**
- å¾… LLM ç”Ÿæˆå…·é«”å»ºè­°

---

## 7. æª”æ¡ˆèˆ‡ç›®éŒ„çµæ§‹ï¼ˆäº¤ä»˜è¦–åœ–ï¼‰

### å—å½±éŸ¿ç›®éŒ„

"""
        
        # Add affected directories
        playbook += f"- {cluster_name}\n\n"
        
        # Add directory tree structure
        playbook += "### çµæ§‹ç¤ºæ„ï¼ˆè®Šæ›´ç¯„åœï¼‰\n\n```\n"
        playbook += self._generate_directory_tree(cluster_name)
        playbook += "\n```\n\n"
        
        # Add file annotations
        playbook += "### æª”æ¡ˆèªªæ˜\n\n"
        annotations = self._generate_file_annotations(cluster_name)
        playbook += "\n".join(annotations)
        playbook += "\n\n---\n\n"
        
        playbook += """## å¦‚ä½•ä½¿ç”¨æœ¬ Playbook

1. **ç«‹å³åŸ·è¡Œ P0 é …ç›®**ï¼šè™•ç†é«˜å„ªå…ˆç´šå•é¡Œ
2. **è¦åŠƒ P1 é‡æ§‹**ï¼šå®‰æ’ä¸€é€±å…§åŸ·è¡Œ
3. **æŒçºŒæ”¹é€²**ï¼šç´å…¥ P2 åˆ°é•·æœŸæŠ€è¡“å‚µè¨ˆç•«
4. **äº¤çµ¦ Auto-Fix Bot**ï¼šè‡ªå‹•åŒ–å¯ä¿®å¾©é …ç›®
5. **äººå·¥å¯©æŸ¥**ï¼šé—œéµæ¶æ§‹èª¿æ•´éœ€è¦å·¥ç¨‹å¸«åƒèˆ‡

"""
        
        return playbook
        
    def generate_all_playbooks(self, use_llm: bool = False):
        """Generate playbooks for all clusters"""
        
        if not self.clusters:
            print("âš ï¸  No clusters found. Creating default clusters from directory structure...")
            self._detect_clusters()
            
        output_dir = self.repo_root / "docs" / "refactor_playbooks"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸš€ Generating playbooks for {len(self.clusters)} clusters...\n")
        
        generated_count = 0
        cached_count = 0
        
        for cluster_name, cluster_data in self.clusters.items():
            cluster_score = cluster_data.get('score', 0) if isinstance(cluster_data, dict) else 0
            
            print(f"  ğŸ“ {cluster_name} (score: {cluster_score})")
            
            # Check cache first
            playbook = self._load_cached_playbook(cluster_name)
            if playbook:
                print("     âš¡ Using cached playbook")
                cached_count += 1
            else:
                if use_llm:
                    # Generate LLM prompt
                    prompt = self.generate_cluster_prompt(cluster_name, cluster_score)
                    
                    # Save prompt for manual LLM processing
                    prompt_file = output_dir / f"{cluster_name.replace('/', '_')}_prompt.txt"
                    with open(prompt_file, 'w', encoding='utf-8') as f:
                        f.write(f"System Prompt:\n{self.SYSTEM_PROMPT}\n\n")
                        f.write(f"User Prompt:\n{prompt}\n")
                    print(f"     ğŸ’¡ LLM prompt saved to {prompt_file}")
                    
                    # For now, generate stub (actual LLM integration would go here)
                    playbook = self.generate_playbook_stub(cluster_name, cluster_score)
                else:
                    # Generate stub playbook
                    playbook = self.generate_playbook_stub(cluster_name, cluster_score)
                
                # Save to cache
                self._save_to_cache(cluster_name, playbook)
                generated_count += 1
                
            # Save playbook (sanitize cluster name for filename)
            safe_cluster_name = cluster_name.replace('/', '_').replace('\\', '_').replace('..', '_')
            playbook_file = output_dir / f"{safe_cluster_name}_playbook.md"
            with open(playbook_file, 'w', encoding='utf-8') as f:
                f.write(playbook)
            print(f"     âœ… Playbook saved to {playbook_file}")
        
        print(f"\nâœ¨ Generated {len(self.clusters)} playbooks in {output_dir}")
        print(f"   ğŸ“Š Stats: {generated_count} generated, {cached_count} from cache")
        if self.cache_enabled:
            cache_hit_rate = (cached_count / len(self.clusters) * 100) if len(self.clusters) > 0 else 0
            print(f"   âš¡ Cache hit rate: {cache_hit_rate:.1f}%")
        
    def _generate_directory_tree(self, cluster_name: str, max_depth: int = 3) -> str:
        """Generate directory tree structure for a cluster
        
        Args:
            cluster_name: Name of the cluster (e.g., "core/", "services/")
            max_depth: Maximum depth to traverse (default: 3)
        
        Returns:
            String representation of directory tree
        """
        cluster_path = self.repo_root / cluster_name.rstrip('/')
        
        if not cluster_path.exists():
            return f"{cluster_name}\n  (ç›®éŒ„ä¸å­˜åœ¨)"
        
        def build_tree(path: Path, prefix: str = "", depth: int = 0, is_last: bool = True) -> list[str]:
            """Recursively build tree structure"""
            if depth >= max_depth:
                return []
            
            lines = []
            items = []
            
            try:
                items = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name))
            except PermissionError:
                return [f"{prefix}(ç„¡æ³•è¨ªå•)"]
            
            # Filter out common ignore patterns
            ignore_patterns = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build', '.next'}
            items = [item for item in items if item.name not in ignore_patterns]
            
            for i, item in enumerate(items[:20]):  # Limit to 20 items per directory
                is_last_item = (i == len(items) - 1)
                connector = "â””â”€â”€ " if is_last_item else "â”œâ”€â”€ "
                
                if item.is_dir():
                    lines.append(f"{prefix}{connector}{item.name}/")
                    if depth < max_depth:
                        extension = "    " if is_last_item else "â”‚   "
                        lines.extend(build_tree(item, prefix + extension, depth + 1, is_last_item))
                else:
                    # Show file with extension
                    lines.append(f"{prefix}{connector}{item.name}")
            
            if len(items) > 20:
                # Always use corner connector for the "more items" indicator
                lines.append(f"{prefix}â””â”€â”€ ... ({len(items) - 20} more items)")
            
            return lines
        
        tree_lines = [f"{cluster_name}"]
        tree_lines.extend(build_tree(cluster_path, "", 0))
        
        return "\n".join(tree_lines)
    
    def _generate_file_annotations(self, cluster_name: str) -> list[str]:
        """Generate annotations for important files in a cluster
        
        Args:
            cluster_name: Name of the cluster
            
        Returns:
            List of annotation strings
        """
        annotations = []
        cluster_path = self.repo_root / cluster_name.rstrip('/')
        
        if not cluster_path.exists():
            return ["ï¼ˆç›®éŒ„ä¸å­˜åœ¨ï¼Œç„¡æ³•ç”Ÿæˆè¨»è§£ï¼‰"]
        
        # Common important files to annotate
        important_patterns = {
            'README.md': 'èªªæ˜æ–‡æª”',
            'package.json': 'Node.js å°ˆæ¡ˆé…ç½®',
            'tsconfig.json': 'TypeScript ç·¨è­¯é…ç½®',
            'pyproject.toml': 'Python å°ˆæ¡ˆé…ç½®',
            'Cargo.toml': 'Rust å°ˆæ¡ˆé…ç½®',
            'go.mod': 'Go æ¨¡çµ„å®šç¾©',
            '__init__.py': 'Python å¥—ä»¶åˆå§‹åŒ–',
            'index.ts': 'æ¨¡çµ„å…¥å£é»',
            'main.py': 'Python ä¸»ç¨‹å¼',
            'main.go': 'Go ä¸»ç¨‹å¼'
        }
        
        # Scan cluster for important files
        try:
            for item in cluster_path.rglob('*'):
                if item.is_file() and item.name in important_patterns:
                    rel_path = item.relative_to(self.repo_root)
                    desc = important_patterns[item.name]
                    annotations.append(f"- `{rel_path}` â€” {desc}")
                    
                    if len(annotations) >= 10:  # Limit to 10 annotations
                        break
        except (PermissionError, OSError) as e:
            # Log error but continue - some directories may not be accessible
            print(f"âš ï¸ Warning: Could not scan {cluster_path}: {e}", file=sys.stderr)
        
        if not annotations:
            annotations.append("ï¼ˆæœªç™¼ç¾é‡è¦æª”æ¡ˆéœ€è¦ç‰¹åˆ¥è¨»è§£ï¼‰")
        
        return annotations
    
    def _detect_clusters(self):
        """Detect clusters from directory structure"""
        # Default clusters based on Unmanned Island structure
        default_clusters = [
            'core/',
            'services/',
            'automation/',
            'autonomous/',
            'governance/',
            'apps/',
            'tools/',
            'infrastructure/'
        ]
        
        for cluster in default_clusters:
            cluster_path = self.repo_root / cluster
            if cluster_path.exists():
                self.clusters[cluster] = {'score': 0, 'exists': True}
                
def main():
    """Main function"""
    parser = argparse.ArgumentParser(
        description='Generate AI Refactor Playbooks for directory clusters'
    )
    parser.add_argument(
        '--repo-root',
        default='.',
        help='Repository root directory'
    )
    parser.add_argument(
        '--use-llm',
        action='store_true',
        help='Generate LLM prompts (for future LLM integration)'
    )
    parser.add_argument(
        '--cluster',
        help='Generate playbook for specific cluster only'
    )
    
    args = parser.parse_args()
    
    print("ğŸï¸  Unmanned Island System - AI Refactor Playbook Generator")
    print("=" * 70)
    
    # Create generator
    generator = RefactorPlaybookGenerator(args.repo_root)
    
    # Load governance data
    generator.load_governance_data()
    
    # Generate playbooks
    if args.cluster:
        # Single cluster
        cluster_score = generator.clusters.get(args.cluster, {}).get('score', 0)
        if args.use_llm:
            prompt = generator.generate_cluster_prompt(args.cluster, cluster_score)
            print("\nSystem Prompt:")
            print(generator.SYSTEM_PROMPT)
            print("\nUser Prompt:")
            print(prompt)
        else:
            playbook = generator.generate_playbook_stub(args.cluster, cluster_score)
            output_file = Path('docs') / 'refactor_playbooks' / f"{args.cluster.replace('/', '_')}_playbook.md"
            output_file.parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(playbook)
            print(f"\nâœ… Playbook saved to {output_file}")
    else:
        # All clusters
        generator.generate_all_playbooks(use_llm=args.use_llm)
        
    print("\nâœ¨ Done!")
    
if __name__ == '__main__':
    main()
