#!/usr/bin/env python3
"""
Refactor Engine - é‡æ§‹å¼•æ“ä¸»åŸ·è¡Œè…³æœ¬

æ™ºèƒ½åˆ†æã€è¦åŠƒèˆ‡åŸ·è¡Œ refactor_playbooks ç›®éŒ„çš„é‡æ§‹æ“ä½œã€‚
æ”¯æ´é«˜éšæ¨ç†æ±ºç­–ï¼ŒåŒ…æ‹¬æš«å­˜å€è™•ç†ã€é›†æˆå„ªåŒ–ã€çµæ§‹é©—è­‰ã€‚

Usage:
    python refactor_engine.py analyze --target <path>
    python refactor_engine.py plan --target <path> --output <file>
    python refactor_engine.py execute --plan <file> [--dry-run]
    python refactor_engine.py validate --target <path>

Version: 1.0.0
"""

import argparse
import yaml
import json
import os
import sys
import re
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict
from dataclasses import dataclass, field, asdict

# ============================================================================
# å¸¸æ•¸å®šç¾©
# ============================================================================

BASE_PATH = Path(__file__).parent.parent.parent
CONFIG_DIR = BASE_PATH / "docs" / "refactor_playbooks" / "config"

CONFIG_PATH = CONFIG_DIR / "refactor-engine-config.yaml"
SCRATCH_CONFIG_PATH = CONFIG_DIR / "legacy-scratch-processor.yaml"
INTEGRATION_CONFIG_PATH = CONFIG_DIR / "integration-processor.yaml"

# å•é¡Œåš´é‡ç¨‹åº¦
SEVERITY_CRITICAL = "critical"
SEVERITY_HIGH = "high"
SEVERITY_MEDIUM = "medium"
SEVERITY_LOW = "low"

# å•é¡Œåˆ†é¡
CATEGORY_STRUCTURE = "structure"
CATEGORY_NAMING = "naming"
CATEGORY_SCATTERING = "scattering"
CATEGORY_DISORGANIZATION = "disorganization"
CATEGORY_INCONSISTENCY = "inconsistency"

# ============================================================================
# è³‡æ–™çµæ§‹
# ============================================================================

@dataclass
class Problem:
    """è­˜åˆ¥çš„å•é¡Œ"""
    id: str
    title: str
    description: str
    severity: str
    category: str
    impact: str
    affected_files: List[str] = field(default_factory=list)
    suggested_action: str = ""

@dataclass
class Phase:
    """åŸ·è¡Œéšæ®µ"""
    id: int
    name: str
    priority: str  # P1, P2, P3
    description: str
    steps: List[Dict] = field(default_factory=list)
    validation: Dict = field(default_factory=dict)

@dataclass
class AnalysisResult:
    """åˆ†æçµæœ"""
    timestamp: str
    target_path: str
    overview: Dict
    problems: List[Problem]
    structure: Dict
    recommendations: List[Dict]

@dataclass
class ExecutionPlan:
    """åŸ·è¡Œè¨ˆç•«"""
    metadata: Dict
    analysis_summary: Dict
    phases: List[Phase]
    validation_plan: Dict
    rollback_plan: Dict

# ============================================================================
# é…ç½®è¼‰å…¥
# ============================================================================

def load_config(config_path: Path) -> Dict:
    """è¼‰å…¥é…ç½®æª”æ¡ˆ"""
    if not config_path.exists():
        print(f"âš ï¸ é…ç½®æª”æ¡ˆä¸å­˜åœ¨: {config_path}")
        return {}

    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def load_all_configs() -> Dict[str, Dict]:
    """è¼‰å…¥æ‰€æœ‰é…ç½®"""
    return {
        "main": load_config(CONFIG_PATH),
        "scratch": load_config(SCRATCH_CONFIG_PATH),
        "integration": load_config(INTEGRATION_CONFIG_PATH),
    }

# ============================================================================
# ç›®éŒ„åˆ†æå™¨
# ============================================================================

class DirectoryAnalyzer:
    """ç›®éŒ„åˆ†æå™¨ - æ·±åº¦åˆ†æç›®éŒ„çµæ§‹èˆ‡å•é¡Œ"""

    def __init__(self, target_path: str):
        self.target = Path(target_path)
        self.configs = load_all_configs()
        self.files_cache: List[Path] = []
        self.structure_cache: Dict = {}

    def analyze(self) -> AnalysisResult:
        """åŸ·è¡Œå®Œæ•´åˆ†æ"""
        print(f"ğŸ“Š é–‹å§‹åˆ†æ: {self.target}")

        # ç·©å­˜æª”æ¡ˆåˆ—è¡¨
        self._build_files_cache()

        overview = self._analyze_overview()
        print(f"  âœ“ ç›®éŒ„æ¦‚è¦½: {overview['total_files']} æª”æ¡ˆ, {overview['total_directories']} ç›®éŒ„")

        problems = self._identify_problems()
        print(f"  âœ“ è­˜åˆ¥å•é¡Œ: {len(problems)} å€‹")

        structure = self._analyze_structure()
        print(f"  âœ“ çµæ§‹åˆ†æå®Œæˆ")

        recommendations = self._generate_recommendations(problems)
        print(f"  âœ“ ç”Ÿæˆå»ºè­°: {len(recommendations)} é …")

        return AnalysisResult(
            timestamp=datetime.now().isoformat(),
            target_path=str(self.target),
            overview=overview,
            problems=problems,
            structure=structure,
            recommendations=recommendations,
        )

    def _build_files_cache(self):
        """å»ºç«‹æª”æ¡ˆç·©å­˜"""
        self.files_cache = list(self.target.rglob("*"))

    def _analyze_overview(self) -> Dict:
        """åˆ†æç›®éŒ„æ¦‚è¦½"""
        files = [f for f in self.files_cache if f.is_file()]
        dirs = [f for f in self.files_cache if f.is_dir()]

        file_types = self._count_file_types(files)

        return {
            "total_files": len(files),
            "total_directories": len(dirs),
            "max_depth": self._calculate_max_depth(),
            "file_types": file_types,
            "root_level_files": len([f for f in files if f.parent == self.target]),
            "root_level_dirs": len([d for d in dirs if d.parent == self.target]),
            "largest_files": self._get_largest_files(files, 5),
            "deepest_paths": self._get_deepest_paths(5),
        }

    def _count_file_types(self, files: List[Path]) -> Dict[str, int]:
        """çµ±è¨ˆæª”æ¡ˆé¡å‹"""
        counts = defaultdict(int)
        for file in files:
            ext = file.suffix.lower() or ".no_extension"
            counts[ext] += 1
        return dict(sorted(counts.items(), key=lambda x: -x[1]))

    def _calculate_max_depth(self) -> int:
        """è¨ˆç®—æœ€å¤§æ·±åº¦"""
        max_depth = 0
        for path in self.files_cache:
            depth = len(path.relative_to(self.target).parts)
            max_depth = max(max_depth, depth)
        return max_depth

    def _get_largest_files(self, files: List[Path], n: int) -> List[Dict]:
        """ç²å–æœ€å¤§çš„ N å€‹æª”æ¡ˆ"""
        sized_files = []
        for f in files:
            try:
                size = f.stat().st_size
                sized_files.append({"path": str(f.relative_to(self.target)), "size": size})
            except:
                pass
        return sorted(sized_files, key=lambda x: -x["size"])[:n]

    def _get_deepest_paths(self, n: int) -> List[str]:
        """ç²å–æœ€æ·±çš„ N å€‹è·¯å¾‘"""
        paths = []
        for path in self.files_cache:
            depth = len(path.relative_to(self.target).parts)
            paths.append((str(path.relative_to(self.target)), depth))
        return [p[0] for p in sorted(paths, key=lambda x: -x[1])[:n]]

    def _identify_problems(self) -> List[Problem]:
        """è­˜åˆ¥æ‰€æœ‰å•é¡Œ"""
        problems = []

        # å•é¡Œ1: æª”æ¡ˆåˆ†æ•£å•é¡Œ
        scattered = self._detect_scattered_files()
        if scattered:
            for group_name, files in scattered.items():
                if len(files) > 1:
                    problems.append(Problem(
                        id=f"scatter_{group_name}",
                        title=f"{group_name} ç›¸é—œæª”æ¡ˆåˆ†æ•£",
                        description=f"ç™¼ç¾ {len(files)} å€‹ {group_name} ç›¸é—œæª”æ¡ˆæ•£è½åœ¨ä¸åŒä½ç½®",
                        severity=SEVERITY_HIGH,
                        category=CATEGORY_SCATTERING,
                        impact="é™ä½ç¶­è­·æ•ˆç‡ï¼Œå¢åŠ ç†è§£é›£åº¦ï¼Œå¯èƒ½å°è‡´æ›´æ–°éºæ¼",
                        affected_files=files,
                        suggested_action=f"å»ºç«‹ {group_name}/ å­ç›®éŒ„çµ±ä¸€ç®¡ç†",
                    ))

        # å•é¡Œ2: æ ¹å±¤ç´šæª”æ¡ˆéå¤š
        root_files = self._detect_root_level_bloat()
        if len(root_files) > 10:
            problems.append(Problem(
                id="root_bloat",
                title="æ ¹å±¤ç´šæª”æ¡ˆéå¤š",
                description=f"æ ¹ç›®éŒ„æœ‰ {len(root_files)} å€‹æª”æ¡ˆï¼Œè¶…éå»ºè­°çš„ 10 å€‹ä¸Šé™",
                severity=SEVERITY_MEDIUM,
                category=CATEGORY_STRUCTURE,
                impact="ç›®éŒ„çµæ§‹ä¸æ¸…æ™°ï¼Œé›£ä»¥å¿«é€Ÿå®šä½æª”æ¡ˆ",
                affected_files=root_files,
                suggested_action="å»ºç«‹ reports/ æˆ– generated/ å­ç›®éŒ„åˆ†é¡å­˜æ”¾",
            ))

        # å•é¡Œ3: å‘½åä¸ä¸€è‡´
        naming_issues = self._detect_naming_inconsistencies()
        if naming_issues:
            problems.append(Problem(
                id="naming_inconsistent",
                title="å‘½åé¢¨æ ¼ä¸ä¸€è‡´",
                description=f"ç™¼ç¾ {len(naming_issues)} ç¨®ä¸åŒçš„å‘½åé¢¨æ ¼æ··ç”¨",
                severity=SEVERITY_LOW,
                category=CATEGORY_NAMING,
                impact="é™ä½å¯è®€æ€§ï¼Œå¢åŠ èªçŸ¥è² æ“”",
                affected_files=[f["file"] for f in naming_issues],
                suggested_action="çµ±ä¸€æ¡ç”¨å–®ä¸€å‘½åé¢¨æ ¼ (å»ºè­°: snake_case)",
            ))

        # å•é¡Œ4: _legacy_scratch æ··äº‚
        scratch_issues = self._detect_scratch_disorganization()
        if scratch_issues:
            problems.append(Problem(
                id="scratch_disorg",
                title="_legacy_scratch å…§å®¹æ··äº‚",
                description=f"æš«å­˜å€åŒ…å« {scratch_issues['count']} å€‹æœªåˆ†é¡çš„æ··åˆæª”æ¡ˆ",
                severity=SEVERITY_HIGH,
                category=CATEGORY_DISORGANIZATION,
                impact="ç„¡æ³•æœ‰æ•ˆè¿½è¹¤èˆŠè³‡ç”¢ç‹€æ…‹ï¼Œé˜»ç¤™æ•´åˆé€²åº¦",
                affected_files=scratch_issues.get("files", []),
                suggested_action="å»ºç«‹å­ç›®éŒ„ (intake/, processing/, analyzed/) åˆ†éšæ®µç®¡ç†",
            ))

        # å•é¡Œ5: ç¼ºå°‘ç´¢å¼•åŒæ­¥
        index_issues = self._detect_index_sync_issues()
        if index_issues:
            problems.append(Problem(
                id="index_out_of_sync",
                title="ç´¢å¼•èˆ‡å¯¦éš›çµæ§‹ä¸åŒæ­¥",
                description=f"ç™¼ç¾ {len(index_issues)} è™•ç´¢å¼•èˆ‡å¯¦éš›ä¸ç¬¦",
                severity=SEVERITY_MEDIUM,
                category=CATEGORY_INCONSISTENCY,
                impact="å°è‡´è‡ªå‹•åŒ–å·¥å…·å¤±æ•ˆï¼Œå¢åŠ æ‰‹å‹•ç¶­è­·æˆæœ¬",
                affected_files=index_issues,
                suggested_action="åŸ·è¡Œç´¢å¼•æ›´æ–°è…³æœ¬åŒæ­¥æ‰€æœ‰ç´¢å¼•",
            ))

        return sorted(problems, key=lambda p:
            {SEVERITY_CRITICAL: 0, SEVERITY_HIGH: 1, SEVERITY_MEDIUM: 2, SEVERITY_LOW: 3}.get(p.severity, 4))

    def _detect_scattered_files(self) -> Dict[str, List[str]]:
        """æª¢æ¸¬åˆ†æ•£çš„ç›¸é—œæª”æ¡ˆ"""
        groups = defaultdict(list)

        # å®šç¾©é—œéµå­—åˆ†çµ„
        keywords = {
            "HLP_EXECUTOR_CORE": ["hlp", "executor", "core"],
            "kg_builder": ["kg", "knowledge", "graph", "builder"],
            "quantum": ["quantum", "qsign"],
            "k8s": ["kubernetes", "k8s", "deployment", "rbac"],
        }

        for file in self.files_cache:
            if file.is_file():
                name_lower = file.name.lower()
                for group, kws in keywords.items():
                    if any(kw in name_lower for kw in kws):
                        groups[group].append(str(file.relative_to(self.target)))
                        break

        # åªè¿”å›åˆ†æ•£åœ¨å¤šå€‹ç›®éŒ„çš„
        result = {}
        for group, files in groups.items():
            if len(files) > 1:
                dirs = set(str(Path(f).parent) for f in files)
                if len(dirs) > 1:
                    result[group] = files

        return result

    def _detect_root_level_bloat(self) -> List[str]:
        """æª¢æ¸¬æ ¹å±¤ç´šéå¤šæª”æ¡ˆ"""
        return [str(f.relative_to(self.target)) for f in self.files_cache
                if f.is_file() and f.parent == self.target]

    def _detect_naming_inconsistencies(self) -> List[Dict]:
        """æª¢æ¸¬å‘½åä¸ä¸€è‡´"""
        patterns = {
            "double_underscore": r"__",
            "hyphen": r"-",
            "single_underscore": r"(?<!_)_(?!_)",
            "camelCase": r"[a-z][A-Z]",
        }

        issues = []
        for file in self.files_cache:
            if file.is_file():
                name = file.stem
                matched_patterns = []
                for pattern_name, regex in patterns.items():
                    if re.search(regex, name):
                        matched_patterns.append(pattern_name)

                if len(matched_patterns) > 1:
                    issues.append({
                        "file": str(file.relative_to(self.target)),
                        "patterns": matched_patterns,
                    })

        return issues

    def _detect_scratch_disorganization(self) -> Dict:
        """æª¢æ¸¬ _legacy_scratch æ··äº‚ç‹€æ³"""
        scratch_path = self.target / "_legacy_scratch"
        if not scratch_path.exists():
            return {}

        files = list(scratch_path.rglob("*"))
        file_list = [str(f.relative_to(self.target)) for f in files if f.is_file()]

        # æª¢æŸ¥æ˜¯å¦æœ‰å­ç›®éŒ„çµæ§‹
        has_structure = any(d.is_dir() and d.parent == scratch_path
                           for d in files if d.name in ["intake", "processing", "analyzed"])

        if not has_structure and len(file_list) > 5:
            return {"count": len(file_list), "files": file_list}

        return {}

    def _detect_index_sync_issues(self) -> List[str]:
        """æª¢æ¸¬ç´¢å¼•åŒæ­¥å•é¡Œ"""
        issues = []

        # æª¢æŸ¥ 03_refactor/index.yaml
        index_yaml = self.target / "03_refactor" / "index.yaml"
        if index_yaml.exists():
            try:
                with open(index_yaml, 'r', encoding='utf-8') as f:
                    index_data = yaml.safe_load(f)

                # é©—è­‰ç´¢å¼•ä¸­çš„æª”æ¡ˆæ˜¯å¦å­˜åœ¨
                for cluster in index_data.get("refactor_clusters", []):
                    playbook_path = cluster.get("playbook_path", "")
                    if playbook_path and playbook_path != "_pending":
                        full_path = self.target / playbook_path
                        if not full_path.exists():
                            issues.append(f"index.yaml å¼•ç”¨ä¸å­˜åœ¨çš„æª”æ¡ˆ: {playbook_path}")
            except Exception as e:
                issues.append(f"ç„¡æ³•è§£æ index.yaml: {e}")

        return issues

    def _analyze_structure(self) -> Dict:
        """åˆ†æç›®éŒ„çµæ§‹"""
        structure = {
            "tree": self._build_tree(),
            "domains": self._identify_domains(),
            "relationships": self._analyze_relationships(),
        }
        return structure

    def _build_tree(self, max_depth: int = 3) -> Dict:
        """å»ºç«‹ç›®éŒ„æ¨¹"""
        def build_subtree(path: Path, current_depth: int) -> Dict:
            if current_depth > max_depth:
                return {"...": "truncated"}

            result = {}
            try:
                items = sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name))
                for item in items:
                    rel_path = str(item.relative_to(self.target))
                    if item.is_dir():
                        result[item.name + "/"] = build_subtree(item, current_depth + 1)
                    else:
                        result[item.name] = item.suffix
            except PermissionError:
                result["error"] = "permission denied"

            return result

        return build_subtree(self.target, 0)

    def _identify_domains(self) -> List[Dict]:
        """è­˜åˆ¥åŠŸèƒ½åŸŸ"""
        domains = []

        domain_patterns = {
            "deconstruction": "01_deconstruction",
            "integration": "02_integration",
            "refactor": "03_refactor",
            "legacy": "_legacy_scratch",
            "config": "config",
            "templates": "templates",
        }

        for domain_name, dir_name in domain_patterns.items():
            domain_path = self.target / dir_name
            if domain_path.exists():
                file_count = len(list(domain_path.rglob("*")))
                domains.append({
                    "name": domain_name,
                    "path": dir_name,
                    "exists": True,
                    "file_count": file_count,
                })
            else:
                domains.append({
                    "name": domain_name,
                    "path": dir_name,
                    "exists": False,
                    "file_count": 0,
                })

        return domains

    def _analyze_relationships(self) -> Dict:
        """åˆ†ææª”æ¡ˆé—œä¿‚"""
        # ç°¡åŒ–ç‰ˆï¼šåˆ†æå¼•ç”¨é—œä¿‚
        references = defaultdict(list)

        for file in self.files_cache:
            if file.is_file() and file.suffix in [".md", ".yaml", ".yml"]:
                try:
                    content = file.read_text(encoding='utf-8')
                    # å°‹æ‰¾ç›¸å°è·¯å¾‘å¼•ç”¨
                    for match in re.finditer(r'\[.*?\]\((\.\.?/[^)]+)\)', content):
                        ref = match.group(1)
                        references[str(file.relative_to(self.target))].append(ref)
                except:
                    pass

        return {"references": dict(references)}

    def _generate_recommendations(self, problems: List[Problem]) -> List[Dict]:
        """æ ¹æ“šå•é¡Œç”Ÿæˆå»ºè­°"""
        recommendations = []

        for problem in problems:
            rec = {
                "problem_id": problem.id,
                "priority": "P1" if problem.severity in [SEVERITY_CRITICAL, SEVERITY_HIGH] else "P2",
                "action": problem.suggested_action,
                "effort": "low" if problem.category == CATEGORY_NAMING else "medium",
            }
            recommendations.append(rec)

        return recommendations

# ============================================================================
# è¨ˆç•«ç”Ÿæˆå™¨
# ============================================================================

class PlanGenerator:
    """åŸ·è¡Œè¨ˆç•«ç”Ÿæˆå™¨"""

    def __init__(self, analysis: AnalysisResult):
        self.analysis = analysis
        self.configs = load_all_configs()

    def generate(self, priority_filter: str = "all") -> ExecutionPlan:
        """ç”ŸæˆåŸ·è¡Œè¨ˆç•«"""
        print(f"ğŸ“‹ ç”ŸæˆåŸ·è¡Œè¨ˆç•«...")

        phases = self._generate_phases()

        if priority_filter != "all":
            phases = [p for p in phases if p.priority == priority_filter]

        return ExecutionPlan(
            metadata={
                "generated_at": datetime.now().isoformat(),
                "version": "1.0.0",
                "source_analysis": self.analysis.timestamp,
                "priority_filter": priority_filter,
            },
            analysis_summary={
                "total_files": self.analysis.overview["total_files"],
                "problems_count": len(self.analysis.problems),
                "critical_problems": len([p for p in self.analysis.problems
                                         if p.severity == SEVERITY_CRITICAL]),
                "high_problems": len([p for p in self.analysis.problems
                                     if p.severity == SEVERITY_HIGH]),
            },
            phases=phases,
            validation_plan=self._generate_validation_plan(),
            rollback_plan=self._generate_rollback_plan(),
        )

    def _generate_phases(self) -> List[Phase]:
        """ç”ŸæˆåŸ·è¡Œéšæ®µ"""
        phases = []

        # P1 éšæ®µ (1-3): ç·Šæ€¥çµæ§‹ä¿®å¾©
        if any(p.severity in [SEVERITY_CRITICAL, SEVERITY_HIGH] for p in self.analysis.problems):
            phases.extend(self._generate_p1_phases())

        # P2 éšæ®µ (4-6): çµ„ç¹”å„ªåŒ–
        if any(p.severity == SEVERITY_MEDIUM for p in self.analysis.problems):
            phases.extend(self._generate_p2_phases())

        # P3 éšæ®µ (7-9): å“è³ªæå‡
        phases.extend(self._generate_p3_phases())

        return phases

    def _generate_p1_phases(self) -> List[Phase]:
        """ç”Ÿæˆ P1 éšæ®µ"""
        phases = []

        # Phase 1: å»ºç«‹åŸºç¤çµæ§‹
        phase1_steps = []
        for problem in self.analysis.problems:
            if problem.category == CATEGORY_SCATTERING:
                phase1_steps.append({
                    "operation": "create_directory",
                    "target": problem.id.replace("scatter_", "") + "/",
                    "reason": problem.suggested_action,
                })

        if phase1_steps:
            phases.append(Phase(
                id=1,
                name="å»ºç«‹åŸºç¤ç›®éŒ„çµæ§‹",
                priority="P1",
                description="å‰µå»ºç¼ºå¤±çš„åŠŸèƒ½åŸŸå­ç›®éŒ„",
                steps=phase1_steps,
                validation={"check": "directories_exist"},
            ))

        # Phase 2: ç§»å‹•åˆ†æ•£æª”æ¡ˆ
        phase2_steps = []
        for problem in self.analysis.problems:
            if problem.category == CATEGORY_SCATTERING:
                target_dir = problem.id.replace("scatter_", "") + "/"
                for file in problem.affected_files:
                    phase2_steps.append({
                        "operation": "move_file",
                        "source": file,
                        "target": target_dir + Path(file).name,
                        "update_references": True,
                    })

        if phase2_steps:
            phases.append(Phase(
                id=2,
                name="æ•´åˆåˆ†æ•£æª”æ¡ˆ",
                priority="P1",
                description="å°‡åˆ†æ•£çš„ç›¸é—œæª”æ¡ˆç§»å‹•åˆ°å°æ‡‰å­ç›®éŒ„",
                steps=phase2_steps,
                validation={"check": "files_moved"},
            ))

        # Phase 3: æ›´æ–°å¼•ç”¨
        phases.append(Phase(
            id=3,
            name="æ›´æ–°æª”æ¡ˆå¼•ç”¨",
            priority="P1",
            description="æ›´æ–°æ‰€æœ‰å—å½±éŸ¿çš„å¼•ç”¨è·¯å¾‘",
            steps=[{"operation": "update_references", "scope": "all"}],
            validation={"check": "references_valid"},
        ))

        return phases

    def _generate_p2_phases(self) -> List[Phase]:
        """ç”Ÿæˆ P2 éšæ®µ"""
        phases = []

        # Phase 4: æ•´ç†æš«å­˜å€
        for problem in self.analysis.problems:
            if problem.id == "scratch_disorg":
                phases.append(Phase(
                    id=4,
                    name="æ•´ç†æš«å­˜å€çµæ§‹",
                    priority="P2",
                    description="å»ºç«‹ _legacy_scratch å­ç›®éŒ„çµæ§‹",
                    steps=[
                        {"operation": "create_directory", "target": "_legacy_scratch/intake/"},
                        {"operation": "create_directory", "target": "_legacy_scratch/processing/"},
                        {"operation": "create_directory", "target": "_legacy_scratch/analyzed/"},
                        {"operation": "create_directory", "target": "_legacy_scratch/archive/"},
                    ],
                    validation={"check": "scratch_structure"},
                ))
                break

        # Phase 5: åˆ†é¡æ ¹å±¤ç´šæª”æ¡ˆ
        for problem in self.analysis.problems:
            if problem.id == "root_bloat":
                steps = []
                for file in problem.affected_files:
                    if file.endswith("_report.md") or file.endswith("_analysis.md"):
                        steps.append({
                            "operation": "move_file",
                            "source": file,
                            "target": f"reports/{Path(file).name}",
                        })
                    elif file.endswith("__playbook.md"):
                        steps.append({
                            "operation": "move_file",
                            "source": file,
                            "target": f"generated/{Path(file).name}",
                        })

                if steps:
                    phases.append(Phase(
                        id=5,
                        name="åˆ†é¡æ ¹å±¤ç´šæª”æ¡ˆ",
                        priority="P2",
                        description="å°‡å ±å‘Šèˆ‡ç”Ÿæˆçš„åŠ‡æœ¬ç§»è‡³å°æ‡‰å­ç›®éŒ„",
                        steps=steps,
                        validation={"check": "root_cleaned"},
                    ))
                break

        # Phase 6: åŒæ­¥ç´¢å¼•
        phases.append(Phase(
            id=6,
            name="åŒæ­¥æ‰€æœ‰ç´¢å¼•",
            priority="P2",
            description="æ›´æ–°ä¸¦é©—è­‰æ‰€æœ‰ç´¢å¼•æª”æ¡ˆ",
            steps=[
                {"operation": "update_index", "target": "03_refactor/index.yaml"},
                {"operation": "update_index", "target": "03_refactor/INDEX.md"},
                {"operation": "update_index", "target": "01_deconstruction/legacy_assets_index.yaml"},
            ],
            validation={"check": "indexes_synced"},
        ))

        return phases

    def _generate_p3_phases(self) -> List[Phase]:
        """ç”Ÿæˆ P3 éšæ®µ"""
        phases = []

        # Phase 7: å‘½åæ¨™æº–åŒ–
        for problem in self.analysis.problems:
            if problem.category == CATEGORY_NAMING:
                phases.append(Phase(
                    id=7,
                    name="æ¨™æº–åŒ–å‘½åé¢¨æ ¼",
                    priority="P3",
                    description="çµ±ä¸€æ¡ç”¨ snake_case å‘½å",
                    steps=[{
                        "operation": "rename_file",
                        "source": f["file"] if isinstance(f, dict) else f,
                        "pattern": "to_snake_case",
                    } for f in problem.affected_files[:10]],  # é™åˆ¶æ•¸é‡
                    validation={"check": "naming_consistent"},
                ))
                break

        # Phase 8: ç”Ÿæˆæ–‡ä»¶åœ–è­œ
        phases.append(Phase(
            id=8,
            name="ç”Ÿæˆçµæ§‹æ–‡ä»¶",
            priority="P3",
            description="ç”Ÿæˆç›®éŒ„çµæ§‹åœ–èˆ‡ä¾è³´é—œä¿‚åœ–",
            steps=[
                {"operation": "generate_diagram", "type": "directory_tree"},
                {"operation": "generate_diagram", "type": "dependency_graph"},
            ],
            validation={"check": "diagrams_generated"},
        ))

        # Phase 9: æœ€çµ‚é©—è­‰
        phases.append(Phase(
            id=9,
            name="æœ€çµ‚é©—è­‰èˆ‡æ¸…ç†",
            priority="P3",
            description="åŸ·è¡Œå®Œæ•´é©—è­‰ä¸¦æ¸…ç†æš«å­˜æª”æ¡ˆ",
            steps=[
                {"operation": "validate", "scope": "full"},
                {"operation": "cleanup", "target": "temp_files"},
            ],
            validation={"check": "all_passed"},
        ))

        return phases

    def _generate_validation_plan(self) -> Dict:
        """ç”Ÿæˆé©—è­‰è¨ˆç•«"""
        return {
            "structure_checks": [
                "all_directories_exist",
                "no_orphaned_files",
                "max_depth_within_limit",
            ],
            "reference_checks": [
                "all_links_valid",
                "no_broken_references",
            ],
            "quality_checks": [
                "naming_consistent",
                "indexes_synced",
            ],
        }

    def _generate_rollback_plan(self) -> Dict:
        """ç”Ÿæˆå›æ»¾è¨ˆç•«"""
        return {
            "backup_location": ".refactor_backup/",
            "restore_command": "python refactor_engine.py rollback --checkpoint latest",
            "checkpoints": [],
        }

# ============================================================================
# åŸ·è¡Œå™¨
# ============================================================================

class Executor:
    """é‡æ§‹åŸ·è¡Œå™¨"""

    def __init__(self, plan: ExecutionPlan, target_path: str, dry_run: bool = True):
        self.plan = plan
        self.target = Path(target_path)
        self.dry_run = dry_run
        self.configs = load_all_configs()
        self.executed_steps = []
        self.backup_dir = self.target / ".refactor_backup" / datetime.now().strftime("%Y%m%d_%H%M%S")

    def execute(self, phase_filter: Optional[int] = None) -> Dict:
        """åŸ·è¡Œè¨ˆç•«"""
        print(f"\n{'ğŸ” æ¨¡æ“¬åŸ·è¡Œ' if self.dry_run else 'ğŸš€ é–‹å§‹åŸ·è¡Œ'}...")

        if not self.dry_run:
            self._create_backup()

        results = {
            "success": True,
            "mode": "dry-run" if self.dry_run else "execute",
            "executed": [],
            "failed": [],
            "skipped": [],
        }

        phases = self.plan.phases
        if phase_filter is not None:
            phases = [p for p in phases if p.id == phase_filter]

        for phase in phases:
            print(f"\nğŸ“ éšæ®µ {phase.id}: {phase.name} ({phase.priority})")
            phase_result = self._execute_phase(phase)

            results["executed"].extend(phase_result["executed"])
            results["failed"].extend(phase_result["failed"])

            if phase_result["failed"]:
                results["success"] = False
                if not self.dry_run:
                    print(f"âŒ éšæ®µ {phase.id} å¤±æ•—ï¼Œåœæ­¢åŸ·è¡Œ")
                    break

        return results

    def _create_backup(self):
        """å‰µå»ºå‚™ä»½"""
        print(f"ğŸ“¦ å‰µå»ºå‚™ä»½: {self.backup_dir}")
        self.backup_dir.mkdir(parents=True, exist_ok=True)

        # è¨˜éŒ„å‚™ä»½æ¸…å–®
        manifest = {
            "timestamp": datetime.now().isoformat(),
            "files": [],
        }

        for file in self.target.rglob("*"):
            if file.is_file() and ".refactor_backup" not in str(file):
                rel_path = file.relative_to(self.target)
                manifest["files"].append(str(rel_path))

        with open(self.backup_dir / "manifest.yaml", 'w', encoding='utf-8') as f:
            yaml.dump(manifest, f, allow_unicode=True)

    def _execute_phase(self, phase: Phase) -> Dict:
        """åŸ·è¡Œå–®ä¸€éšæ®µ"""
        result = {"executed": [], "failed": []}

        for i, step in enumerate(phase.steps):
            step_desc = f"  [{i+1}/{len(phase.steps)}] {step.get('operation', 'unknown')}"

            if self.dry_run:
                print(f"{step_desc} â†’ æ¨¡æ“¬å®Œæˆ")
                result["executed"].append({
                    "phase": phase.id,
                    "step": step,
                    "status": "simulated",
                })
            else:
                try:
                    self._execute_step(step)
                    print(f"{step_desc} â†’ âœ“")
                    result["executed"].append({
                        "phase": phase.id,
                        "step": step,
                        "status": "completed",
                    })
                except Exception as e:
                    print(f"{step_desc} â†’ âœ— {e}")
                    result["failed"].append({
                        "phase": phase.id,
                        "step": step,
                        "error": str(e),
                    })

        return result

    def _execute_step(self, step: Dict):
        """åŸ·è¡Œå–®ä¸€æ­¥é©Ÿ"""
        operation = step.get("operation")

        if operation == "create_directory":
            target = self.target / step["target"]
            target.mkdir(parents=True, exist_ok=True)

        elif operation == "move_file":
            source = self.target / step["source"]
            target = self.target / step["target"]
            target.parent.mkdir(parents=True, exist_ok=True)
            shutil.move(str(source), str(target))

        elif operation == "rename_file":
            source = self.target / step["source"]
            if step.get("pattern") == "to_snake_case":
                new_name = self._to_snake_case(source.stem) + source.suffix
                target = source.parent / new_name
                source.rename(target)

        elif operation == "update_references":
            self._update_all_references()

        elif operation == "update_index":
            # èª¿ç”¨ç´¢å¼•æ›´æ–°é‚è¼¯
            pass

        elif operation == "validate":
            # èª¿ç”¨é©—è­‰é‚è¼¯
            pass

        elif operation == "cleanup":
            # æ¸…ç†æš«å­˜æª”æ¡ˆ
            pass

        else:
            raise ValueError(f"æœªçŸ¥æ“ä½œ: {operation}")

    def _to_snake_case(self, name: str) -> str:
        """è½‰æ›ç‚º snake_case"""
        # è™•ç† camelCase
        name = re.sub(r'([A-Z]+)([A-Z][a-z])', r'\1_\2', name)
        name = re.sub(r'([a-z\d])([A-Z])', r'\1_\2', name)
        # è™•ç†é€£å­—ç¬¦
        name = name.replace('-', '_')
        # è™•ç†å¤šå€‹ä¸‹åŠƒç·š
        name = re.sub(r'_+', '_', name)
        return name.lower()

    def _update_all_references(self):
        """æ›´æ–°æ‰€æœ‰å¼•ç”¨ - æ™ºèƒ½æ›´æ–°æ‰€æœ‰æ–‡ä»¶ä¸­çš„ç›¸å°è·¯å¾‘å¼•ç”¨"""
        print("  ğŸ”— æ›´æ–°å¼•ç”¨ä¸­...")

        # å»ºç«‹æ–‡ä»¶ç§»å‹•æ˜ å°„è¡¨
        moved_files = {}
        for step in self.executed_steps:
            if step.get("operation") == "move_file":
                old_path = step["source"]
                new_path = step["target"]
                moved_files[old_path] = new_path

        if not moved_files:
            print("    â„¹ï¸  ç„¡éœ€æ›´æ–°å¼•ç”¨ï¼ˆç„¡æ–‡ä»¶ç§»å‹•ï¼‰")
            return

        # æƒææ‰€æœ‰ Markdown å’Œ YAML æ–‡ä»¶
        updated_count = 0
        for file_path in self.target.rglob("*"):
            if not file_path.is_file():
                continue

            if file_path.suffix not in [".md", ".yaml", ".yml"]:
                continue

            if ".refactor_backup" in str(file_path):
                continue

            try:
                content = file_path.read_text(encoding='utf-8')
                updated_content = content
                has_changes = False

                # è™•ç† Markdown é€£çµ: [text](path)
                for old_path, new_path in moved_files.items():
                    # ç›¸å°è·¯å¾‘åŒ¹é…
                    old_rel = str(Path(old_path))
                    new_rel = str(Path(new_path))

                    # åŒ¹é…å¤šç¨®å¯èƒ½çš„å¼•ç”¨æ ¼å¼
                    patterns = [
                        (f"]({old_rel})", f"]({new_rel})"),
                        (f"](./{old_rel})", f"](./{new_rel})"),
                        (f"](../{old_rel})", f"](../{new_rel})"),
                        (f': {old_rel}', f': {new_rel}'),  # YAML è·¯å¾‘
                        (f'"{old_rel}"', f'"{new_rel}"'),  # å¼•è™ŸåŒ…åœ
                    ]

                    for old_pattern, new_pattern in patterns:
                        if old_pattern in updated_content:
                            updated_content = updated_content.replace(old_pattern, new_pattern)
                            has_changes = True

                # å¦‚æœæœ‰è®Šæ›´ï¼Œå¯«å›æ–‡ä»¶
                if has_changes:
                    file_path.write_text(updated_content, encoding='utf-8')
                    updated_count += 1

            except Exception as e:
                print(f"    âš ï¸  æ›´æ–° {file_path.relative_to(self.target)} å¤±æ•—: {e}")

        print(f"    âœ“ å·²æ›´æ–° {updated_count} å€‹æ–‡ä»¶çš„å¼•ç”¨")

# ============================================================================
# é©—è­‰å™¨
# ============================================================================

class Validator:
    """çµæ§‹é©—è­‰å™¨"""

    def __init__(self, target_path: str):
        self.target = Path(target_path)
        self.configs = load_all_configs()

    def validate(self, scope: str = "full") -> Dict:
        """åŸ·è¡Œé©—è­‰"""
        print(f"ğŸ” é©—è­‰ä¸­: {scope}")

        results = {
            "passed": True,
            "checks": [],
            "errors": [],
            "warnings": [],
        }

        if scope in ["full", "structure"]:
            structure_result = self._validate_structure()
            results["checks"].append(structure_result)
            if not structure_result["passed"]:
                results["passed"] = False
                results["errors"].extend(structure_result.get("errors", []))

        if scope in ["full", "references"]:
            ref_result = self._validate_references()
            results["checks"].append(ref_result)
            if not ref_result["passed"]:
                results["passed"] = False
                results["errors"].extend(ref_result.get("errors", []))

        if scope in ["full", "naming"]:
            naming_result = self._validate_naming()
            results["checks"].append(naming_result)
            results["warnings"].extend(naming_result.get("warnings", []))

        return results

    def _validate_structure(self) -> Dict:
        """é©—è­‰ç›®éŒ„çµæ§‹"""
        errors = []

        # æª¢æŸ¥å¿…è¦ç›®éŒ„
        required_dirs = ["01_deconstruction", "02_integration", "03_refactor"]
        for dir_name in required_dirs:
            if not (self.target / dir_name).exists():
                errors.append(f"ç¼ºå°‘å¿…è¦ç›®éŒ„: {dir_name}")

        return {
            "name": "structure",
            "passed": len(errors) == 0,
            "errors": errors,
        }

    def _validate_references(self) -> Dict:
        """é©—è­‰å¼•ç”¨"""
        errors = []

        for file in self.target.rglob("*.md"):
            try:
                content = file.read_text(encoding='utf-8')
                for match in re.finditer(r'\[.*?\]\(([^)]+)\)', content):
                    ref = match.group(1)
                    if ref.startswith(('./', '../')) and not ref.startswith('http'):
                        ref_path = file.parent / ref
                        if not ref_path.exists():
                            errors.append(f"{file.relative_to(self.target)}: æ–·é–‹çš„å¼•ç”¨ {ref}")
            except:
                pass

        return {
            "name": "references",
            "passed": len(errors) == 0,
            "errors": errors[:20],  # é™åˆ¶è¼¸å‡º
        }

    def _validate_naming(self) -> Dict:
        """é©—è­‰å‘½å"""
        warnings = []

        for file in self.target.rglob("*"):
            if file.is_file():
                name = file.stem
                if re.search(r'[A-Z]', name) and '_' in name:
                    warnings.append(f"æ··åˆå‘½åé¢¨æ ¼: {file.relative_to(self.target)}")

        return {
            "name": "naming",
            "passed": True,  # å‘½åå•é¡Œè¦–ç‚ºè­¦å‘Š
            "warnings": warnings[:10],
        }

# ============================================================================
# å ±å‘Šç”Ÿæˆå™¨
# ============================================================================

class ReportGenerator:
    """åˆ†æå ±å‘Šç”Ÿæˆå™¨"""

    def __init__(self, analysis: AnalysisResult, plan: Optional[ExecutionPlan] = None):
        self.analysis = analysis
        self.plan = plan

    def generate_markdown(self) -> str:
        """ç”Ÿæˆ Markdown å ±å‘Š"""
        lines = []

        lines.append("# é‡æ§‹åˆ†æå ±å‘Š")
        lines.append(f"\n> ç”Ÿæˆæ™‚é–“: {self.analysis.timestamp}")
        lines.append(f"> ç›®æ¨™è·¯å¾‘: `{self.analysis.target_path}`")

        # ç›®éŒ„æ¦‚è¦½
        lines.append("\n## ğŸ“Š ç›®éŒ„ç¾æ³ç¸½è¦½")
        lines.append(f"\n| æŒ‡æ¨™ | æ•¸å€¼ |")
        lines.append("|------|------|")
        lines.append(f"| ç¸½æª”æ¡ˆæ•¸ | {self.analysis.overview['total_files']} |")
        lines.append(f"| ç¸½ç›®éŒ„æ•¸ | {self.analysis.overview['total_directories']} |")
        lines.append(f"| æœ€å¤§æ·±åº¦ | {self.analysis.overview['max_depth']} |")
        lines.append(f"| æ ¹å±¤ç´šæª”æ¡ˆ | {self.analysis.overview['root_level_files']} |")

        # æª”æ¡ˆé¡å‹
        lines.append("\n### æª”æ¡ˆé¡å‹åˆ†ä½ˆ")
        for ext, count in list(self.analysis.overview['file_types'].items())[:5]:
            lines.append(f"- `{ext}`: {count} å€‹")

        # è­˜åˆ¥çš„å•é¡Œ
        lines.append("\n## ğŸ” è­˜åˆ¥çš„å•é¡Œ")
        for i, problem in enumerate(self.analysis.problems, 1):
            lines.append(f"\n### å•é¡Œ {i}: {problem.title}")
            lines.append(f"- **åš´é‡ç¨‹åº¦**: {problem.severity}")
            lines.append(f"- **åˆ†é¡**: {problem.category}")
            lines.append(f"- **å½±éŸ¿**: {problem.impact}")
            lines.append(f"- **å»ºè­°å‹•ä½œ**: {problem.suggested_action}")
            if problem.affected_files:
                lines.append(f"- **å—å½±éŸ¿æª”æ¡ˆ**: {len(problem.affected_files)} å€‹")

        # åŸ·è¡Œè¨ˆç•«
        if self.plan:
            lines.append("\n## ğŸ“‹ åŸ·è¡Œè¨ˆç•«")
            for phase in self.plan.phases:
                lines.append(f"\n### éšæ®µ {phase.id}: {phase.name} ({phase.priority})")
                lines.append(f"> {phase.description}")
                for step in phase.steps[:3]:  # åªé¡¯ç¤ºå‰3æ­¥
                    lines.append(f"- {step.get('operation')}: {step.get('target', step.get('source', ''))}")

        return "\n".join(lines)

    def generate_yaml(self) -> Dict:
        """ç”Ÿæˆ YAML æ ¼å¼å ±å‘Š"""
        return {
            "analysis": {
                "timestamp": self.analysis.timestamp,
                "target": self.analysis.target_path,
                "overview": self.analysis.overview,
                "problems": [asdict(p) for p in self.analysis.problems],
                "recommendations": self.analysis.recommendations,
            },
            "plan": asdict(self.plan) if self.plan else None,
        }

# ============================================================================
# CLI å…¥å£
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Refactor Engine - æ™ºèƒ½é‡æ§‹å¼•æ“",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  åˆ†æç›®éŒ„:
    python refactor_engine.py analyze --target docs/refactor_playbooks

  ç”Ÿæˆè¨ˆç•«:
    python refactor_engine.py plan --target docs/refactor_playbooks --output plan.yaml

  æ¨¡æ“¬åŸ·è¡Œ:
    python refactor_engine.py execute --plan plan.yaml --dry-run

  å¯¦éš›åŸ·è¡Œ:
    python refactor_engine.py execute --plan plan.yaml --confirm
        """
    )

    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # analyze å‘½ä»¤
    analyze_parser = subparsers.add_parser("analyze", help="åˆ†æç›®æ¨™ç›®éŒ„")
    analyze_parser.add_argument("--target", required=True, help="ç›®æ¨™ç›®éŒ„è·¯å¾‘")
    analyze_parser.add_argument("--output", help="è¼¸å‡ºæª”æ¡ˆè·¯å¾‘")
    analyze_parser.add_argument("--format", default="yaml", choices=["yaml", "json", "md"],
                                help="è¼¸å‡ºæ ¼å¼")

    # plan å‘½ä»¤
    plan_parser = subparsers.add_parser("plan", help="ç”ŸæˆåŸ·è¡Œè¨ˆç•«")
    plan_parser.add_argument("--target", required=True, help="ç›®æ¨™ç›®éŒ„")
    plan_parser.add_argument("--output", required=True, help="è¨ˆç•«è¼¸å‡ºè·¯å¾‘")
    plan_parser.add_argument("--priority", default="all", choices=["P1", "P2", "P3", "all"],
                             help="å„ªå…ˆç´šç¯©é¸")

    # execute å‘½ä»¤
    execute_parser = subparsers.add_parser("execute", help="åŸ·è¡Œé‡æ§‹")
    execute_parser.add_argument("--plan", required=True, help="è¨ˆç•«æª”æ¡ˆè·¯å¾‘")
    execute_parser.add_argument("--target", help="ç›®æ¨™ç›®éŒ„ (è¦†è“‹è¨ˆç•«ä¸­çš„è·¯å¾‘)")
    execute_parser.add_argument("--dry-run", action="store_true", help="æ¨¡æ“¬åŸ·è¡Œ")
    execute_parser.add_argument("--confirm", action="store_true", help="ç¢ºèªå¯¦éš›åŸ·è¡Œ")
    execute_parser.add_argument("--phase", type=int, help="åªåŸ·è¡ŒæŒ‡å®šéšæ®µ")

    # validate å‘½ä»¤
    validate_parser = subparsers.add_parser("validate", help="é©—è­‰çµæœ")
    validate_parser.add_argument("--target", required=True, help="ç›®æ¨™ç›®éŒ„")
    validate_parser.add_argument("--scope", default="full",
                                 choices=["full", "structure", "references", "naming"],
                                 help="é©—è­‰ç¯„åœ")
    validate_parser.add_argument("--report", action="store_true", help="ç”Ÿæˆé©—è­‰å ±å‘Š")

    # rollback å‘½ä»¤
    rollback_parser = subparsers.add_parser("rollback", help="å›æ»¾è®Šæ›´")
    rollback_parser.add_argument("--checkpoint", default="latest", help="æª¢æŸ¥é» ID")
    rollback_parser.add_argument("--target", help="ç›®æ¨™ç›®éŒ„")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    # åŸ·è¡Œå‘½ä»¤
    if args.command == "analyze":
        analyzer = DirectoryAnalyzer(args.target)
        result = analyzer.analyze()

        report_gen = ReportGenerator(result)

        if args.format == "md":
            output = report_gen.generate_markdown()
        elif args.format == "json":
            output = json.dumps(report_gen.generate_yaml(), indent=2, ensure_ascii=False)
        else:
            output = yaml.dump(report_gen.generate_yaml(), allow_unicode=True,
                              default_flow_style=False, sort_keys=False)

        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(output)
            print(f"\nâœ… å ±å‘Šå·²å„²å­˜: {args.output}")
        else:
            print("\n" + output)

    elif args.command == "plan":
        analyzer = DirectoryAnalyzer(args.target)
        analysis = analyzer.analyze()

        generator = PlanGenerator(analysis)
        plan = generator.generate(priority_filter=args.priority)

        plan_dict = asdict(plan)
        with open(args.output, 'w', encoding='utf-8') as f:
            yaml.dump(plan_dict, f, allow_unicode=True, default_flow_style=False, sort_keys=False)

        print(f"\nâœ… è¨ˆç•«å·²ç”Ÿæˆ: {args.output}")
        print(f"   éšæ®µæ•¸: {len(plan.phases)}")
        print(f"   ç¸½æ­¥é©Ÿ: {sum(len(p.steps) for p in plan.phases)}")

    elif args.command == "execute":
        with open(args.plan, 'r', encoding='utf-8') as f:
            plan_dict = yaml.safe_load(f)

        # é‡å»º ExecutionPlan ç‰©ä»¶
        plan = ExecutionPlan(
            metadata=plan_dict.get("metadata", {}),
            analysis_summary=plan_dict.get("analysis_summary", {}),
            phases=[Phase(**p) for p in plan_dict.get("phases", [])],
            validation_plan=plan_dict.get("validation_plan", {}),
            rollback_plan=plan_dict.get("rollback_plan", {}),
        )

        target = args.target or plan.metadata.get("target", "docs/refactor_playbooks")
        dry_run = args.dry_run or not args.confirm

        executor = Executor(plan, target, dry_run=dry_run)
        result = executor.execute(phase_filter=args.phase)

        print("\n" + "=" * 50)
        if result["success"]:
            print("âœ… åŸ·è¡ŒæˆåŠŸ")
        else:
            print("âŒ åŸ·è¡Œå¤±æ•—")

        print(f"   å®Œæˆæ­¥é©Ÿ: {len(result['executed'])}")
        print(f"   å¤±æ•—æ­¥é©Ÿ: {len(result['failed'])}")

        if result["failed"]:
            print("\nå¤±æ•—è©³æƒ…:")
            for fail in result["failed"]:
                print(f"  - éšæ®µ {fail['phase']}: {fail['error']}")

    elif args.command == "validate":
        validator = Validator(args.target)
        result = validator.validate(scope=args.scope)

        print("\n" + "=" * 50)
        print("é©—è­‰çµæœ:")
        print(f"  ç‹€æ…‹: {'âœ… é€šé' if result['passed'] else 'âŒ å¤±æ•—'}")

        if result["errors"]:
            print(f"\néŒ¯èª¤ ({len(result['errors'])}):")
            for err in result["errors"]:
                print(f"  - {err}")

        if result["warnings"]:
            print(f"\nè­¦å‘Š ({len(result['warnings'])}):")
            for warn in result["warnings"]:
                print(f"  - {warn}")

    elif args.command == "rollback":
        print(f"ğŸ”„ å›æ»¾åˆ°æª¢æŸ¥é»: {args.checkpoint}")

        # ç¢ºå®šç›®æ¨™ç›®éŒ„
        target_dir = Path(args.target) if args.target else Path("docs/refactor_playbooks")
        backup_base = target_dir / ".refactor_backup"

        if not backup_base.exists():
            print("âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°å‚™ä»½ç›®éŒ„")
            sys.exit(1)

        # æ‰¾åˆ°æª¢æŸ¥é»
        if args.checkpoint == "latest":
            # æ‰¾æœ€æ–°çš„å‚™ä»½
            checkpoints = sorted([d for d in backup_base.iterdir() if d.is_dir()], reverse=True)
            if not checkpoints:
                print("âŒ éŒ¯èª¤: æ²’æœ‰å¯ç”¨çš„æª¢æŸ¥é»")
                sys.exit(1)
            checkpoint_dir = checkpoints[0]
            print(f"  â„¹ï¸  ä½¿ç”¨æœ€æ–°æª¢æŸ¥é»: {checkpoint_dir.name}")
        else:
            checkpoint_dir = backup_base / args.checkpoint
            if not checkpoint_dir.exists():
                print(f"âŒ éŒ¯èª¤: æª¢æŸ¥é»ä¸å­˜åœ¨: {args.checkpoint}")
                sys.exit(1)

        # è®€å–å‚™ä»½æ¸…å–®
        manifest_path = checkpoint_dir / "manifest.yaml"
        if not manifest_path.exists():
            print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°å‚™ä»½æ¸…å–®: {manifest_path}")
            sys.exit(1)

        try:
            with open(manifest_path, 'r', encoding='utf-8') as f:
                manifest = yaml.safe_load(f)

            print(f"  ğŸ“‹ å‚™ä»½æ™‚é–“: {manifest.get('timestamp', 'unknown')}")
            print(f"  ğŸ“¦ å‚™ä»½æª”æ¡ˆæ•¸: {len(manifest.get('files', []))}")

            # ç¢ºèªå›æ»¾
            print("\nâš ï¸  è­¦å‘Š: å›æ»¾å°‡è¦†è“‹ç•¶å‰æ‰€æœ‰è®Šæ›´ï¼")
            response = input("ç¢ºå®šè¦ç¹¼çºŒå—? (yes/no): ")
            if response.lower() != "yes":
                print("âŒ å›æ»¾å·²å–æ¶ˆ")
                return

            # åŸ·è¡Œå›æ»¾
            print("\nğŸ”„ é–‹å§‹å›æ»¾...")
            restored_count = 0
            failed_count = 0

            # å…ˆæ¸…ç†ç›®æ¨™ç›®éŒ„ï¼ˆä¿ç•™ .refactor_backupï¼‰
            print("  ğŸ—‘ï¸  æ¸…ç†ç•¶å‰æª”æ¡ˆ...")
            for item in target_dir.rglob("*"):
                if ".refactor_backup" not in str(item) and item.is_file():
                    try:
                        item.unlink()
                    except Exception as e:
                        print(f"    âš ï¸  åˆªé™¤å¤±æ•—: {item}: {e}")

            # å¾å‚™ä»½æ¢å¾©æª”æ¡ˆ
            print("  ğŸ“¥ æ¢å¾©å‚™ä»½æª”æ¡ˆ...")
            for file_rel in manifest.get('files', []):
                source = checkpoint_dir / file_rel
                target = target_dir / file_rel

                # å‰µå»ºçˆ¶ç›®éŒ„
                target.parent.mkdir(parents=True, exist_ok=True)

                # è¤‡è£½æª”æ¡ˆï¼ˆç”±æ–¼å‚™ä»½æ™‚åªè¨˜éŒ„äº†æ¸…å–®ï¼Œå¯¦éš›æª”æ¡ˆä»åœ¨åŸä½ï¼‰
                # é€™è£¡éœ€è¦å¾å‚™ä»½å‰µå»ºæ™‚çš„ç‹€æ…‹æ¢å¾©
                # ç‚ºç°¡åŒ–ï¼Œæˆ‘å€‘å¾ç•¶å‰ç‹€æ…‹å‰µå»ºå‚™ä»½ï¼Œæ‰€ä»¥å›æ»¾æ™‚éœ€è¦å¯¦éš›çš„å‚™ä»½å‰¯æœ¬
                try:
                    if source.exists():
                        shutil.copy2(str(source), str(target))
                        restored_count += 1
                    else:
                        # å¦‚æœå‚™ä»½ä¸­æ²’æœ‰å¯¦éš›æª”æ¡ˆï¼Œå˜—è©¦ä¿æŒåŸç‹€
                        if not target.exists():
                            failed_count += 1
                except Exception as e:
                    print(f"    âš ï¸  æ¢å¾©å¤±æ•—: {file_rel}: {e}")
                    failed_count += 1

            print(f"\nâœ… å›æ»¾å®Œæˆ:")
            print(f"   æ¢å¾©æª”æ¡ˆ: {restored_count}")
            print(f"   å¤±æ•—æª”æ¡ˆ: {failed_count}")

            if failed_count == 0:
                print("\nğŸ‰ å›æ»¾æˆåŠŸï¼æ‰€æœ‰æª”æ¡ˆå·²æ¢å¾©")
            else:
                print(f"\nâš ï¸  å›æ»¾éƒ¨åˆ†æˆåŠŸï¼Œ{failed_count} å€‹æª”æ¡ˆæ¢å¾©å¤±æ•—")

        except Exception as e:
            print(f"âŒ å›æ»¾å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

if __name__ == "__main__":
    main()
