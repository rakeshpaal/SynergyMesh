#!/usr/bin/env python3
"""
Index Updater - ç´¢å¼•æ›´æ–°å¼•æ“

æ›´æ–°ä¸¦åŒæ­¥å„ç¨®ç´¢å¼•æª”æ¡ˆï¼ŒåŒ…æ‹¬ï¼š
1. index.yaml (æ©Ÿå™¨å¯è®€)
2. INDEX.md (äººé¡å¯è®€)
3. legacy_assets_index.yaml
4. é©—è­‰ç´¢å¼•ä¸€è‡´æ€§

Usage:
    python update_indexes.py all
    python update_indexes.py machine
    python update_indexes.py human
    python update_indexes.py verify

Version: 1.0.0
"""

import argparse
import yaml
import os
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field, asdict
from collections import defaultdict

# ============================================================================
# å¸¸æ•¸èˆ‡é…ç½®
# ============================================================================

BASE_PATH = Path(__file__).parent.parent.parent
PLAYBOOKS_PATH = BASE_PATH / "docs" / "refactor_playbooks"

INDEX_YAML_PATH = PLAYBOOKS_PATH / "03_refactor" / "index.yaml"
INDEX_MD_PATH = PLAYBOOKS_PATH / "03_refactor" / "INDEX.md"
LEGACY_INDEX_PATH = PLAYBOOKS_PATH / "01_deconstruction" / "legacy_assets_index.yaml"

# ============================================================================
# è³‡æ–™çµæ§‹
# ============================================================================

@dataclass
class IndexEntry:
    """ç´¢å¼•æ¢ç›®"""
    id: str
    name: str
    path: str
    type: str
    status: str
    description: str = ""
    metadata: Dict = field(default_factory=dict)

@dataclass
class ClusterEntry:
    """å¢é›†æ¢ç›®"""
    cluster_id: str
    name: str
    description: str
    playbook_path: str
    status: str
    priority: str
    files: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)

@dataclass
class LegacyAssetEntry:
    """éºç•™è³‡ç”¢æ¢ç›®"""
    asset_id: str
    name: str
    source_path: str
    type: str
    status: str
    target_path: Optional[str] = None
    notes: str = ""

@dataclass
class IndexVerificationResult:
    """ç´¢å¼•é©—è­‰çµæœ"""
    is_valid: bool
    errors: List[str]
    warnings: List[str]
    missing_files: List[str]
    orphan_files: List[str]
    sync_status: Dict[str, bool]

# ============================================================================
# ç´¢å¼•æƒæå™¨
# ============================================================================

class IndexScanner:
    """
    ç´¢å¼•æƒæå™¨ï¼šæƒæç›®éŒ„çµæ§‹ç”Ÿæˆç´¢å¼•è³‡æ–™
    """

    def __init__(self, base_path: Path):
        self.base_path = base_path

    def scan_refactor_clusters(self) -> List[ClusterEntry]:
        """æƒæé‡æ§‹å¢é›†"""
        clusters = []
        refactor_path = self.base_path / "03_refactor"

        if not refactor_path.exists():
            return clusters

        # æƒæ playbook æª”æ¡ˆ
        playbook_files = list(refactor_path.rglob("*_playbook.md")) + \
                        list(refactor_path.rglob("*__playbook.md"))

        for pb_file in playbook_files:
            cluster_info = self._parse_playbook(pb_file)
            if cluster_info:
                clusters.append(cluster_info)

        # æƒæå­ç›®éŒ„ä½œç‚ºå¢é›†
        for subdir in refactor_path.iterdir():
            if subdir.is_dir() and not subdir.name.startswith(('_', '.')):
                # æª¢æŸ¥æ˜¯å¦å·²æœ‰ playbook
                existing = next((c for c in clusters if subdir.name in c.playbook_path), None)
                if not existing:
                    clusters.append(ClusterEntry(
                        cluster_id=subdir.name,
                        name=self._format_name(subdir.name),
                        description=f"{subdir.name} ç›¸é—œæª”æ¡ˆ",
                        playbook_path="_pending",
                        status="pending",
                        priority="P3",
                        files=self._list_files(subdir),
                        tags=[subdir.name],
                    ))

        return clusters

    def scan_legacy_assets(self) -> List[LegacyAssetEntry]:
        """æƒæéºç•™è³‡ç”¢"""
        assets = []
        legacy_path = self.base_path / "_legacy_scratch"

        if not legacy_path.exists():
            return assets

        for file in legacy_path.rglob("*"):
            if file.is_file() and not file.name.startswith('.'):
                assets.append(LegacyAssetEntry(
                    asset_id=file.stem[:8],
                    name=file.name,
                    source_path=str(file.relative_to(self.base_path)),
                    type=self._classify_type(file),
                    status="intake",
                ))

        return assets

    def scan_all_documents(self) -> List[IndexEntry]:
        """æƒææ‰€æœ‰æ–‡æª”"""
        entries = []

        for file in self.base_path.rglob("*.md"):
            if file.is_file() and not any(p.startswith('_') for p in file.parts):
                entries.append(IndexEntry(
                    id=file.stem,
                    name=self._format_name(file.stem),
                    path=str(file.relative_to(self.base_path)),
                    type="document",
                    status="active",
                    description=self._extract_description(file),
                ))

        return entries

    def _parse_playbook(self, pb_file: Path) -> Optional[ClusterEntry]:
        """è§£æ playbook æª”æ¡ˆ"""
        try:
            content = pb_file.read_text(encoding='utf-8')

            # æå–æ¨™é¡Œ
            title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
            name = title_match.group(1) if title_match else pb_file.stem

            # æå–æè¿°
            desc_match = re.search(r'^>\s*(.+)$', content, re.MULTILINE)
            description = desc_match.group(1) if desc_match else ""

            # æå–ç‹€æ…‹
            status = "active"
            if "pending" in content.lower() or "TODO" in content:
                status = "pending"

            # æå–å„ªå…ˆç´š
            priority = "P2"
            if "P1" in content:
                priority = "P1"
            elif "P3" in content:
                priority = "P3"

            return ClusterEntry(
                cluster_id=pb_file.stem.replace("__playbook", "").replace("_playbook", ""),
                name=name,
                description=description,
                playbook_path=str(pb_file.relative_to(self.base_path)),
                status=status,
                priority=priority,
                files=[],
                tags=[],
            )

        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è§£æ {pb_file}: {e}")
            return None

    def _format_name(self, raw_name: str) -> str:
        """æ ¼å¼åŒ–åç¨±"""
        # ç§»é™¤å‰ç¶´æ•¸å­—
        name = re.sub(r'^\d+_', '', raw_name)
        # è½‰æ›ä¸‹åŠƒç·šç‚ºç©ºæ ¼
        name = name.replace('_', ' ').replace('-', ' ')
        # é¦–å­—æ¯å¤§å¯«
        return name.title()

    def _classify_type(self, file: Path) -> str:
        """åˆ†é¡æª”æ¡ˆé¡å‹"""
        ext = file.suffix.lower()
        type_map = {
            '.md': 'documentation',
            '.yaml': 'configuration',
            '.yml': 'configuration',
            '.json': 'data',
            '.py': 'code',
            '.txt': 'text',
        }
        return type_map.get(ext, 'unknown')

    def _extract_description(self, file: Path) -> str:
        """æå–æª”æ¡ˆæè¿°"""
        try:
            content = file.read_text(encoding='utf-8')
            # å˜—è©¦æå–ç¬¬ä¸€æ®µéæ¨™é¡Œæ–‡å­—
            for line in content.split('\n'):
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('>'):
                    return line[:200]
        except:
            pass
        return ""

    def _list_files(self, directory: Path) -> List[str]:
        """åˆ—å‡ºç›®éŒ„ä¸­çš„æª”æ¡ˆ"""
        return [str(f.relative_to(self.base_path))
                for f in directory.rglob("*") if f.is_file()]

# ============================================================================
# ç´¢å¼•ç”Ÿæˆå™¨
# ============================================================================

class IndexGenerator:
    """
    ç´¢å¼•ç”Ÿæˆå™¨ï¼šç”Ÿæˆå„ç¨®æ ¼å¼çš„ç´¢å¼•
    """

    def __init__(self, scanner: IndexScanner):
        self.scanner = scanner

    def generate_machine_index(self, clusters: List[ClusterEntry]) -> Dict:
        """ç”Ÿæˆæ©Ÿå™¨å¯è®€ç´¢å¼• (YAML)"""
        return {
            "version": "1.0.0",
            "generated_at": datetime.now().isoformat(),
            "description": "Refactor Playbooks æ©Ÿå™¨å¯è®€ç´¢å¼•",
            "refactor_clusters": [
                {
                    "cluster_id": c.cluster_id,
                    "name": c.name,
                    "description": c.description,
                    "playbook_path": c.playbook_path,
                    "status": c.status,
                    "priority": c.priority,
                    "file_count": len(c.files),
                    "tags": c.tags,
                }
                for c in clusters
            ],
            "statistics": {
                "total_clusters": len(clusters),
                "by_status": self._count_by_field(clusters, "status"),
                "by_priority": self._count_by_field(clusters, "priority"),
            },
        }

    def generate_human_index(self, clusters: List[ClusterEntry]) -> str:
        """ç”Ÿæˆäººé¡å¯è®€ç´¢å¼• (Markdown)"""
        lines = [
            "# Refactor Playbooks ç´¢å¼•",
            "",
            f"> è‡ªå‹•ç”Ÿæˆæ–¼ {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            "",
            "## æ¦‚è¦½",
            "",
            f"- ç¸½å¢é›†æ•¸: {len(clusters)}",
            f"- å¾…è™•ç†: {sum(1 for c in clusters if c.status == 'pending')}",
            f"- é€²è¡Œä¸­: {sum(1 for c in clusters if c.status == 'active')}",
            f"- å·²å®Œæˆ: {sum(1 for c in clusters if c.status == 'completed')}",
            "",
            "## æŒ‰å„ªå…ˆç´šåˆ†é¡",
            "",
        ]

        # æŒ‰å„ªå…ˆç´šåˆ†çµ„
        by_priority = defaultdict(list)
        for c in clusters:
            by_priority[c.priority].append(c)

        for priority in ["P1", "P2", "P3"]:
            if priority in by_priority:
                lines.append(f"### {priority} - {'ç·Šæ€¥' if priority == 'P1' else 'ä¸€èˆ¬' if priority == 'P2' else 'ä½å„ªå…ˆç´š'}")
                lines.append("")
                for c in by_priority[priority]:
                    status_emoji = {"pending": "â³", "active": "ğŸ”„", "completed": "âœ…"}.get(c.status, "â“")
                    if c.playbook_path != "_pending":
                        lines.append(f"- [{c.name}]({c.playbook_path}) {status_emoji}")
                    else:
                        lines.append(f"- {c.name} {status_emoji} *(playbook å¾…å»ºç«‹)*")
                lines.append("")

        # æ·»åŠ ç›®éŒ„çµæ§‹
        lines.extend([
            "## ç›®éŒ„çµæ§‹",
            "",
            "```",
            "03_refactor/",
        ])

        for c in sorted(clusters, key=lambda x: x.cluster_id):
            lines.append(f"â”œâ”€â”€ {c.cluster_id}/")

        lines.extend([
            "â””â”€â”€ index.yaml",
            "```",
            "",
            "---",
            "",
            "*æ­¤æª”æ¡ˆç”± `update_indexes.py` è‡ªå‹•ç”Ÿæˆ*",
        ])

        return "\n".join(lines)

    def generate_legacy_index(self, assets: List[LegacyAssetEntry]) -> Dict:
        """ç”Ÿæˆéºç•™è³‡ç”¢ç´¢å¼•"""
        return {
            "version": "1.0.0",
            "generated_at": datetime.now().isoformat(),
            "description": "éºç•™è³‡ç”¢æ¸…å–®",
            "assets": [
                {
                    "asset_id": a.asset_id,
                    "name": a.name,
                    "source_path": a.source_path,
                    "type": a.type,
                    "status": a.status,
                    "target_path": a.target_path,
                    "notes": a.notes,
                }
                for a in assets
            ],
            "statistics": {
                "total_assets": len(assets),
                "by_type": self._count_by_field(assets, "type"),
                "by_status": self._count_by_field(assets, "status"),
            },
        }

    def _count_by_field(self, items: List, field: str) -> Dict[str, int]:
        """æŒ‰æ¬„ä½çµ±è¨ˆæ•¸é‡"""
        counts = defaultdict(int)
        for item in items:
            value = getattr(item, field, "unknown")
            counts[value] += 1
        return dict(counts)

# ============================================================================
# ç´¢å¼•é©—è­‰å™¨
# ============================================================================

class IndexVerifier:
    """
    ç´¢å¼•é©—è­‰å™¨ï¼šé©—è­‰ç´¢å¼•èˆ‡å¯¦éš›çµæ§‹çš„ä¸€è‡´æ€§
    """

    def __init__(self, base_path: Path):
        self.base_path = base_path

    def verify_all(self) -> IndexVerificationResult:
        """åŸ·è¡Œå®Œæ•´é©—è­‰"""
        errors = []
        warnings = []
        missing_files = []
        orphan_files = []
        sync_status = {}

        # é©—è­‰ index.yaml
        yaml_result = self._verify_yaml_index()
        errors.extend(yaml_result.get("errors", []))
        missing_files.extend(yaml_result.get("missing", []))
        sync_status["index.yaml"] = len(yaml_result.get("errors", [])) == 0

        # é©—è­‰ INDEX.md
        md_result = self._verify_md_index()
        warnings.extend(md_result.get("warnings", []))
        sync_status["INDEX.md"] = len(md_result.get("warnings", [])) == 0

        # é©—è­‰ legacy_assets_index.yaml
        legacy_result = self._verify_legacy_index()
        errors.extend(legacy_result.get("errors", []))
        sync_status["legacy_assets_index.yaml"] = len(legacy_result.get("errors", [])) == 0

        # æª¢æŸ¥å­¤ç«‹æª”æ¡ˆ
        orphan_files = self._find_orphan_files()
        if orphan_files:
            warnings.append(f"ç™¼ç¾ {len(orphan_files)} å€‹æœªè¢«ç´¢å¼•çš„æª”æ¡ˆ")

        return IndexVerificationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings,
            missing_files=missing_files,
            orphan_files=orphan_files,
            sync_status=sync_status,
        )

    def _verify_yaml_index(self) -> Dict:
        """é©—è­‰ YAML ç´¢å¼•"""
        result = {"errors": [], "missing": []}

        if not INDEX_YAML_PATH.exists():
            result["errors"].append("index.yaml ä¸å­˜åœ¨")
            return result

        try:
            with open(INDEX_YAML_PATH, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)

            # æª¢æŸ¥çµæ§‹
            if "refactor_clusters" not in data:
                result["errors"].append("ç¼ºå°‘ refactor_clusters æ¬„ä½")
                return result

            # é©—è­‰æ¯å€‹å¢é›†
            for cluster in data["refactor_clusters"]:
                playbook_path = cluster.get("playbook_path", "")
                if playbook_path and playbook_path != "_pending":
                    full_path = self.base_path / playbook_path
                    if not full_path.exists():
                        result["missing"].append(playbook_path)
                        result["errors"].append(f"Playbook ä¸å­˜åœ¨: {playbook_path}")

        except yaml.YAMLError as e:
            result["errors"].append(f"YAML è§£æéŒ¯èª¤: {e}")

        return result

    def _verify_md_index(self) -> Dict:
        """é©—è­‰ Markdown ç´¢å¼•"""
        result = {"warnings": []}

        if not INDEX_MD_PATH.exists():
            result["warnings"].append("INDEX.md ä¸å­˜åœ¨")
            return result

        try:
            content = INDEX_MD_PATH.read_text(encoding='utf-8')

            # æª¢æŸ¥é€£çµ
            links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
            for text, href in links:
                if not href.startswith(('http', '#')):
                    full_path = INDEX_MD_PATH.parent / href
                    if not full_path.exists():
                        result["warnings"].append(f"æ–·é–‹çš„é€£çµ: {href}")

        except Exception as e:
            result["warnings"].append(f"è®€å–éŒ¯èª¤: {e}")

        return result

    def _verify_legacy_index(self) -> Dict:
        """é©—è­‰éºç•™è³‡ç”¢ç´¢å¼•"""
        result = {"errors": []}

        if not LEGACY_INDEX_PATH.exists():
            # éºç•™ç´¢å¼•å¯é¸
            return result

        try:
            with open(LEGACY_INDEX_PATH, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)

            # é©—è­‰æ¯å€‹è³‡ç”¢
            for asset in data.get("assets", []):
                source_path = asset.get("source_path", "")
                if source_path:
                    full_path = self.base_path / source_path
                    if not full_path.exists():
                        result["errors"].append(f"è³‡ç”¢ä¸å­˜åœ¨: {source_path}")

        except yaml.YAMLError as e:
            result["errors"].append(f"YAML è§£æéŒ¯èª¤: {e}")

        return result

    def _find_orphan_files(self) -> List[str]:
        """æ‰¾å‡ºæœªè¢«ç´¢å¼•çš„æª”æ¡ˆ"""
        orphans = []

        # æ”¶é›†ç´¢å¼•ä¸­çš„æ‰€æœ‰è·¯å¾‘
        indexed_paths = set()

        if INDEX_YAML_PATH.exists():
            try:
                with open(INDEX_YAML_PATH, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f)
                for cluster in data.get("refactor_clusters", []):
                    if cluster.get("playbook_path"):
                        indexed_paths.add(cluster["playbook_path"])
            except:
                pass

        # æƒæå¯¦éš›æª”æ¡ˆ
        refactor_path = self.base_path / "03_refactor"
        if refactor_path.exists():
            for file in refactor_path.rglob("*.md"):
                rel_path = str(file.relative_to(self.base_path))
                if rel_path not in indexed_paths and "INDEX" not in file.name:
                    orphans.append(rel_path)

        return orphans

# ============================================================================
# ç´¢å¼•æ›´æ–°å™¨ (ä¸»é¡)
# ============================================================================

class IndexUpdater:
    """
    ç´¢å¼•æ›´æ–°å™¨ï¼šå”èª¿æ‰€æœ‰ç´¢å¼•æ“ä½œ
    """

    def __init__(self, base_path: Optional[Path] = None):
        self.base_path = base_path or PLAYBOOKS_PATH
        self.scanner = IndexScanner(self.base_path)
        self.generator = IndexGenerator(self.scanner)
        self.verifier = IndexVerifier(self.base_path)

    def update_all(self) -> Dict[str, bool]:
        """æ›´æ–°æ‰€æœ‰ç´¢å¼•"""
        results = {}

        print("ğŸ“š æ›´æ–°æ‰€æœ‰ç´¢å¼•...")

        # æ›´æ–°æ©Ÿå™¨ç´¢å¼•
        results["machine"] = self.update_machine_index()

        # æ›´æ–°äººé¡ç´¢å¼•
        results["human"] = self.update_human_index()

        # æ›´æ–°éºç•™ç´¢å¼•
        results["legacy"] = self.update_legacy_index()

        return results

    def update_machine_index(self) -> bool:
        """æ›´æ–°æ©Ÿå™¨å¯è®€ç´¢å¼•"""
        print("   æ›´æ–° index.yaml...")

        try:
            clusters = self.scanner.scan_refactor_clusters()
            index_data = self.generator.generate_machine_index(clusters)

            INDEX_YAML_PATH.parent.mkdir(parents=True, exist_ok=True)
            with open(INDEX_YAML_PATH, 'w', encoding='utf-8') as f:
                yaml.dump(index_data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)

            print(f"   âœ“ å·²æ›´æ–° ({len(clusters)} å¢é›†)")
            return True

        except Exception as e:
            print(f"   âœ— éŒ¯èª¤: {e}")
            return False

    def update_human_index(self) -> bool:
        """æ›´æ–°äººé¡å¯è®€ç´¢å¼•"""
        print("   æ›´æ–° INDEX.md...")

        try:
            clusters = self.scanner.scan_refactor_clusters()
            index_content = self.generator.generate_human_index(clusters)

            INDEX_MD_PATH.parent.mkdir(parents=True, exist_ok=True)
            with open(INDEX_MD_PATH, 'w', encoding='utf-8') as f:
                f.write(index_content)

            print(f"   âœ“ å·²æ›´æ–°")
            return True

        except Exception as e:
            print(f"   âœ— éŒ¯èª¤: {e}")
            return False

    def update_legacy_index(self) -> bool:
        """æ›´æ–°éºç•™è³‡ç”¢ç´¢å¼•"""
        print("   æ›´æ–° legacy_assets_index.yaml...")

        try:
            assets = self.scanner.scan_legacy_assets()
            index_data = self.generator.generate_legacy_index(assets)

            LEGACY_INDEX_PATH.parent.mkdir(parents=True, exist_ok=True)
            with open(LEGACY_INDEX_PATH, 'w', encoding='utf-8') as f:
                yaml.dump(index_data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)

            print(f"   âœ“ å·²æ›´æ–° ({len(assets)} è³‡ç”¢)")
            return True

        except Exception as e:
            print(f"   âœ— éŒ¯èª¤: {e}")
            return False

    def verify(self) -> IndexVerificationResult:
        """é©—è­‰ç´¢å¼•"""
        print("ğŸ” é©—è­‰ç´¢å¼•...")
        return self.verifier.verify_all()

# ============================================================================
# CLI å…¥å£
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Index Updater - ç´¢å¼•æ›´æ–°å¼•æ“",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # all å‘½ä»¤
    all_parser = subparsers.add_parser("all", help="æ›´æ–°æ‰€æœ‰ç´¢å¼•")

    # machine å‘½ä»¤
    machine_parser = subparsers.add_parser("machine", help="æ›´æ–° index.yaml")

    # human å‘½ä»¤
    human_parser = subparsers.add_parser("human", help="æ›´æ–° INDEX.md")

    # legacy å‘½ä»¤
    legacy_parser = subparsers.add_parser("legacy", help="æ›´æ–° legacy_assets_index.yaml")

    # verify å‘½ä»¤
    verify_parser = subparsers.add_parser("verify", help="é©—è­‰ç´¢å¼•")
    verify_parser.add_argument("--fix", action="store_true", help="è‡ªå‹•ä¿®å¾©å•é¡Œ")

    # scan å‘½ä»¤
    scan_parser = subparsers.add_parser("scan", help="æƒæç›®éŒ„çµæ§‹")
    scan_parser.add_argument("--output", "-o", help="è¼¸å‡ºæª”æ¡ˆ")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    updater = IndexUpdater()

    if args.command == "all":
        results = updater.update_all()
        print("\nçµæœ:")
        for name, success in results.items():
            status = "âœ…" if success else "âŒ"
            print(f"  {name}: {status}")

    elif args.command == "machine":
        success = updater.update_machine_index()
        print(f"\n{'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'}")

    elif args.command == "human":
        success = updater.update_human_index()
        print(f"\n{'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'}")

    elif args.command == "legacy":
        success = updater.update_legacy_index()
        print(f"\n{'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'}")

    elif args.command == "verify":
        result = updater.verify()

        print(f"\né©—è­‰çµæœ: {'âœ… é€šé' if result.is_valid else 'âŒ å¤±æ•—'}")

        if result.errors:
            print(f"\néŒ¯èª¤ ({len(result.errors)}):")
            for err in result.errors:
                print(f"  - {err}")

        if result.warnings:
            print(f"\nè­¦å‘Š ({len(result.warnings)}):")
            for warn in result.warnings:
                print(f"  - {warn}")

        if result.missing_files:
            print(f"\nç¼ºå¤±æª”æ¡ˆ ({len(result.missing_files)}):")
            for f in result.missing_files:
                print(f"  - {f}")

        if result.orphan_files:
            print(f"\nå­¤ç«‹æª”æ¡ˆ ({len(result.orphan_files)}):")
            for f in result.orphan_files[:10]:  # åªé¡¯ç¤ºå‰10å€‹
                print(f"  - {f}")
            if len(result.orphan_files) > 10:
                print(f"  ... é‚„æœ‰ {len(result.orphan_files) - 10} å€‹")

        print("\nåŒæ­¥ç‹€æ…‹:")
        for name, synced in result.sync_status.items():
            status = "âœ…" if synced else "âŒ"
            print(f"  {name}: {status}")

        if args.fix and not result.is_valid:
            print("\nğŸ”§ å˜—è©¦è‡ªå‹•ä¿®å¾©...")
            updater.update_all()

    elif args.command == "scan":
        scanner = IndexScanner(PLAYBOOKS_PATH)

        output = {
            "clusters": [asdict(c) for c in scanner.scan_refactor_clusters()],
            "legacy_assets": [asdict(a) for a in scanner.scan_legacy_assets()],
            "documents": [asdict(d) for d in scanner.scan_all_documents()],
        }

        output_str = yaml.dump(output, allow_unicode=True, default_flow_style=False)

        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(output_str)
            print(f"æƒæçµæœå·²å„²å­˜: {args.output}")
        else:
            print(output_str)

if __name__ == "__main__":
    main()
