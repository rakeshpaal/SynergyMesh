#!/usr/bin/env python3
"""
Legacy Scratch Processor - æš«å­˜å€è™•ç†å¼•æ“

è™•ç† _legacy_scratch æš«å­˜å€ä¸­çš„èˆŠè³‡ç”¢ï¼ŒåŸ·è¡Œï¼š
1. è©å½™/æ˜ å°„/å¼•ç”¨æƒæ
2. é«˜éšæ¨ç†åˆ†ææ±ºç­–
3. ä½ç½®åˆ†é…èˆ‡æ•´åˆé¡å‹åˆ¤å®š
4. æ‰¹é‡è™•ç†èˆ‡å ±å‘Šç”Ÿæˆ

Usage:
    python process_legacy_scratch.py scan
    python process_legacy_scratch.py analyze --asset <filename>
    python process_legacy_scratch.py decide --asset <filename>
    python process_legacy_scratch.py batch --all

Version: 1.0.0
"""

import argparse
import yaml
import json
import os
import re
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Set
from dataclasses import dataclass, field, asdict
from enum import Enum
from collections import defaultdict

# å°å…¥èªçŸ¥å¼•æ“
try:
    from cognitive_engine import (
        CognitiveEngine, UnderstandingLayer, ReasoningLayer,
        SearchLayer, IntegrationLayer, CognitiveContext
    )
    COGNITIVE_AVAILABLE = True
except ImportError:
    COGNITIVE_AVAILABLE = False

# ============================================================================
# å¸¸æ•¸èˆ‡é…ç½®
# ============================================================================

BASE_PATH = Path(__file__).parent.parent.parent
PLAYBOOKS_PATH = BASE_PATH / "docs" / "refactor_playbooks"
SCRATCH_PATH = PLAYBOOKS_PATH / "_legacy_scratch"
CONFIG_PATH = PLAYBOOKS_PATH / "config" / "legacy-scratch-processor.yaml"

# ============================================================================
# æšèˆ‰å®šç¾©
# ============================================================================

class AssetType(Enum):
    """è³‡ç”¢é¡å‹"""
    DOCUMENTATION = "documentation"      # æ–‡æª”
    CONFIGURATION = "configuration"      # é…ç½®
    CODE = "code"                        # ä»£ç¢¼
    DATA = "data"                        # æ•¸æ“š
    SCHEMA = "schema"                    # æ¶æ§‹å®šç¾©
    MANIFEST = "manifest"                # æ¸…å–®
    TEMPLATE = "template"                # æ¨¡æ¿
    UNKNOWN = "unknown"                  # æœªçŸ¥

class IntegrationType(Enum):
    """æ•´åˆé¡å‹"""
    FULL_INTEGRATION = "full_integration"        # å®Œæ•´æ•´åˆ
    EMBEDDED_INTEGRATION = "embedded_integration" # åµŒå…¥å¼æ•´åˆ
    REFERENCE_ONLY = "reference_only"            # åƒ…å¼•ç”¨
    ARCHIVE = "archive"                          # æ­¸æª”
    DISCARD = "discard"                          # ä¸Ÿæ£„

class ProcessingStage(Enum):
    """è™•ç†éšæ®µ"""
    INTAKE = "intake"           # æ¥æ”¶
    SCANNING = "scanning"       # æƒæä¸­
    ANALYZING = "analyzing"     # åˆ†æä¸­
    DECIDED = "decided"         # å·²æ±ºç­–
    INTEGRATED = "integrated"   # å·²æ•´åˆ
    ARCHIVED = "archived"       # å·²æ­¸æª”

# ============================================================================
# è³‡æ–™çµæ§‹
# ============================================================================

@dataclass
class VocabularyMatch:
    """è©å½™åŒ¹é…"""
    term: str
    category: str           # namespace, module, domain, keyword
    context: str            # åŒ¹é…çš„ä¸Šä¸‹æ–‡
    line_number: int
    confidence: float

@dataclass
class ReferenceMatch:
    """å¼•ç”¨åŒ¹é…"""
    reference_type: str     # file, url, module, function
    target: str
    source_location: str
    is_internal: bool
    exists: bool

@dataclass
class StructureAnalysis:
    """çµæ§‹åˆ†æ"""
    file_type: str
    encoding: str
    line_count: int
    sections: List[Dict]
    hierarchy_depth: int
    has_frontmatter: bool
    metadata: Dict

@dataclass
class AssetAnalysis:
    """è³‡ç”¢åˆ†æçµæœ"""
    asset_id: str
    filename: str
    asset_type: AssetType
    file_hash: str
    vocabulary_matches: List[VocabularyMatch]
    reference_matches: List[ReferenceMatch]
    structure: StructureAnalysis
    domain_classification: Dict[str, float]
    quality_score: float
    timestamp: str

@dataclass
class PlacementDecision:
    """ä½ç½®æ±ºç­–"""
    asset_id: str
    integration_type: IntegrationType
    target_directory: str
    target_filename: Optional[str]
    embedding_location: Optional[str]    # ç”¨æ–¼åµŒå…¥å¼æ•´åˆ
    embedding_section: Optional[str]
    reasoning: List[str]
    confidence: float
    alternatives: List[Dict]
    requires_manual_review: bool

@dataclass
class AssetInventory:
    """è³‡ç”¢æ¸…å–®"""
    scan_timestamp: str
    scratch_path: str
    total_assets: int
    by_type: Dict[str, int]
    by_stage: Dict[str, int]
    assets: List[Dict]

# ============================================================================
# è©å½™æƒæå™¨
# ============================================================================

class VocabularyScanner:
    """
    è©å½™æƒæå™¨ï¼šæƒæè³‡ç”¢ä¸­çš„é—œéµè©å½™ä¸¦èˆ‡ç¾æœ‰å°ˆæ¡ˆæ˜ å°„
    """

    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}

        # å‘½åç©ºé–“é—œéµå­—
        self.namespace_keywords = {
            "hlp": ["hlp", "executor", "core", "HLP_EXECUTOR"],
            "kg": ["kg", "knowledge", "graph", "builder", "KG_BUILDER"],
            "quantum": ["quantum", "qsign", "signature"],
            "machinenativenops": ["machinenativenops", "architecture", "foundation"],
            "k8s": ["kubernetes", "k8s", "deployment", "rbac", "helm"],
            "refactor": ["refactor", "playbook", "migration"],
        }

        # æ¨¡çµ„é¡å‹é—œéµå­—
        self.module_keywords = {
            "config": ["config", "configuration", "settings", "env"],
            "schema": ["schema", "model", "type", "interface"],
            "service": ["service", "handler", "processor", "worker"],
            "util": ["util", "helper", "common", "shared"],
            "test": ["test", "spec", "mock", "fixture"],
        }

        # é ˜åŸŸé—œéµå­—
        self.domain_keywords = {
            "deconstruction": ["deconstruct", "decompose", "break", "split"],
            "integration": ["integrate", "merge", "combine", "unify"],
            "refactor": ["refactor", "restructure", "reorganize", "optimize"],
            "legacy": ["legacy", "deprecated", "old", "migration"],
        }

    def scan(self, content: str, filename: str) -> List[VocabularyMatch]:
        """æƒæå…§å®¹ä¸­çš„è©å½™"""
        matches = []
        lines = content.split('\n')

        for line_num, line in enumerate(lines, 1):
            line_lower = line.lower()

            # æƒæå‘½åç©ºé–“
            for ns, keywords in self.namespace_keywords.items():
                for kw in keywords:
                    if kw.lower() in line_lower:
                        matches.append(VocabularyMatch(
                            term=kw,
                            category="namespace",
                            context=line.strip()[:100],
                            line_number=line_num,
                            confidence=0.9 if kw == ns else 0.7,
                        ))

            # æƒææ¨¡çµ„é¡å‹
            for mod, keywords in self.module_keywords.items():
                for kw in keywords:
                    if kw.lower() in line_lower:
                        matches.append(VocabularyMatch(
                            term=kw,
                            category="module_type",
                            context=line.strip()[:100],
                            line_number=line_num,
                            confidence=0.8,
                        ))

            # æƒæé ˜åŸŸ
            for domain, keywords in self.domain_keywords.items():
                for kw in keywords:
                    if kw.lower() in line_lower:
                        matches.append(VocabularyMatch(
                            term=kw,
                            category="domain",
                            context=line.strip()[:100],
                            line_number=line_num,
                            confidence=0.75,
                        ))

        # å»é‡ä¸¦ä¿ç•™æœ€é«˜ä¿¡å¿ƒåº¦
        unique_matches = {}
        for match in matches:
            key = (match.term, match.category)
            if key not in unique_matches or match.confidence > unique_matches[key].confidence:
                unique_matches[key] = match

        return list(unique_matches.values())

    def extract_domain_classification(self, matches: List[VocabularyMatch]) -> Dict[str, float]:
        """å¾è©å½™åŒ¹é…ä¸­æå–é ˜åŸŸåˆ†é¡"""
        classification = defaultdict(float)

        for match in matches:
            if match.category == "namespace":
                classification[match.term] += match.confidence * 2
            elif match.category == "domain":
                classification[match.term] += match.confidence
            elif match.category == "module_type":
                classification[f"module_{match.term}"] += match.confidence * 0.5

        # æ­£è¦åŒ–
        total = sum(classification.values())
        if total > 0:
            classification = {k: round(v/total, 3) for k, v in classification.items()}

        return dict(sorted(classification.items(), key=lambda x: -x[1]))

# ============================================================================
# å¼•ç”¨æƒæå™¨
# ============================================================================

class ReferenceScanner:
    """
    å¼•ç”¨æƒæå™¨ï¼šæƒæè³‡ç”¢ä¸­çš„å¼•ç”¨ä¸¦é©—è­‰
    """

    def __init__(self, project_root: Path):
        self.project_root = project_root

        # å¼•ç”¨æ¨¡å¼
        self.patterns = {
            "markdown_link": r'\[([^\]]+)\]\(([^)]+)\)',
            "file_path": r'(?:^|[\s\'"])([a-zA-Z0-9_\-./]+\.[a-zA-Z]{2,4})(?:[\s\'"]|$)',
            "url": r'https?://[^\s\'"<>]+',
            "import": r'(?:from|import)\s+([a-zA-Z0-9_.]+)',
            "yaml_ref": r'\$ref:\s*[\'"]?([^\'">\s]+)[\'"]?',
        }

    def scan(self, content: str, source_path: Path) -> List[ReferenceMatch]:
        """æƒæå…§å®¹ä¸­çš„å¼•ç”¨"""
        matches = []

        for ref_type, pattern in self.patterns.items():
            for match in re.finditer(pattern, content, re.MULTILINE):
                if ref_type == "markdown_link":
                    target = match.group(2)
                elif ref_type == "url":
                    target = match.group(0)
                else:
                    target = match.group(1) if match.lastindex else match.group(0)

                # åˆ¤æ–·æ˜¯å¦ç‚ºå…§éƒ¨å¼•ç”¨
                is_internal = not target.startswith(('http://', 'https://', 'ftp://'))

                # é©—è­‰å¼•ç”¨æ˜¯å¦å­˜åœ¨
                exists = self._check_reference_exists(target, source_path, is_internal)

                matches.append(ReferenceMatch(
                    reference_type=ref_type,
                    target=target,
                    source_location=str(source_path),
                    is_internal=is_internal,
                    exists=exists,
                ))

        # å»é‡
        seen = set()
        unique = []
        for m in matches:
            key = (m.reference_type, m.target)
            if key not in seen:
                seen.add(key)
                unique.append(m)

        return unique

    def _check_reference_exists(self, target: str, source_path: Path, is_internal: bool) -> bool:
        """æª¢æŸ¥å¼•ç”¨æ˜¯å¦å­˜åœ¨"""
        if not is_internal:
            return True  # å¤–éƒ¨é€£çµå‡è¨­å­˜åœ¨

        # å˜—è©¦è§£æç›¸å°è·¯å¾‘
        if target.startswith('./') or target.startswith('../'):
            resolved = (source_path.parent / target).resolve()
        else:
            resolved = self.project_root / target

        return resolved.exists()

# ============================================================================
# çµæ§‹åˆ†æå™¨
# ============================================================================

class StructureAnalyzer:
    """
    çµæ§‹åˆ†æå™¨ï¼šåˆ†æè³‡ç”¢çš„çµæ§‹ç‰¹æ€§
    """

    def analyze(self, content: str, filepath: Path) -> StructureAnalysis:
        """åˆ†æå…§å®¹çµæ§‹"""
        lines = content.split('\n')

        # æª¢æ¸¬æª”æ¡ˆé¡å‹
        file_type = self._detect_file_type(filepath, content)

        # æª¢æ¸¬ç·¨ç¢¼
        encoding = self._detect_encoding(content)

        # åˆ†ææ®µè½çµæ§‹
        sections = self._analyze_sections(content, file_type)

        # è¨ˆç®—å±¤ç´šæ·±åº¦
        hierarchy_depth = self._calculate_hierarchy_depth(content, file_type)

        # æª¢æŸ¥ frontmatter
        has_frontmatter = content.startswith('---\n')

        # æå–å…ƒæ•¸æ“š
        metadata = self._extract_metadata(content, file_type)

        return StructureAnalysis(
            file_type=file_type,
            encoding=encoding,
            line_count=len(lines),
            sections=sections,
            hierarchy_depth=hierarchy_depth,
            has_frontmatter=has_frontmatter,
            metadata=metadata,
        )

    def _detect_file_type(self, filepath: Path, content: str) -> str:
        """æª¢æ¸¬æª”æ¡ˆé¡å‹"""
        ext = filepath.suffix.lower()

        type_map = {
            '.md': 'markdown',
            '.yaml': 'yaml',
            '.yml': 'yaml',
            '.json': 'json',
            '.py': 'python',
            '.ts': 'typescript',
            '.js': 'javascript',
            '.txt': 'text',
        }

        return type_map.get(ext, 'unknown')

    def _detect_encoding(self, content: str) -> str:
        """æª¢æ¸¬ç·¨ç¢¼"""
        try:
            content.encode('ascii')
            return 'ascii'
        except UnicodeEncodeError:
            return 'utf-8'

    def _analyze_sections(self, content: str, file_type: str) -> List[Dict]:
        """åˆ†ææ®µè½çµæ§‹"""
        sections = []

        if file_type == 'markdown':
            # æå– Markdown æ¨™é¡Œ
            for match in re.finditer(r'^(#{1,6})\s+(.+)$', content, re.MULTILINE):
                sections.append({
                    'level': len(match.group(1)),
                    'title': match.group(2),
                    'position': match.start(),
                })

        elif file_type == 'yaml':
            # æå– YAML é ‚å±¤éµ
            try:
                data = yaml.safe_load(content)
                if isinstance(data, dict):
                    sections = [{'key': k, 'type': type(v).__name__} for k, v in data.items()]
            except:
                pass

        return sections

    def _calculate_hierarchy_depth(self, content: str, file_type: str) -> int:
        """è¨ˆç®—å±¤ç´šæ·±åº¦"""
        if file_type == 'markdown':
            levels = re.findall(r'^(#{1,6})', content, re.MULTILINE)
            return max(len(l) for l in levels) if levels else 0

        elif file_type in ['yaml', 'json']:
            # åŸºæ–¼ç¸®æ’è¨ˆç®—
            max_indent = 0
            for line in content.split('\n'):
                stripped = line.lstrip()
                if stripped:
                    indent = len(line) - len(stripped)
                    max_indent = max(max_indent, indent // 2)
            return max_indent

        return 0

    def _extract_metadata(self, content: str, file_type: str) -> Dict:
        """æå–å…ƒæ•¸æ“š"""
        metadata = {}

        if file_type == 'markdown' and content.startswith('---\n'):
            # æå– YAML frontmatter
            end = content.find('\n---\n', 4)
            if end > 0:
                try:
                    metadata = yaml.safe_load(content[4:end])
                except:
                    pass

        elif file_type == 'yaml':
            try:
                data = yaml.safe_load(content)
                if isinstance(data, dict):
                    for key in ['version', 'name', 'description', 'metadata']:
                        if key in data:
                            metadata[key] = data[key]
            except:
                pass

        return metadata

# ============================================================================
# é«˜éšæ±ºç­–å¼•æ“
# ============================================================================

class DecisionEngine:
    """
    é«˜éšæ±ºç­–å¼•æ“ï¼šåŸºæ–¼åˆ†æçµæœåšå‡ºæ•´åˆæ±ºç­–

    æ±ºç­–æµç¨‹ï¼š
    1. è©•ä¼°è³‡ç”¢å“è³ª
    2. è­˜åˆ¥ç›®æ¨™é ˜åŸŸ
    3. åˆ¤å®šæ•´åˆé¡å‹
    4. ç¢ºå®šç›®æ¨™ä½ç½®
    5. è©•ä¼°ç½®ä¿¡åº¦
    """

    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}

        # ç›®éŒ„æ˜ å°„è¦å‰‡
        self.directory_mapping = {
            "hlp": "03_refactor/hlp-executor-core/",
            "kg": "03_refactor/kg-builder/",
            "quantum": "03_refactor/quantum/",
            "machinenativenops": "03_refactor/machinenativenops/",
            "k8s": "02_integration/k8s/",
            "deconstruction": "01_deconstruction/",
            "integration": "02_integration/",
            "refactor": "03_refactor/",
            "config": "config/",
            "template": "templates/",
        }

        # æ•´åˆé¡å‹é–¾å€¼
        self.thresholds = {
            "full_integration": 0.7,
            "embedded_integration": 0.5,
            "reference_only": 0.3,
            "archive": 0.1,
        }

    def decide(self, analysis: AssetAnalysis) -> PlacementDecision:
        """åšå‡ºä½ç½®æ±ºç­–"""
        reasoning = []

        # æ­¥é©Ÿ1: è©•ä¼°å“è³ª
        quality = analysis.quality_score
        reasoning.append(f"å“è³ªè©•åˆ†: {quality:.2f}")

        # æ­¥é©Ÿ2: è­˜åˆ¥ä¸»è¦é ˜åŸŸ
        primary_domain = self._identify_primary_domain(analysis)
        reasoning.append(f"ä¸»è¦é ˜åŸŸ: {primary_domain}")

        # æ­¥é©Ÿ3: åˆ¤å®šæ•´åˆé¡å‹
        integration_type = self._determine_integration_type(analysis, quality)
        reasoning.append(f"æ•´åˆé¡å‹: {integration_type.value}")

        # æ­¥é©Ÿ4: ç¢ºå®šç›®æ¨™ä½ç½®
        target_dir, target_file = self._determine_target_location(
            analysis, primary_domain, integration_type
        )
        reasoning.append(f"ç›®æ¨™ä½ç½®: {target_dir}")

        # æ­¥é©Ÿ5: ç¢ºå®šåµŒå…¥ä½ç½® (å¦‚æœæ˜¯åµŒå…¥å¼æ•´åˆ)
        embedding_loc, embedding_sec = None, None
        if integration_type == IntegrationType.EMBEDDED_INTEGRATION:
            embedding_loc, embedding_sec = self._determine_embedding_location(
                analysis, target_dir
            )
            reasoning.append(f"åµŒå…¥ä½ç½®: {embedding_loc} / {embedding_sec}")

        # æ­¥é©Ÿ6: è¨ˆç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(analysis, integration_type)
        reasoning.append(f"æ±ºç­–ç½®ä¿¡åº¦: {confidence:.2f}")

        # æ­¥é©Ÿ7: ç”Ÿæˆå‚™é¸æ–¹æ¡ˆ
        alternatives = self._generate_alternatives(analysis, primary_domain)

        # æ­¥é©Ÿ8: åˆ¤æ–·æ˜¯å¦éœ€è¦äººå·¥å¯©æ ¸
        requires_review = confidence < 0.6 or quality < 0.5

        return PlacementDecision(
            asset_id=analysis.asset_id,
            integration_type=integration_type,
            target_directory=target_dir,
            target_filename=target_file,
            embedding_location=embedding_loc,
            embedding_section=embedding_sec,
            reasoning=reasoning,
            confidence=confidence,
            alternatives=alternatives,
            requires_manual_review=requires_review,
        )

    def _identify_primary_domain(self, analysis: AssetAnalysis) -> str:
        """è­˜åˆ¥ä¸»è¦é ˜åŸŸ"""
        if not analysis.domain_classification:
            return "unknown"

        # è¿”å›æœ€é«˜è©•åˆ†çš„é ˜åŸŸ
        return list(analysis.domain_classification.keys())[0]

    def _determine_integration_type(self, analysis: AssetAnalysis,
                                   quality: float) -> IntegrationType:
        """åˆ¤å®šæ•´åˆé¡å‹"""
        # åŸºæ–¼å“è³ªå’Œçµæ§‹åˆ¤å®š
        if quality >= self.thresholds["full_integration"]:
            # é«˜å“è³ªï¼Œå®Œæ•´æ•´åˆ
            if analysis.structure.line_count > 50:
                return IntegrationType.FULL_INTEGRATION
            else:
                return IntegrationType.EMBEDDED_INTEGRATION

        elif quality >= self.thresholds["embedded_integration"]:
            # ä¸­ç­‰å“è³ªï¼ŒåµŒå…¥å¼
            return IntegrationType.EMBEDDED_INTEGRATION

        elif quality >= self.thresholds["reference_only"]:
            # ä½å“è³ªï¼Œåƒ…å¼•ç”¨
            return IntegrationType.REFERENCE_ONLY

        else:
            # æ¥µä½å“è³ªï¼Œæ­¸æª”
            return IntegrationType.ARCHIVE

    def _determine_target_location(self, analysis: AssetAnalysis,
                                   primary_domain: str,
                                   integration_type: IntegrationType) -> Tuple[str, Optional[str]]:
        """ç¢ºå®šç›®æ¨™ä½ç½®"""
        # åŸºæ–¼é ˜åŸŸæ˜ å°„
        if primary_domain in self.directory_mapping:
            target_dir = self.directory_mapping[primary_domain]
        else:
            # é»˜èªä½ç½®
            if integration_type == IntegrationType.ARCHIVE:
                target_dir = "_legacy_scratch/archive/"
            else:
                target_dir = "03_refactor/misc/"

        # ç¢ºå®šç›®æ¨™æª”å
        if integration_type == IntegrationType.FULL_INTEGRATION:
            target_file = analysis.filename
        else:
            target_file = None

        return target_dir, target_file

    def _determine_embedding_location(self, analysis: AssetAnalysis,
                                      target_dir: str) -> Tuple[Optional[str], Optional[str]]:
        """ç¢ºå®šåµŒå…¥ä½ç½®"""
        # å°‹æ‰¾ç›®æ¨™ç›®éŒ„ä¸­çš„ä¸»è¦æ–‡ä»¶
        target_path = PLAYBOOKS_PATH / target_dir

        if target_path.exists():
            # å„ªå…ˆåµŒå…¥åˆ° INDEX.md æˆ– README.md
            for candidate in ["INDEX.md", "README.md", "index.yaml"]:
                if (target_path / candidate).exists():
                    return str(target_path / candidate), "## Related Assets"

        return None, None

    def _calculate_confidence(self, analysis: AssetAnalysis,
                             integration_type: IntegrationType) -> float:
        """è¨ˆç®—æ±ºç­–ç½®ä¿¡åº¦"""
        confidence = 0.5

        # åŸºæ–¼è©å½™åŒ¹é…æ•¸é‡
        vocab_count = len(analysis.vocabulary_matches)
        if vocab_count > 10:
            confidence += 0.2
        elif vocab_count > 5:
            confidence += 0.1

        # åŸºæ–¼å¼•ç”¨é©—è­‰
        valid_refs = sum(1 for r in analysis.reference_matches if r.exists)
        total_refs = len(analysis.reference_matches)
        if total_refs > 0:
            ref_ratio = valid_refs / total_refs
            confidence += ref_ratio * 0.2

        # åŸºæ–¼çµæ§‹å®Œæ•´æ€§
        if analysis.structure.has_frontmatter:
            confidence += 0.1

        return min(confidence, 1.0)

    def _generate_alternatives(self, analysis: AssetAnalysis,
                              primary_domain: str) -> List[Dict]:
        """ç”Ÿæˆå‚™é¸æ–¹æ¡ˆ"""
        alternatives = []

        # åŸºæ–¼æ¬¡è¦é ˜åŸŸç”Ÿæˆå‚™é¸
        for domain, score in list(analysis.domain_classification.items())[1:3]:
            if domain in self.directory_mapping:
                alternatives.append({
                    "target": self.directory_mapping[domain],
                    "reason": f"æ¬¡è¦é ˜åŸŸåŒ¹é…: {domain} ({score:.2f})",
                    "confidence": score,
                })

        return alternatives

# ============================================================================
# æš«å­˜å€è™•ç†å™¨ (ä¸»é¡)
# ============================================================================

class LegacyScratchProcessor:
    """
    æš«å­˜å€è™•ç†å™¨ï¼šå”èª¿æ‰€æœ‰è™•ç†çµ„ä»¶
    """

    def __init__(self, scratch_path: Optional[Path] = None):
        self.scratch_path = scratch_path or SCRATCH_PATH
        self.playbooks_path = PLAYBOOKS_PATH

        # åˆå§‹åŒ–çµ„ä»¶
        self.vocab_scanner = VocabularyScanner()
        self.ref_scanner = ReferenceScanner(BASE_PATH)
        self.struct_analyzer = StructureAnalyzer()
        self.decision_engine = DecisionEngine()

        # åˆå§‹åŒ–èªçŸ¥å¼•æ“ (å¦‚æœå¯ç”¨)
        if COGNITIVE_AVAILABLE:
            self.cognitive_engine = CognitiveEngine()
        else:
            self.cognitive_engine = None

    def scan_inventory(self) -> AssetInventory:
        """æƒææš«å­˜å€ç”Ÿæˆè³‡ç”¢æ¸…å–®"""
        print(f"ğŸ“‚ æƒææš«å­˜å€: {self.scratch_path}")

        assets = []
        by_type = defaultdict(int)
        by_stage = defaultdict(int)

        if not self.scratch_path.exists():
            print(f"âš ï¸ æš«å­˜å€ä¸å­˜åœ¨: {self.scratch_path}")
            return AssetInventory(
                scan_timestamp=datetime.now().isoformat(),
                scratch_path=str(self.scratch_path),
                total_assets=0,
                by_type={},
                by_stage={},
                assets=[],
            )

        for file in self.scratch_path.rglob("*"):
            if file.is_file() and not file.name.startswith('.'):
                # åˆ¤æ–·è³‡ç”¢é¡å‹
                asset_type = self._classify_asset_type(file)
                by_type[asset_type.value] += 1

                # åˆ¤æ–·è™•ç†éšæ®µ
                stage = self._determine_stage(file)
                by_stage[stage.value] += 1

                assets.append({
                    "filename": file.name,
                    "path": str(file.relative_to(self.scratch_path)),
                    "type": asset_type.value,
                    "stage": stage.value,
                    "size": file.stat().st_size,
                    "modified": datetime.fromtimestamp(file.stat().st_mtime).isoformat(),
                })

        print(f"   æ‰¾åˆ° {len(assets)} å€‹è³‡ç”¢")

        return AssetInventory(
            scan_timestamp=datetime.now().isoformat(),
            scratch_path=str(self.scratch_path),
            total_assets=len(assets),
            by_type=dict(by_type),
            by_stage=dict(by_stage),
            assets=assets,
        )

    def analyze_asset(self, filename: str, deep: bool = False) -> AssetAnalysis:
        """åˆ†æå–®ä¸€è³‡ç”¢"""
        filepath = self.scratch_path / filename

        if not filepath.exists():
            raise FileNotFoundError(f"è³‡ç”¢ä¸å­˜åœ¨: {filename}")

        print(f"ğŸ” åˆ†æè³‡ç”¢: {filename}")

        # è®€å–å…§å®¹
        content = filepath.read_text(encoding='utf-8', errors='ignore')

        # è¨ˆç®—å“ˆå¸Œ
        file_hash = hashlib.md5(content.encode()).hexdigest()

        # è©å½™æƒæ
        print("   è©å½™æƒæ...")
        vocab_matches = self.vocab_scanner.scan(content, filename)

        # å¼•ç”¨æƒæ
        print("   å¼•ç”¨æƒæ...")
        ref_matches = self.ref_scanner.scan(content, filepath)

        # çµæ§‹åˆ†æ
        print("   çµæ§‹åˆ†æ...")
        structure = self.struct_analyzer.analyze(content, filepath)

        # é ˜åŸŸåˆ†é¡
        domain_class = self.vocab_scanner.extract_domain_classification(vocab_matches)

        # è¨ˆç®—å“è³ªè©•åˆ†
        quality = self._calculate_quality_score(vocab_matches, ref_matches, structure)

        # å¦‚æœå•Ÿç”¨æ·±åº¦åˆ†æä¸”èªçŸ¥å¼•æ“å¯ç”¨
        if deep and self.cognitive_engine:
            print("   æ·±åº¦èªçŸ¥åˆ†æ...")
            cognitive_result = self.cognitive_engine.process({
                "text": content[:5000],  # é™åˆ¶é•·åº¦
                "filename": filename,
                "type": "asset_analysis",
            })
            # å¯ä»¥ç”¨èªçŸ¥çµæœé€²ä¸€æ­¥è±å¯Œåˆ†æ

        analysis = AssetAnalysis(
            asset_id=file_hash[:8],
            filename=filename,
            asset_type=self._classify_asset_type(filepath),
            file_hash=file_hash,
            vocabulary_matches=vocab_matches,
            reference_matches=ref_matches,
            structure=structure,
            domain_classification=domain_class,
            quality_score=quality,
            timestamp=datetime.now().isoformat(),
        )

        print(f"   å“è³ªè©•åˆ†: {quality:.2f}")
        print(f"   è©å½™åŒ¹é…: {len(vocab_matches)}")
        print(f"   å¼•ç”¨åŒ¹é…: {len(ref_matches)}")

        return analysis

    def decide_placement(self, analysis: AssetAnalysis) -> PlacementDecision:
        """ç‚ºè³‡ç”¢åšå‡ºä½ç½®æ±ºç­–"""
        print(f"ğŸ¯ æ±ºç­–: {analysis.filename}")

        decision = self.decision_engine.decide(analysis)

        print(f"   æ•´åˆé¡å‹: {decision.integration_type.value}")
        print(f"   ç›®æ¨™ä½ç½®: {decision.target_directory}")
        print(f"   ç½®ä¿¡åº¦: {decision.confidence:.2f}")

        if decision.requires_manual_review:
            print("   âš ï¸ éœ€è¦äººå·¥å¯©æ ¸")

        return decision

    def batch_process(self, filter_stage: Optional[ProcessingStage] = None) -> List[Dict]:
        """æ‰¹é‡è™•ç†æ‰€æœ‰è³‡ç”¢"""
        print("ğŸ”„ é–‹å§‹æ‰¹é‡è™•ç†...")

        inventory = self.scan_inventory()
        results = []

        for asset_info in inventory.assets:
            # éæ¿¾éšæ®µ
            if filter_stage and asset_info["stage"] != filter_stage.value:
                continue

            try:
                # åˆ†æ
                analysis = self.analyze_asset(asset_info["filename"])

                # æ±ºç­–
                decision = self.decide_placement(analysis)

                results.append({
                    "asset": asset_info["filename"],
                    "analysis_id": analysis.asset_id,
                    "decision": asdict(decision),
                    "status": "success",
                })

            except Exception as e:
                results.append({
                    "asset": asset_info["filename"],
                    "status": "error",
                    "error": str(e),
                })

        print(f"\nâœ… æ‰¹é‡è™•ç†å®Œæˆ: {len(results)} å€‹è³‡ç”¢")
        return results

    def _classify_asset_type(self, filepath: Path) -> AssetType:
        """åˆ†é¡è³‡ç”¢é¡å‹"""
        ext = filepath.suffix.lower()
        name_lower = filepath.name.lower()

        if ext == '.md':
            return AssetType.DOCUMENTATION
        elif ext in ['.yaml', '.yml']:
            if 'config' in name_lower:
                return AssetType.CONFIGURATION
            elif 'schema' in name_lower:
                return AssetType.SCHEMA
            else:
                return AssetType.DATA
        elif ext == '.json':
            if 'manifest' in name_lower:
                return AssetType.MANIFEST
            else:
                return AssetType.DATA
        elif ext in ['.py', '.ts', '.js']:
            return AssetType.CODE
        elif 'template' in name_lower:
            return AssetType.TEMPLATE
        else:
            return AssetType.UNKNOWN

    def _determine_stage(self, filepath: Path) -> ProcessingStage:
        """ç¢ºå®šè™•ç†éšæ®µ"""
        rel_path = str(filepath.relative_to(self.scratch_path))

        if rel_path.startswith('intake/'):
            return ProcessingStage.INTAKE
        elif rel_path.startswith('processing/'):
            return ProcessingStage.SCANNING
        elif rel_path.startswith('analyzed/'):
            return ProcessingStage.DECIDED
        elif rel_path.startswith('archive/'):
            return ProcessingStage.ARCHIVED
        else:
            return ProcessingStage.INTAKE

    def _calculate_quality_score(self, vocab_matches: List[VocabularyMatch],
                                 ref_matches: List[ReferenceMatch],
                                 structure: StructureAnalysis) -> float:
        """è¨ˆç®—å“è³ªè©•åˆ†"""
        score = 0.5  # åŸºç¤åˆ†

        # è©å½™è±å¯Œåº¦
        if len(vocab_matches) > 10:
            score += 0.15
        elif len(vocab_matches) > 5:
            score += 0.1

        # å¼•ç”¨æœ‰æ•ˆæ€§
        valid_refs = sum(1 for r in ref_matches if r.exists)
        if ref_matches:
            score += (valid_refs / len(ref_matches)) * 0.15

        # çµæ§‹å®Œæ•´æ€§
        if structure.has_frontmatter:
            score += 0.1
        if structure.hierarchy_depth > 0:
            score += 0.05
        if len(structure.sections) > 3:
            score += 0.05

        return min(score, 1.0)

# ============================================================================
# CLI å…¥å£
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Legacy Scratch Processor - æš«å­˜å€è™•ç†å¼•æ“",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # scan å‘½ä»¤
    scan_parser = subparsers.add_parser("scan", help="æƒææš«å­˜å€")
    scan_parser.add_argument("--output", "-o", help="è¼¸å‡ºæª”æ¡ˆ")

    # analyze å‘½ä»¤
    analyze_parser = subparsers.add_parser("analyze", help="åˆ†æè³‡ç”¢")
    analyze_parser.add_argument("--asset", "-a", required=True, help="è³‡ç”¢æª”å")
    analyze_parser.add_argument("--deep", action="store_true", help="æ·±åº¦åˆ†æ")
    analyze_parser.add_argument("--output", "-o", help="è¼¸å‡ºæª”æ¡ˆ")

    # decide å‘½ä»¤
    decide_parser = subparsers.add_parser("decide", help="ä½ç½®æ±ºç­–")
    decide_parser.add_argument("--asset", "-a", required=True, help="è³‡ç”¢æª”å")
    decide_parser.add_argument("--output", "-o", help="è¼¸å‡ºæª”æ¡ˆ")

    # batch å‘½ä»¤
    batch_parser = subparsers.add_parser("batch", help="æ‰¹é‡è™•ç†")
    batch_parser.add_argument("--all", action="store_true", help="è™•ç†æ‰€æœ‰")
    batch_parser.add_argument("--stage", choices=["intake", "scanning", "analyzing"],
                             help="éæ¿¾éšæ®µ")
    batch_parser.add_argument("--output", "-o", help="è¼¸å‡ºæª”æ¡ˆ")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    processor = LegacyScratchProcessor()

    if args.command == "scan":
        inventory = processor.scan_inventory()
        output = asdict(inventory)

        output_str = yaml.dump(output, allow_unicode=True, default_flow_style=False)

        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(output_str)
            print(f"\nâœ… æ¸…å–®å·²å„²å­˜: {args.output}")
        else:
            print("\n" + output_str)

    elif args.command == "analyze":
        analysis = processor.analyze_asset(args.asset, deep=args.deep)

        output = {
            "asset_id": analysis.asset_id,
            "filename": analysis.filename,
            "type": analysis.asset_type.value,
            "quality_score": analysis.quality_score,
            "domain_classification": analysis.domain_classification,
            "vocabulary_count": len(analysis.vocabulary_matches),
            "reference_count": len(analysis.reference_matches),
            "structure": asdict(analysis.structure),
        }

        output_str = yaml.dump(output, allow_unicode=True, default_flow_style=False)

        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(output_str)
            print(f"\nâœ… åˆ†æå·²å„²å­˜: {args.output}")
        else:
            print("\n" + output_str)

    elif args.command == "decide":
        analysis = processor.analyze_asset(args.asset)
        decision = processor.decide_placement(analysis)

        output = asdict(decision)
        output["integration_type"] = decision.integration_type.value

        output_str = yaml.dump(output, allow_unicode=True, default_flow_style=False)

        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(output_str)
            print(f"\nâœ… æ±ºç­–å·²å„²å­˜: {args.output}")
        else:
            print("\n" + output_str)

    elif args.command == "batch":
        stage = ProcessingStage(args.stage) if args.stage else None
        results = processor.batch_process(filter_stage=stage)

        output_str = yaml.dump(results, allow_unicode=True, default_flow_style=False)

        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(output_str)
            print(f"\nâœ… çµæœå·²å„²å­˜: {args.output}")
        else:
            print("\n" + output_str)

if __name__ == "__main__":
    main()
