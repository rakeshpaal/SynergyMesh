#!/usr/bin/env python3

"""
Enhanced Memory Synchronization System
å¢å¼ºè®°å¿†åŒæ­¥ç³»ç»Ÿ - åŒ…å«æ™ºèƒ½å†…å®¹åˆ†æã€çŸ¥è¯†å›¾è°±é›†æˆã€è·¨æ–‡ä»¶å…³è”æ£€æµ‹
"""

from __future__ import annotations

import json
import re
import subprocess
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from collections import defaultdict

# Use sha3-512 for cryptographic hashing (governance compliance)
try:
    import hashlib
    # Verify sha3_512 is available
    hashlib.sha3_512()
    HASH_ALGO = 'sha3_512'
except (AttributeError, ValueError):
    # Fallback to sha256 if sha3_512 not available
    HASH_ALGO = 'sha256'


@dataclass
class FileAnalysis:
    """æ–‡ä»¶åˆ†æç»“æœ"""
    path: str
    type: str  # config, spec, registry, doc, workflow
    category: str
    priority: int
    dependencies: List[str]
    impact_level: str  # high, medium, low
    content_hash: str
    entities: List[str]
    relationships: List[Dict[str, Any]]


@dataclass
class ChangeAnalysis:
    """å˜æ›´åˆ†æç»“æœ"""
    sha: str
    author: str
    subject: str
    timestamp: str
    added: List[FileAnalysis]
    modified: List[FileAnalysis]
    deleted: List[str]
    total_impact: str


class EnhancedMemorySync:
    """å¢å¼ºè®°å¿†åŒæ­¥ç³»ç»Ÿ"""
    
    def __init__(self, repo_root: Path):
        self.repo_root = repo_root
        self.controlplane = repo_root / "controlplane"
        self.workspace = repo_root / "workspace"
        self.knowledge_graph_path = repo_root / "workspace/docs/knowledge_graph.json"
        self.memory_index_path = repo_root / "workspace/docs/memory_index.json"
        
    def _run(self, cmd: List[str]) -> subprocess.CompletedProcess[str]:
        """æ‰§è¡Œå‘½ä»¤"""
        try:
            return subprocess.run(cmd, capture_output=True, text=True, check=True, cwd=self.repo_root)
        except subprocess.CalledProcessError as exc:
            message = exc.stderr.strip() or exc.stdout.strip() or "no output"
            raise RuntimeError(f"Command '{' '.join(cmd)}' failed ({exc.returncode}): {message}") from exc

    def _now(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

    def _get_file_hash(self, path: Path) -> str:
        """è·å–æ–‡ä»¶å†…å®¹å“ˆå¸Œ - ä½¿ç”¨ sha3-512 ç¬¦åˆæ²»ç†è§„èŒƒ"""
        try:
            content = path.read_text(encoding="utf-8")
            if HASH_ALGO == 'sha3_512':
                return hashlib.sha3_512(content.encode()).hexdigest()
            else:
                return hashlib.sha256(content.encode()).hexdigest()
        except (OSError, UnicodeDecodeError):
            return "unavailable"

    def _extract_dependencies(self, content: str, file_path: str) -> List[str]:
        """æå–æ–‡ä»¶ä¾èµ–å…³ç³»"""
        dependencies = []
        
        # Extract YAML imports/includes
        yaml_imports = re.findall(r'(?:import|include|extends):\s*["\']?([^\s"\']+)["\']?', content)
        dependencies.extend(yaml_imports)
        
        # Extract Python imports (relative to repo)
        py_imports = re.findall(r'from\s+([a-zA-Z0-9_.]+)\s+import', content)
        dependencies.extend(py_imports)
        
        # Extract file references with proper path patterns
        file_refs = re.findall(r'[\w\-./]+\.(?:yaml|yml|md|py|sh)', content)
        dependencies.extend(file_refs)
        
        # Remove duplicates and self-references
        dependencies = list(set(dep for dep in dependencies if dep != file_path))
        
        return dependencies

    def _analyze_file(self, file_path: str) -> FileAnalysis:
        """åˆ†ææ–‡ä»¶ç±»å‹å’Œå†…å®¹"""
        full_path = self.repo_root / file_path
        
        # ç¡®å®šæ–‡ä»¶ç±»å‹
        if file_path.endswith(('.yaml', '.yml')):
            if 'root.' in file_path and 'config' in file_path:
                file_type = 'config'
            elif 'specs.' in file_path:
                file_type = 'spec'
            elif 'registry.' in file_path:
                file_type = 'registry'
            elif 'workflow' in file_path or file_path.startswith('.github/workflows/'):
                file_type = 'workflow'
            else:
                file_type = 'config'
        elif file_path.endswith('.md'):
            file_type = 'doc'
        elif file_path.endswith('.py'):
            file_type = 'script'
        else:
            file_type = 'other'
        
        # ç¡®å®šç±»åˆ«
        if 'controlplane/' in file_path:
            category = 'controlplane'
        elif 'workspace/' in file_path:
            category = 'workspace'
        elif '.github/' in file_path:
            category = 'automation'
        else:
            category = 'root'
        
        # ç¡®å®šä¼˜å…ˆçº§
        priority_map = {
            'config': 100,
            'spec': 90,
            'registry': 95,
            'workflow': 80,
            'doc': 50,
            'script': 70
        }
        priority = priority_map.get(file_type, 30)
        
        # æå–å®ä½“å’Œå…³ç³»
        entities = []
        relationships = []
        dependencies = []
        
        try:
            content = full_path.read_text(encoding="utf-8")
            
            # æå–URNå¼•ç”¨ - ä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼åŒ¹é…å¹³å°è§„èŒƒ
            # Pattern: urn:axiom:module:<name>:<version>
            urns = re.findall(r'urn:axiom:(?:module|device|namespace):[a-zA-Z0-9_-]+:[a-zA-Z0-9._-]+', content)
            entities.extend(urns)
            
            # æå–æ¨¡å—å
            modules = re.findall(r'\b[A-Za-z][A-Za-z0-9_-]+\.(?:yaml|yml|py|md)\b', content)
            entities.extend(modules)
            
            # æå–ä¾èµ–å…³ç³»
            dependencies = self._extract_dependencies(content, file_path)
            
            # åˆ†æä¾èµ–å…³ç³»
            if file_type in ['config', 'spec']:
                for entity in entities:
                    if entity != file_path:
                        relationships.append({
                            'type': 'depends_on',
                            'target': entity,
                            'strength': 'strong' if entity.startswith('urn:') else 'weak'
                        })
            
        except (OSError, UnicodeDecodeError):
            pass
        
        # ç¡®å®šå½±å“çº§åˆ«
        impact_map = {
            'config': 'high',
            'spec': 'high',
            'registry': 'high',
            'workflow': 'medium',
            'doc': 'low'
        }
        impact_level = impact_map.get(file_type, 'low')
        
        return FileAnalysis(
            path=file_path,
            type=file_type,
            category=category,
            priority=priority,
            dependencies=dependencies,  # Now properly populated
            impact_level=impact_level,
            content_hash=self._get_file_hash(full_path),
            entities=list(set(entities)),
            relationships=relationships
        )

    def _get_changed_files(self) -> Dict[str, Any]:
        """è·å–å˜æ›´æ–‡ä»¶ä¿¡æ¯"""
        result = self._run([
            "git", "log", "-1", "--name-status", "--pretty=format:%H%n%an%n%s"
        ])
        lines = [line for line in result.stdout.splitlines() if line.strip()]
        
        if len(lines) < 3:
            return {
                "sha": "", "author": "", "subject": "", 
                "files": [], "added": 0, "modified": 0, "deleted": 0
            }
        
        sha, author, subject = lines[0], lines[1], lines[2]
        files_info = []
        
        for line in lines[3:]:
            # Fixed: Use proper tab separator
            parts = line.split("\t")
            if len(parts) >= 2:
                status, file_path = parts[0], parts[1]
                files_info.append((status, file_path))
        
        # Fixed: Return actual subject instead of hardcoded "subject"
        return {
            "sha": sha, "author": author, "subject": subject,
            "files": files_info, "added": 0, "modified": 0, "deleted": 0
        }

    def _analyze_changes(self) -> ChangeAnalysis:
        """åˆ†æå˜æ›´"""
        files_info = self._get_changed_files()
        timestamp = self._now()
        
        added = []
        modified = []
        deleted = []
        
        high_impact_count = 0
        
        for status, file_path in files_info["files"]:
            if status.startswith("A"):
                analysis = self._analyze_file(file_path)
                added.append(analysis)
                if analysis.impact_level == 'high':
                    high_impact_count += 1
            elif status.startswith("M"):
                analysis = self._analyze_file(file_path)
                modified.append(analysis)
                if analysis.impact_level == 'high':
                    high_impact_count += 1
            elif status.startswith("D"):
                deleted.append(file_path)
        
        # ç¡®å®šæ€»ä½“å½±å“çº§åˆ«
        if high_impact_count >= 3:
            total_impact = "critical"
        elif high_impact_count >= 1:
            total_impact = "high"
        elif len(added) + len(modified) >= 5:
            total_impact = "medium"
        else:
            total_impact = "low"
        
        return ChangeAnalysis(
            sha=files_info["sha"],
            author=files_info["author"],
            subject=files_info["subject"],
            timestamp=timestamp,
            added=added,
            modified=modified,
            deleted=deleted,
            total_impact=total_impact
        )

    def _deduplicate_relationships(self, relationships: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡å…³ç³» - ä¿ç•™æœ€æ–°çš„å…³ç³»è®°å½•"""
        # Use (source, target, type) as key, keep latest by timestamp
        rel_dict = {}
        for rel in relationships:
            key = (rel["source"], rel["target"], rel["type"])
            if key not in rel_dict or rel["timestamp"] > rel_dict[key]["timestamp"]:
                rel_dict[key] = rel
        
        return list(rel_dict.values())

    def _update_knowledge_graph(self, analysis: ChangeAnalysis) -> None:
        """æ›´æ–°çŸ¥è¯†å›¾è°±"""
        # åŠ è½½ç°æœ‰å›¾è°±
        graph = {"entities": {}, "relationships": [], "last_updated": ""}
        
        if self.knowledge_graph_path.exists():
            try:
                graph = json.loads(self.knowledge_graph_path.read_text(encoding="utf-8"))
            except (json.JSONDecodeError, UnicodeDecodeError):
                pass
        
        # æ›´æ–°å®ä½“
        for file_analysis in analysis.added + analysis.modified:
            entity_id = file_analysis.path
            graph["entities"][entity_id] = {
                "type": file_analysis.type,
                "category": file_analysis.category,
                "priority": file_analysis.priority,
                "impact_level": file_analysis.impact_level,
                "last_modified": analysis.timestamp,
                "content_hash": file_analysis.content_hash,
                "entities": file_analysis.entities,
                "dependencies": file_analysis.dependencies
            }
        
        # æ›´æ–°å…³ç³»
        for file_analysis in analysis.added + analysis.modified:
            for relationship in file_analysis.relationships:
                rel = {
                    "source": file_analysis.path,
                    "target": relationship["target"],
                    "type": relationship["type"],
                    "strength": relationship["strength"],
                    "timestamp": analysis.timestamp
                }
                graph["relationships"].append(rel)
        
        # å»é‡å…³ç³»
        graph["relationships"] = self._deduplicate_relationships(graph["relationships"])
        
        # æ¸…ç†åˆ é™¤çš„å®ä½“
        for deleted_file in analysis.deleted:
            if deleted_file in graph["entities"]:
                del graph["entities"][deleted_file]
            graph["relationships"] = [
                rel for rel in graph["relationships"] 
                if rel["source"] != deleted_file and rel["target"] != deleted_file
            ]
        
        graph["last_updated"] = analysis.timestamp
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.knowledge_graph_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å›¾è°±
        self.knowledge_graph_path.write_text(
            json.dumps(graph, indent=2, ensure_ascii=False), 
            encoding="utf-8"
        )

    def _generate_insights(self, analysis: ChangeAnalysis) -> List[str]:
        """ç”Ÿæˆæ™ºèƒ½æ´å¯Ÿ"""
        insights = []
        
        # åˆ†æå˜æ›´æ¨¡å¼
        total_changes = len(analysis.added) + len(analysis.modified)
        
        if total_changes >= 10:
            insights.append(f"ğŸ”„ å¤§è§„æ¨¡å˜æ›´ï¼šæœ¬æ¬¡æäº¤åŒ…å« {total_changes} ä¸ªæ–‡ä»¶å˜æ›´")
        
        # åˆ†æé…ç½®å˜æ›´
        config_changes = [f for f in analysis.added + analysis.modified if f.type == 'config']
        if config_changes:
            insights.append(f"âš™ï¸ é…ç½®å˜æ›´ï¼š{len(config_changes)} ä¸ªé…ç½®æ–‡ä»¶è¢«ä¿®æ”¹")
            
            # æ£€æŸ¥å…³é”®é…ç½®
            critical_configs = [f for f in config_changes if 'root.' in f.path]
            if critical_configs:
                insights.append(f"ğŸ”§ æ ¹é…ç½®å˜æ›´ï¼š{len(critical_configs)} ä¸ªæ ¹é…ç½®æ–‡ä»¶è¢«å½±å“")
        
        # åˆ†æè§„èŒƒå˜æ›´
        spec_changes = [f for f in analysis.added + analysis.modified if f.type == 'spec']
        if spec_changes:
            insights.append(f"ğŸ“‹ è§„èŒƒå˜æ›´ï¼š{len(spec_changes)} ä¸ªè§„èŒƒæ–‡ä»¶è¢«æ›´æ–°")
        
        # åˆ†æä¾èµ–å…³ç³»
        all_relationships = []
        for file_analysis in analysis.added + analysis.modified:
            all_relationships.extend(file_analysis.relationships)
        
        if all_relationships:
            strong_deps = [r for r in all_relationships if r.get('strength') == 'strong']
            if strong_deps:
                insights.append(f"ğŸ”— å¼ºä¾èµ–ï¼šå‘ç° {len(strong_deps)} ä¸ªå¼ºä¾èµ–å…³ç³»")
        
        # å½±å“è¯„ä¼°
        if analysis.total_impact == 'critical':
            insights.append("ğŸš¨ å…³é”®å½±å“ï¼šæœ¬æ¬¡å˜æ›´å¯èƒ½å½±å“ç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½")
        elif analysis.total_impact == 'high':
            insights.append("âš ï¸ é«˜å½±å“ï¼šå»ºè®®è¿›è¡Œå……åˆ†æµ‹è¯•")
        
        return insights

    def _enhanced_memory_body(self, analysis: ChangeAnalysis) -> str:
        """ç”Ÿæˆå¢å¼ºçš„è®°å¿†å†…å®¹"""
        insights = self._generate_insights(analysis)
        
        sections = [
            f"### ğŸ§  å¢å¼ºè®°å¿†æ›´æ–° ({analysis.timestamp})",
            f"**å˜æ›´å½±å“çº§åˆ«**: {analysis.total_impact.upper()}",
            f"**Commit**: {analysis.sha} ({analysis.subject})",
            f"**ä½œè€…**: {analysis.author}",
            "",
            "**ğŸ“Š å˜æ›´ç»Ÿè®¡**:",
            f"- æ–°å¢æ–‡ä»¶: {len(analysis.added)}",
            f"- ä¿®æ”¹æ–‡ä»¶: {len(analysis.modified)}",
            f"- åˆ é™¤æ–‡ä»¶: {len(analysis.deleted)}",
            ""
        ]
        
        # æ·»åŠ æ™ºèƒ½æ´å¯Ÿ
        if insights:
            sections.append("**ğŸ” æ™ºèƒ½æ´å¯Ÿ**:")
            for insight in insights:
                sections.append(f"- {insight}")
            sections.append("")
        
        # æ·»åŠ é«˜ä¼˜å…ˆçº§å˜æ›´è¯¦æƒ…
        high_priority_changes = [
            f for f in analysis.added + analysis.modified 
            if f.priority >= 90
        ]
        
        if high_priority_changes:
            sections.append("**âš¡ é«˜ä¼˜å…ˆçº§å˜æ›´**:")
            for file_analysis in high_priority_changes[:5]:
                sections.append(f"- `{file_analysis.path}` ({file_analysis.type}, å½±å“çº§åˆ«: {file_analysis.impact_level})")
            
            if len(high_priority_changes) > 5:
                sections.append(f"- ... è¿˜æœ‰ {len(high_priority_changes) - 5} ä¸ªé«˜ä¼˜å…ˆçº§å˜æ›´")
            sections.append("")
        
        # æ·»åŠ å®ä½“å…³ç³»æ‘˜è¦
        all_entities = set()
        for file_analysis in analysis.added + analysis.modified:
            all_entities.update(file_analysis.entities)
        
        if all_entities:
            sections.append("**ğŸ·ï¸ ç›¸å…³å®ä½“**:")
            entities_list = sorted(list(all_entities))[:10]
            for entity in entities_list:
                sections.append(f"- `{entity}`")
            
            if len(all_entities) > 10:
                sections.append(f"- ... è¿˜æœ‰ {len(all_entities) - 10} ä¸ªå®ä½“")
            sections.append("")
        
        # æ·»åŠ çŸ¥è¯†å›¾è°±ç»Ÿè®¡
        if self.knowledge_graph_path.exists():
            try:
                graph = json.loads(self.knowledge_graph_path.read_text(encoding="utf-8"))
                sections.append("**ğŸ“Š çŸ¥è¯†å›¾è°±ç»Ÿè®¡**:")
                sections.append(f"- å®ä½“æ€»æ•°: {len(graph.get('entities', {}))}")
                sections.append(f"- å…³ç³»æ€»æ•°: {len(graph.get('relationships', []))}")
                sections.append(f"- æœ€åæ›´æ–°: {graph.get('last_updated', 'æœªçŸ¥')}")
                sections.append("")
            except:
                pass
        
        return "\n".join(sections)

    def _update_section(self, path: Path, start: str, end: str, body: str) -> bool:
        """æ›´æ–°æ–‡æ¡£ç‰‡æ®µ - æ”¹è¿›çš„æ­£åˆ™æ›¿æ¢é€»è¾‘"""
        try:
            # Ensure path exists
            if not path.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                path.write_text("", encoding="utf-8")
            
            content = path.read_text(encoding="utf-8")
            block = f"{start}\n{body}\n{end}"
            
            if start in content and end in content:
                # Use non-greedy match with anchored markers
                pattern = rf"{re.escape(start)}.*?{re.escape(end)}"
                new_content = re.sub(
                    pattern,
                    block,
                    content,
                    count=1,  # Only replace first occurrence
                    flags=re.DOTALL,
                )
            else:
                new_content = f"{block}\n\n{content}"
            
            if new_content != content:
                path.write_text(new_content, encoding="utf-8")
                return True
        except (OSError, UnicodeDecodeError) as e:
            print(f"Warning: Failed to update {path}: {e}")
        
        return False

    def sync_memory(self) -> bool:
        """æ‰§è¡Œè®°å¿†åŒæ­¥"""
        analysis = self._analyze_changes()
        
        # æ›´æ–°çŸ¥è¯†å›¾è°±
        self._update_knowledge_graph(analysis)
        
        # æ›´æ–°è®°å¿†æ–‡ä»¶
        project_memory = self.repo_root / "controlplane/governance/docs/PROJECT_MEMORY.md"
        conversation_log = self.repo_root / "workspace/projects/CONVERSATION_LOG.md"
        
        updated = False
        
        # æ›´æ–°é¡¹ç›®è®°å¿†
        enhanced_body = self._enhanced_memory_body(analysis)
        updated |= self._update_section(
            project_memory,
            "<!-- AUTO-MEMORY-UPDATE:START -->",
            "<!-- AUTO-MEMORY-UPDATE:END -->",
            enhanced_body,
        )
        
        # æ›´æ–°å¯¹è¯è®°å½•
        conversation_body = "\n".join([
            f"### {analysis.timestamp} - å¢å¼ºè®°å¿†æ›´æ–°",
            f"- ç›®æ ‡: æ™ºèƒ½åˆ†æå˜æ›´å¹¶æ›´æ–°çŸ¥è¯†å›¾è°±",
            f"- å½±å“: {analysis.total_impact}",
            f"- å®ä½“è¯†åˆ«: {len(set().union(*[f.entities for f in analysis.added + analysis.modified]))}",
            f"- å…³ç³»å‘ç°: {sum(len(f.relationships) for f in analysis.added + analysis.modified)}",
        ])
        
        updated |= self._update_section(
            conversation_log,
            "<!-- AUTO-CONVERSATION-LOG:START -->",
            "<!-- AUTO-CONVERSATION-LOG:END -->",
            conversation_body,
        )
        
        if updated:
            print("Enhanced memory synchronization completed.")
            print(f"Knowledge graph updated: {len(analysis.added + analysis.modified)} files analyzed")
        else:
            print("No memory updates required.")
        
        return updated


def get_repo_root() -> Path:
    """è·å–ä»“åº“æ ¹ç›®å½• - ä½¿ç”¨ git å‘½ä»¤è€Œéç¡¬ç¼–ç è·¯å¾„"""
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--show-toplevel"],
            capture_output=True,
            text=True,
            check=True
        )
        return Path(result.stdout.strip())
    except subprocess.CalledProcessError:
        # Fallback to relative path if git command fails
        return Path(__file__).resolve().parents[4]


def main():
    """ä¸»å‡½æ•°"""
    repo_root = get_repo_root()
    
    sync = EnhancedMemorySync(repo_root)
    sync.sync_memory()


if __name__ == "__main__":
    main()