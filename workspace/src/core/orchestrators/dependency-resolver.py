#!/usr/bin/env python3
"""
Dependency Resolver - æ™ºèƒ½ä¾è³´è§£æå’Œç®¡ç†ç³»çµ±

åŠŸèƒ½ï¼š
1. ä¾è³´åœ–æ§‹å»º
2. å¾ªç’°ä¾è³´æª¢æ¸¬
3. æ‹“æ’²æ’åº
4. å„ªå…ˆç´šè¨ˆç®—
5. ä¸¦è¡ŒåŒ–åˆ†æ
6. æ€§èƒ½å„ªåŒ–å»ºè­°
"""

import logging
from typing import Dict, List, Set, Tuple, Optional
from dataclasses import dataclass, field
from collections import defaultdict, deque
import json

logger = logging.getLogger(__name__)


@dataclass
class DependencyNode:
    """ä¾è³´åœ–ä¸­çš„ç¯€é»"""
    component_id: str
    component_type: str
    priority: int = 0
    weight: float = 1.0
    dependencies: List[str] = field(default_factory=list)
    dependent_on: List[str] = field(default_factory=list)


@dataclass
class ExecutionPhase:
    """åŸ·è¡Œéšæ®µ"""
    phase_number: int
    components: List[str]
    can_parallel: bool
    estimated_duration_ms: float
    dependency_count: int


class DependencyResolver:
    """
    æ™ºèƒ½ä¾è³´è§£æå™¨

    æä¾›ï¼š
    - ä¾è³´åœ–åˆ†æ
    - æ‹“æ’²æ’åº
    - ä¸¦è¡ŒåŒ–æª¢æ¸¬
    - é—œéµè·¯å¾‘åˆ†æ
    - å„ªåŒ–å»ºè­°
    """

    def __init__(self):
        """åˆå§‹åŒ–è§£æå™¨"""
        self.nodes: Dict[str, DependencyNode] = {}
        self.graph: Dict[str, Set[str]] = defaultdict(set)
        self.reverse_graph: Dict[str, Set[str]] = defaultdict(set)
        self.memo_cache: Dict[str, List[str]] = {}
        logger.info("ğŸ”„ DependencyResolver åˆå§‹åŒ–å®Œæˆ")

    def add_component(
        self,
        component_id: str,
        component_type: str,
        priority: int = 0,
        weight: float = 1.0
    ) -> bool:
        """æ·»åŠ çµ„ä»¶"""
        try:
            self.nodes[component_id] = DependencyNode(
                component_id=component_id,
                component_type=component_type,
                priority=priority,
                weight=weight
            )
            return True
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ çµ„ä»¶å¤±æ•— {component_id}: {e}")
            return False

    def add_dependency(
        self,
        from_component: str,
        to_component: str
    ) -> bool:
        """æ·»åŠ ä¾è³´é—œä¿‚"""
        try:
            # é©—è­‰çµ„ä»¶å­˜åœ¨
            if from_component not in self.nodes or to_component not in self.nodes:
                raise ValueError("çµ„ä»¶ä¸å­˜åœ¨")

            # æª¢æŸ¥å¾ªç’°ä¾è³´
            if self._would_create_cycle(from_component, to_component):
                raise ValueError(f"å¾ªç’°ä¾è³´: {from_component} â†’ {to_component}")

            self.graph[from_component].add(to_component)
            self.reverse_graph[to_component].add(from_component)

            # æ›´æ–°ç¯€é»å¼•ç”¨
            self.nodes[from_component].dependencies.append(to_component)
            self.nodes[to_component].dependent_on.append(from_component)

            # æ¸…é™¤å¿«å–
            self.memo_cache.clear()

            logger.info(f"âœ… ä¾è³´å·²æ·»åŠ : {from_component} â†’ {to_component}")
            return True

        except ValueError as e:
            logger.error(f"âŒ æ·»åŠ ä¾è³´å¤±æ•—: {e}")
            return False

    def _would_create_cycle(
        self,
        from_component: str,
        to_component: str,
        visited: Optional[Set[str]] = None
    ) -> bool:
        """æª¢æŸ¥æ˜¯å¦æœƒå‰µå»ºå¾ªç’°"""
        if visited is None:
            visited = set()

        if from_component in visited:
            return True

        if from_component == to_component:
            return True

        visited.add(from_component)

        for neighbor in self.graph.get(from_component, set()):
            if self._would_create_cycle(neighbor, to_component, visited.copy()):
                return True

        return False

    def topological_sort(
        self,
        component_ids: Optional[List[str]] = None
    ) -> List[str]:
        """æ‹“æ’²æ’åº"""
        if component_ids is None:
            component_ids = list(self.nodes.keys())

        # å¿«å–æª¢æŸ¥
        cache_key = ",".join(sorted(component_ids))
        if cache_key in self.memo_cache:
            return self.memo_cache[cache_key]

        # Kahn æ¼”ç®—æ³•
        in_degree = {comp: 0 for comp in component_ids}
        for comp in component_ids:
            for dep in self.graph.get(comp, set()):
                if dep in in_degree:
                    in_degree[dep] += 1

        queue = deque([comp for comp in component_ids if in_degree[comp] == 0])
        result = []

        while queue:
            # æŒ‰å„ªå…ˆç´šæ’åº
            queue_list = sorted(
                queue,
                key=lambda x: (self.nodes[x].priority, x),
                reverse=True
            )
            current = queue_list.pop(0)
            queue = deque(queue_list)
            result.append(current)

            for dep in self.graph.get(current, set()):
                if dep in in_degree:
                    in_degree[dep] -= 1
                    if in_degree[dep] == 0:
                        queue.append(dep)

        self.memo_cache[cache_key] = result
        logger.info(f"âœ… æ‹“æ’²æ’åºå®Œæˆ: {result}")

        return result

    def get_execution_phases(
        self,
        component_ids: Optional[List[str]] = None
    ) -> List[ExecutionPhase]:
        """ç²å–åŸ·è¡Œéšæ®µï¼ˆä¸¦è¡ŒåŒ–åˆ†æï¼‰"""
        sorted_components = self.topological_sort(component_ids)

        phases = []
        processed = set()
        phase_number = 1

        while len(processed) < len(sorted_components):
            current_phase = []

            for comp in sorted_components:
                if comp in processed:
                    continue

                # æª¢æŸ¥ä¾è³´æ˜¯å¦éƒ½å·²è™•ç†
                dependencies = self.graph.get(comp, set())
                if all(dep in processed for dep in dependencies):
                    current_phase.append(comp)

            if not current_phase:
                # å¦‚æœæ²’æœ‰å¯è™•ç†çš„çµ„ä»¶ï¼Œå¯èƒ½æœ‰å•é¡Œ
                logger.warning("âš ï¸ ç„¡æ³•ç¹¼çºŒåŸ·è¡Œæ‹“æ’²æ’åº")
                break

            # è¨ˆç®—åŸ·è¡Œæ™‚é–“
            estimated_time = sum(
                self.nodes[comp].weight * 100 for comp in current_phase
            )

            phase = ExecutionPhase(
                phase_number=phase_number,
                components=current_phase,
                can_parallel=len(current_phase) > 1,
                estimated_duration_ms=estimated_time,
                dependency_count=len([
                    d for comp in current_phase
                    for d in self.graph.get(comp, set())
                ])
            )

            phases.append(phase)
            processed.update(current_phase)
            phase_number += 1

        logger.info(f"âœ… åŸ·è¡Œéšæ®µåˆ†æå®Œæˆ: {len(phases)} å€‹éšæ®µ")
        return phases

    def get_critical_path(self) -> List[str]:
        """ç²å–é—œéµè·¯å¾‘"""
        component_ids = list(self.nodes.keys())

        if not component_ids:
            return []

        # ç°¡åŒ–ç‰ˆæœ¬ï¼šæœ€é•·è·¯å¾‘
        def dfs_longest_path(node: str, visited: Set[str]) -> Tuple[float, List[str]]:
            if node not in self.graph or not self.graph[node]:
                return self.nodes[node].weight, [node]

            max_weight = self.nodes[node].weight
            longest = [node]

            for dep in self.graph[node]:
                if dep not in visited:
                    visited.add(dep)
                    weight, path = dfs_longest_path(dep, visited)
                    if weight + self.nodes[node].weight > max_weight:
                        max_weight = weight + self.nodes[node].weight
                        longest = [node] + path
                    visited.remove(dep)

            return max_weight, longest

        visited = set()
        _, critical_path = dfs_longest_path(component_ids[0], visited)

        logger.info(f"âœ… é—œéµè·¯å¾‘: {critical_path}")
        return critical_path

    def get_parallelization_analysis(
        self,
        component_ids: Optional[List[str]] = None
    ) -> Dict[str, float]:
        """ç²å–ä¸¦è¡ŒåŒ–åˆ†æ"""
        phases = self.get_execution_phases(component_ids)

        sequential_time = sum(p.estimated_duration_ms for p in phases)
        parallel_time = max(p.estimated_duration_ms for p in phases)
        parallelization_factor = sequential_time / max(parallel_time, 1)

        return {
            "total_components": len(component_ids or self.nodes),
            "execution_phases": len(phases),
            "sequential_time_ms": sequential_time,
            "parallel_time_ms": parallel_time,
            "parallelization_factor": parallelization_factor,
            "potential_speedup": f"{parallelization_factor:.2f}x"
        }

    def get_dependency_stats(self) -> Dict[str, any]:
        """ç²å–ä¾è³´çµ±è¨ˆ"""
        total_edges = sum(len(deps) for deps in self.graph.values())
        avg_degree = total_edges / max(len(self.nodes), 1)

        return {
            "total_components": len(self.nodes),
            "total_dependencies": total_edges,
            "average_dependency_count": avg_degree,
            "max_dependency_depth": self._calculate_max_depth(),
            "circular_dependencies": 0  # æˆ‘å€‘æª¢æ¸¬ä¸¦é˜²æ­¢äº†å¾ªç’°
        }

    def _calculate_max_depth(self) -> int:
        """è¨ˆç®—æœ€å¤§ä¾è³´æ·±åº¦"""
        max_depth = 0

        for component in self.nodes:
            depth = self._calculate_depth(component, set())
            max_depth = max(max_depth, depth)

        return max_depth

    def _calculate_depth(
        self,
        component: str,
        visited: Set[str]
    ) -> int:
        """éæ­¸è¨ˆç®—æ·±åº¦"""
        if component in visited:
            return 0

        visited.add(component)

        dependencies = self.graph.get(component, set())
        if not dependencies:
            return 1

        max_dep_depth = max(
            self._calculate_depth(dep, visited.copy())
            for dep in dependencies
        )

        return 1 + max_dep_depth

    def get_optimization_recommendations(self) -> List[str]:
        """ç²å–å„ªåŒ–å»ºè­°"""
        recommendations = []
        stats = self.get_dependency_stats()
        analysis = self.get_parallelization_analysis()

        # æª¢æŸ¥ä¾è³´è¤‡é›œæ€§
        if stats["average_dependency_count"] > 5:
            recommendations.append(
                "âš ï¸ ä¾è³´è¤‡é›œæ€§é«˜ï¼Œè€ƒæ…®é‡æ§‹ä»¥æ¸›å°‘è€¦åˆ"
            )

        # æª¢æŸ¥ä¸¦è¡ŒåŒ–æ©Ÿæœƒ
        if analysis["parallelization_factor"] < 2:
            recommendations.append(
                "ğŸ’¡ ä½ä¸¦è¡ŒåŒ–æ©Ÿæœƒï¼Œè€ƒæ…®å„ªåŒ–ä¾è³´é—œä¿‚"
            )

        # æª¢æŸ¥æ·±åº¦
        if stats["max_dependency_depth"] > 5:
            recommendations.append(
                "ğŸ”— ä¾è³´æ·±åº¦æ·±ï¼Œè€ƒæ…®å¼•å…¥ä¸­é–“å±¤"
            )

        if not recommendations:
            recommendations.append(
                "âœ… ä¾è³´çµæ§‹å¥åº·ï¼Œç„¡ç‰¹åˆ¥å»ºè­°"
            )

        return recommendations

    def export_graph(self) -> Dict[str, any]:
        """å°å‡ºä¾è³´åœ–"""
        return {
            "nodes": [
                {
                    "id": node.component_id,
                    "type": node.component_type,
                    "priority": node.priority,
                    "weight": node.weight
                }
                for node in self.nodes.values()
            ],
            "edges": [
                {"from": from_comp, "to": to_comp}
                for from_comp in self.graph
                for to_comp in self.graph[from_comp]
            ],
            "statistics": self.get_dependency_stats(),
            "recommendations": self.get_optimization_recommendations()
        }


# å°å‡º
__all__ = [
    "DependencyResolver",
    "DependencyNode",
    "ExecutionPhase"
]
