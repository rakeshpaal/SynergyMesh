"""
AI Hallucination Detector (AI å¹»è¦ºæª¢æ¸¬å™¨)

Phase 5 Component: Detects and prevents AI-generated hallucinations in code and outputs.

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»£ç¢¼å¹»è¦ºæª¢æ¸¬ - è­˜åˆ¥ AI ç”Ÿæˆçš„çœ‹ä¼¼æ­£ç¢ºä½†å¯¦éš›æœ‰å•é¡Œçš„ä»£ç¢¼
2. ç½®ä¿¡åº¦è©•ä¼° - è©•ä¼° AI è¼¸å‡ºçš„å¯é æ€§
3. äº‹å¯¦é©—è­‰ - é©—è­‰ AI ç”Ÿæˆå…§å®¹çš„æº–ç¢ºæ€§
4. å®‰å…¨æ¼æ´æª¢æ¸¬ - è­˜åˆ¥ AI å¯èƒ½å¿½ç•¥çš„å®‰å…¨å•é¡Œ

Design Philosophy: "è®“ç¨‹å¼æœå‹™æ–¼äººé¡ï¼Œè€Œéäººé¡æœå‹™æ–¼ç¨‹å¼"
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Optional
import re
import hashlib


class HallucinationType(Enum):
    """Types of AI hallucinations (AI å¹»è¦ºé¡å‹)"""
    CODE_SYNTAX = "code_syntax"           # èªæ³•å¹»è¦º
    SECURITY_FLAW = "security_flaw"       # å®‰å…¨æ¼æ´å¹»è¦º
    LOGIC_ERROR = "logic_error"           # é‚è¼¯éŒ¯èª¤å¹»è¦º
    API_MISUSE = "api_misuse"             # API èª¤ç”¨å¹»è¦º
    CONTEXT_MISMATCH = "context_mismatch" # ä¸Šä¸‹æ–‡ä¸åŒ¹é…
    OVERCONFIDENCE = "overconfidence"     # éåº¦è‡ªä¿¡å¹»è¦º
    INCOMPLETE = "incomplete"             # ä¸å®Œæ•´å¯¦ç¾
    FALSE_CLAIM = "false_claim"           # è™›å‡è²æ˜


class SeverityLevel(Enum):
    """Severity levels for detected issues (å•é¡Œåš´é‡ç¨‹åº¦)"""
    CRITICAL = "critical"   # è‡´å‘½ - å¿…é ˆç«‹å³è™•ç†
    HIGH = "high"           # é«˜ - æ‡‰ç›¡å¿«è™•ç†
    MEDIUM = "medium"       # ä¸­ - éœ€è¦é—œæ³¨
    LOW = "low"             # ä½ - å»ºè­°æ”¹é€²
    INFO = "info"           # è³‡è¨Š - åƒ…ä¾›åƒè€ƒ


@dataclass
class HallucinationDetection:
    """Detected hallucination record (æª¢æ¸¬åˆ°çš„å¹»è¦ºè¨˜éŒ„)"""
    detection_id: str
    hallucination_type: HallucinationType
    severity: SeverityLevel
    description: str
    location: Optional[str] = None
    suggested_fix: Optional[str] = None
    confidence: float = 0.0  # æª¢æ¸¬ç½®ä¿¡åº¦ 0-1
    detected_at: datetime = field(default_factory=datetime.now)
    metadata: dict = field(default_factory=dict)


@dataclass
class ValidationResult:
    """Code validation result (ä»£ç¢¼é©—è­‰çµæœ)"""
    is_valid: bool
    overall_confidence: float  # æ•´é«”ç½®ä¿¡åº¦ 0-1
    hallucinations: list[HallucinationDetection] = field(default_factory=list)
    warnings: list[str] = field(default_factory=list)
    suggestions: list[str] = field(default_factory=list)
    validated_at: datetime = field(default_factory=datetime.now)


class SecurityPattern:
    """Security vulnerability patterns (å®‰å…¨æ¼æ´æ¨¡å¼)"""
    
    # æ˜æ–‡å¯†ç¢¼å­˜å„²æ¨¡å¼
    PLAINTEXT_PASSWORD = [
        r'password\s*[:=]\s*["\'][^"\']+["\']',
        r'password\s*[:=]\s*data\.password',
        r'password\s*[:=]\s*\w+\.password(?!\s*\.\s*hash)',
    ]
    
    # SQL æ³¨å…¥é¢¨éšªæ¨¡å¼
    SQL_INJECTION = [
        r'execute\s*\(\s*["\'].*\$\{',
        r'query\s*\(\s*["\'].*\+\s*\w+',
        r'raw\s*\(\s*["\'].*\+',
    ]
    
    # ç¼ºå°‘è¼¸å…¥é©—è­‰
    MISSING_VALIDATION = [
        r'async\s+\w+\s*\([^)]*data[^)]*\)\s*\{[^}]*(?!validate|check|verify)',
    ]
    
    # ç¼ºå°‘éŒ¯èª¤è™•ç†
    MISSING_ERROR_HANDLING = [
        r'await\s+\w+\.[^;]+(?!\s*\.catch|\s*try)',
    ]
    
    # æ•æ„Ÿæ•¸æ“šæš´éœ²
    SENSITIVE_DATA_EXPOSURE = [
        r'console\.log\s*\([^)]*password',
        r'console\.log\s*\([^)]*secret',
        r'console\.log\s*\([^)]*token',
    ]


class HallucinationDetector:
    """
    AI Hallucination Detector (AI å¹»è¦ºæª¢æ¸¬å™¨)
    
    æ ¸å¿ƒåŸå‰‡ï¼š
    1. ä¸ä¿¡ä»» AI çš„ä»»ä½•è¼¸å‡ºï¼Œå…¨éƒ¨éœ€è¦é©—è­‰
    2. å¤šå±¤æª¢æ¸¬ï¼Œå¾èªæ³•åˆ°èªç¾©åˆ°å®‰å…¨
    3. æä¾›ä¿®å¾©å»ºè­°ï¼Œè€Œéåƒ…æŒ‡å‡ºå•é¡Œ
    
    ç ”ç©¶é¡¯ç¤ºï¼šç´„ 50% çš„ AI ç”Ÿæˆä»£ç¢¼å¯©æŸ¥åŒ…å«å¹»è¦º
    """
    
    def __init__(self) -> None:
        """Initialize the detector with in-memory state.

        The constructor prepares all bookkeeping structures used across the
        detection lifecycle so that downstream methods can focus purely on
        analysis. State tracked includes:

        - detection history for auditability and post-hoc analysis
        - a lookup index for detection identifiers to enable feedback mapping
        - custom validators registered by callers for domain-specific checks
        - a false-positive cache to suppress repeated noise from known code
        - monotonically increasing detection identifiers for traceability
        - statistics that summarize validations and severities for reporting
        - feedback summaries to understand false-positive clusters and trend lines
        """

        self._detection_history: list[HallucinationDetection] = []
        self._detection_index: dict[str, HallucinationDetection] = {}
        self._custom_validators: list[Callable[[str], list[HallucinationDetection]]] = []
        self._false_positive_hashes: set[str] = set()
        self._detection_count = 0

        # çµ±è¨ˆæ•¸æ“š
        self._stats = {
            "total_validations": 0,
            "total_hallucinations": 0,
            "by_type": {},
            "by_severity": {},
            "by_pattern": {},
            "false_positive_marks": 0,
        }

        # äººé¡å›é¥‹è¨˜éŒ„ï¼Œç”¨æ–¼è‡ªå‹•æ¼”åŒ–èˆ‡èª¤å ±æŠ‘åˆ¶
        self._feedback_log: list[dict[str, Any]] = []
        self._feedback_stats: dict[str, Any] = {
            "total": 0,
            "by_verdict": {},
            "by_pattern": {},
            "by_type": {},
        }
    
    def validate_code(self, code: str, language: str = "python") -> ValidationResult:
        """Validate AI-generated code for hallucinations.

        The validator runs layered checks ranging from regular-expression
        heuristics to caller-supplied validators, filters known false
        positives, and produces both a structured list of detections and
        actionable suggestions. This is the primary API entry point for the
        detector.

        Args:
            code: Source code produced by an AI system or other generator.
            language: Programming language hint used by some heuristics
                (currently tuned for Python defaults).

        Returns:
            ValidationResult capturing validity, detections, warnings,
            suggestions, and the computed overall confidence score.
        """
        hallucinations: list[HallucinationDetection] = []
        warnings: list[str] = []
        suggestions: list[str] = []
        
        # 1. å®‰å…¨æ¼æ´æª¢æ¸¬
        security_issues = self._detect_security_flaws(code)
        hallucinations.extend(security_issues)
        
        # 2. é‚è¼¯éŒ¯èª¤æª¢æ¸¬
        logic_issues = self._detect_logic_errors(code, language)
        hallucinations.extend(logic_issues)
        
        # 3. ä¸å®Œæ•´å¯¦ç¾æª¢æ¸¬
        incomplete_issues = self._detect_incomplete_implementation(code)
        hallucinations.extend(incomplete_issues)
        
        # 4. éåº¦è‡ªä¿¡æª¢æ¸¬
        overconfidence_issues = self._detect_overconfidence(code)
        hallucinations.extend(overconfidence_issues)
        
        # 5. é‹è¡Œè‡ªå®šç¾©é©—è­‰å™¨
        for validator in self._custom_validators:
            try:
                custom_issues = validator(code)
                hallucinations.extend(custom_issues)
            except Exception:
                warnings.append("Custom validator failed")
        
        # éæ¿¾å·²æ¨™è¨˜ç‚ºèª¤å ±çš„æª¢æ¸¬
        hallucinations = self._filter_false_positives(hallucinations, code)
        
        # è¨ˆç®—æ•´é«”ç½®ä¿¡åº¦
        overall_confidence = self._calculate_confidence(hallucinations)
        
        # ç”Ÿæˆå»ºè­°
        suggestions = self._generate_suggestions(hallucinations)
        
        # æ›´æ–°çµ±è¨ˆ
        self._update_stats(hallucinations)

        # è¨˜éŒ„æ­·å²
        self._record_detections(hallucinations)
        
        return ValidationResult(
            is_valid=len([h for h in hallucinations if h.severity in [SeverityLevel.CRITICAL, SeverityLevel.HIGH]]) == 0,
            overall_confidence=overall_confidence,
            hallucinations=hallucinations,
            warnings=warnings,
            suggestions=suggestions,
        )
    
    def _detect_security_flaws(self, code: str) -> list[HallucinationDetection]:
        """Detect security vulnerabilities (æª¢æ¸¬å®‰å…¨æ¼æ´).

        This helper scans for common vulnerability signatures, including
        plaintext credential handling, SQL injection patterns, and logging of
        secrets. Matches are converted into strongly typed detection objects so
        that callers can trace the precise pattern and suggested remediation.

        Args:
            code: Raw code text to be inspected.

        Returns:
            A list of security-focused :class:`HallucinationDetection` entries.
        """
        detections: list[HallucinationDetection] = []
        
        # æª¢æ¸¬æ˜æ–‡å¯†ç¢¼
        for pattern in SecurityPattern.PLAINTEXT_PASSWORD:
            matches = re.finditer(pattern, code, re.IGNORECASE)
            for match in matches:
                self._detection_count += 1
                detections.append(HallucinationDetection(
                    detection_id=f"SEC-{self._detection_count:06d}",
                    hallucination_type=HallucinationType.SECURITY_FLAW,
                    severity=SeverityLevel.CRITICAL,
                    description="Potential plaintext password storage detected (å¯èƒ½çš„æ˜æ–‡å¯†ç¢¼å­˜å„²)",
                    location=f"Line containing: {match.group()[:50]}...",
                    suggested_fix="Use bcrypt or argon2 for password hashing",
                    confidence=0.85,
                    metadata={"pattern": "PLAINTEXT_PASSWORD"},
                ))
        
        # æª¢æ¸¬ SQL æ³¨å…¥
        for pattern in SecurityPattern.SQL_INJECTION:
            matches = re.finditer(pattern, code, re.IGNORECASE)
            for match in matches:
                self._detection_count += 1
                detections.append(HallucinationDetection(
                    detection_id=f"SEC-{self._detection_count:06d}",
                    hallucination_type=HallucinationType.SECURITY_FLAW,
                    severity=SeverityLevel.CRITICAL,
                    description="Potential SQL injection vulnerability (å¯èƒ½çš„ SQL æ³¨å…¥æ¼æ´)",
                    location=f"Line containing: {match.group()[:50]}...",
                    suggested_fix="Use parameterized queries or ORM",
                    confidence=0.80,
                    metadata={"pattern": "SQL_INJECTION"},
                ))
        
        # æª¢æ¸¬æ•æ„Ÿæ•¸æ“šæš´éœ²
        for pattern in SecurityPattern.SENSITIVE_DATA_EXPOSURE:
            matches = re.finditer(pattern, code, re.IGNORECASE)
            for match in matches:
                self._detection_count += 1
                detections.append(HallucinationDetection(
                    detection_id=f"SEC-{self._detection_count:06d}",
                    hallucination_type=HallucinationType.SECURITY_FLAW,
                    severity=SeverityLevel.HIGH,
                    description="Sensitive data may be exposed in logs (æ•æ„Ÿæ•¸æ“šå¯èƒ½åœ¨æ—¥èªŒä¸­æš´éœ²)",
                    location=f"Line containing: {match.group()[:50]}...",
                    suggested_fix="Remove sensitive data from logs or use redaction",
                    confidence=0.75,
                    metadata={"pattern": "SENSITIVE_DATA_EXPOSURE"},
                ))
        
        return detections
    
    def _detect_logic_errors(self, code: str, language: str) -> list[HallucinationDetection]:
        """Detect logic errors (æª¢æ¸¬é‚è¼¯éŒ¯èª¤).

        Looks for implementation smells that often slip past superficial
        reviews, such as empty catch blocks, potential infinite loops, and
        unused variable assignments. Python-specific heuristics are applied
        only when the ``language`` hint is set accordingly.

        Args:
            code: The candidate code to assess.
            language: Programming language context that gates some heuristics.

        Returns:
            A collection of logic-oriented detections with severity and fixes.
        """
        detections: list[HallucinationDetection] = []
        
        # æª¢æ¸¬ç©º catch å¡Š
        empty_catch_pattern = r'catch\s*\([^)]*\)\s*\{\s*\}'
        matches = re.finditer(empty_catch_pattern, code)
        for match in matches:
            self._detection_count += 1
            detections.append(HallucinationDetection(
                detection_id=f"LOG-{self._detection_count:06d}",
                hallucination_type=HallucinationType.LOGIC_ERROR,
                severity=SeverityLevel.MEDIUM,
                description="Empty catch block swallows errors (ç©ºçš„ catch å¡Šåå™¬éŒ¯èª¤)",
                location=f"Line containing: {match.group()[:50]}...",
                suggested_fix="Log the error or handle it appropriately",
                confidence=0.90,
            ))
        
        # æª¢æ¸¬ç„¡é™å¾ªç’°é¢¨éšª
        infinite_loop_patterns = [
            r'while\s*\(\s*true\s*\)\s*\{(?![^}]*break)',
            r'for\s*\(\s*;\s*;\s*\)\s*\{(?![^}]*break)',
        ]
        for pattern in infinite_loop_patterns:
            matches = re.finditer(pattern, code)
            for match in matches:
                self._detection_count += 1
                detections.append(HallucinationDetection(
                    detection_id=f"LOG-{self._detection_count:06d}",
                    hallucination_type=HallucinationType.LOGIC_ERROR,
                    severity=SeverityLevel.HIGH,
                    description="Potential infinite loop without break condition (å¯èƒ½çš„ç„¡é™å¾ªç’°)",
                    location=f"Line containing: {match.group()[:50]}...",
                    suggested_fix="Add a break condition or timeout",
                    confidence=0.70,
                ))
        
        # æª¢æ¸¬æœªä½¿ç”¨çš„è®Šé‡ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        if language == "python":
            # æŸ¥æ‰¾è³¦å€¼ä½†æœªä½¿ç”¨çš„è®Šé‡
            assignments = re.findall(r'^(\s*)(\w+)\s*=\s*.+$', code, re.MULTILINE)
            for _, var_name in assignments:
                # æ’é™¤å¸¸è¦‹çš„ç‰¹æ®Šè®Šé‡
                if var_name.startswith('_') or var_name in ['self', 'cls']:
                    continue
                # æª¢æŸ¥è®Šé‡æ˜¯å¦åœ¨å¾ŒçºŒä»£ç¢¼ä¸­ä½¿ç”¨
                usage_pattern = rf'\b{re.escape(var_name)}\b'
                usages = re.findall(usage_pattern, code)
                if len(usages) == 1:  # åªæœ‰ä¸€æ¬¡å‡ºç¾ï¼ˆè³¦å€¼æœ¬èº«ï¼‰
                    self._detection_count += 1
                    detections.append(HallucinationDetection(
                        detection_id=f"LOG-{self._detection_count:06d}",
                        hallucination_type=HallucinationType.LOGIC_ERROR,
                        severity=SeverityLevel.LOW,
                        description=f"Variable '{var_name}' may be unused (è®Šé‡å¯èƒ½æœªä½¿ç”¨)",
                        suggested_fix="Remove or use the variable",
                        confidence=0.60,
                    ))
        
        return detections
    
    def _detect_incomplete_implementation(self, code: str) -> list[HallucinationDetection]:
        """Detect incomplete implementations (æª¢æ¸¬ä¸å®Œæ•´å¯¦ç¾).

        Flags placeholders that indicate unfinished work, including TODO/FIXME
        markers and deliberate ``NotImplementedError`` stubs. The resulting
        detections enable downstream tooling to block promotion of code that
        still contains scaffolding.

        Args:
            code: Source text that may include unfinished fragments.

        Returns:
            Detected incompleteness signals annotated with guidance.
        """
        detections: list[HallucinationDetection] = []
        
        # æª¢æ¸¬ TODO/FIXME è¨»é‡‹
        todo_pattern = r'#\s*(TODO|FIXME|XXX|HACK)\s*:?\s*(.+)'
        matches = re.finditer(todo_pattern, code, re.IGNORECASE)
        for match in matches:
            self._detection_count += 1
            detections.append(HallucinationDetection(
                detection_id=f"INC-{self._detection_count:06d}",
                hallucination_type=HallucinationType.INCOMPLETE,
                severity=SeverityLevel.MEDIUM,
                description=f"Incomplete implementation: {match.group(2)[:50]}",
                location=f"Line containing: {match.group()[:50]}...",
                suggested_fix="Complete the implementation before deployment",
                confidence=0.95,
            ))
        
        # æª¢æ¸¬ pass èªå¥ï¼ˆå¯èƒ½æ˜¯ä½”ä½ç¬¦ï¼‰
        pass_pattern = r'def\s+\w+\s*\([^)]*\)\s*:\s*\n\s*pass'
        matches = re.finditer(pass_pattern, code)
        for match in matches:
            self._detection_count += 1
            detections.append(HallucinationDetection(
                detection_id=f"INC-{self._detection_count:06d}",
                hallucination_type=HallucinationType.INCOMPLETE,
                severity=SeverityLevel.HIGH,
                description="Function with only 'pass' statement (åƒ…æœ‰ pass çš„å‡½æ•¸)",
                location=f"Line containing: {match.group()[:50]}...",
                suggested_fix="Implement the function body",
                confidence=0.85,
            ))
        
        # æª¢æ¸¬ NotImplementedError
        not_impl_pattern = r'raise\s+NotImplementedError'
        matches = re.finditer(not_impl_pattern, code)
        for match in matches:
            self._detection_count += 1
            detections.append(HallucinationDetection(
                detection_id=f"INC-{self._detection_count:06d}",
                hallucination_type=HallucinationType.INCOMPLETE,
                severity=SeverityLevel.HIGH,
                description="NotImplementedError indicates incomplete code",
                location=f"Line containing: {match.group()[:50]}...",
                suggested_fix="Implement the required functionality",
                confidence=0.90,
            ))
        
        return detections
    
    def _detect_overconfidence(self, code: str) -> list[HallucinationDetection]:
        """Detect overconfident claims in comments (æª¢æ¸¬éåº¦è‡ªä¿¡çš„è²æ˜).

        Searches for unverified assertions about security, completeness, or
        testing rigor. These markers often mask missing evidence and should be
        challenged before acceptance.

        Args:
            code: Code and associated comments to analyze.

        Returns:
            Detections indicating overconfident statements and suggested next
            steps to validate them.
        """
        detections: list[HallucinationDetection] = []
        
        # éåº¦è‡ªä¿¡çš„è¨»é‡‹æ¨¡å¼
        overconfident_patterns = [
            (r'#\s*(This is|This code is)\s*(secure|safe|perfect|complete|optimal)', "Security/quality claim"),
            (r'#\s*(Fully|Completely)\s*(tested|validated|verified)', "Testing claim"),
            (r'#\s*(No\s+)?(bugs?|errors?|issues?)\s*(here|in this)', "Bug-free claim"),
            (r'âœ….*å®Œæˆ.*å®‰å…¨', "Completion and security claim"),
        ]
        
        for pattern, claim_type in overconfident_patterns:
            matches = re.finditer(pattern, code, re.IGNORECASE)
            for match in matches:
                self._detection_count += 1
                detections.append(HallucinationDetection(
                    detection_id=f"OVR-{self._detection_count:06d}",
                    hallucination_type=HallucinationType.OVERCONFIDENCE,
                    severity=SeverityLevel.MEDIUM,
                    description=f"Overconfident {claim_type} without verification (æœªç¶“é©—è­‰çš„éåº¦è‡ªä¿¡è²æ˜)",
                    location=f"Line containing: {match.group()[:50]}...",
                    suggested_fix="Remove or verify the claim with tests",
                    confidence=0.80,
                ))
        
        return detections
    
    def _filter_false_positives(
        self,
        detections: list[HallucinationDetection],
        code: str
    ) -> list[HallucinationDetection]:
        """Filter out known false positives (éæ¿¾å·²çŸ¥çš„èª¤å ±).

        Uses a content hash combined with the detection identifier to
        de-duplicate issues that the user has explicitly suppressed for the
        given code snapshot.

        Args:
            detections: Candidate detections to evaluate.
            code: The exact code string associated with the detections.

        Returns:
            A filtered list excluding detections previously marked as noise.
        """
        code_hash = hashlib.sha256(code.encode()).hexdigest()
        return [
            d for d in detections 
            if f"{code_hash}:{d.detection_id}" not in self._false_positive_hashes
        ]
    
    def mark_false_positive(self, code: str, detection_id: str) -> None:
        """Mark a detection as false positive (æ¨™è¨˜ç‚ºèª¤å ±).

        Persists a hash-based fingerprint so subsequent validation passes ignore
        the same detection on identical code, preventing repetitive alerts.

        Args:
            code: The code sample that produced the detection.
            detection_id: Identifier of the detection to silence.
        """
        code_hash = hashlib.sha256(code.encode()).hexdigest()
        self._false_positive_hashes.add(f"{code_hash}:{detection_id}")
    
    def _calculate_confidence(self, detections: list[HallucinationDetection]) -> float:
        """Calculate overall confidence score (è¨ˆç®—æ•´é«”ç½®ä¿¡åº¦).

        Applies weighted penalties based on severity to derive an aggregate
        confidence value between 0 and 1, where 1 represents no detected
        hallucinations.

        Args:
            detections: Detections generated for a single validation run.

        Returns:
            Normalized confidence score reflecting remaining risk.
        """
        if not detections:
            return 1.0
        
        # æ ¹æ“šæª¢æ¸¬çµæœè¨ˆç®—ç½®ä¿¡åº¦
        severity_weights = {
            SeverityLevel.CRITICAL: 0.4,
            SeverityLevel.HIGH: 0.25,
            SeverityLevel.MEDIUM: 0.15,
            SeverityLevel.LOW: 0.08,
            SeverityLevel.INFO: 0.02,
        }
        
        total_penalty = sum(
            severity_weights.get(d.severity, 0.1) * d.confidence
            for d in detections
        )

        return max(0.0, 1.0 - min(total_penalty, 1.0))

    def _record_detections(self, detections: list[HallucinationDetection]) -> None:
        """Persist detections to history and lookup index (ä¿å­˜æª¢æ¸¬ç´€éŒ„èˆ‡ç´¢å¼•)."""

        self._detection_history.extend(detections)
        for detection in detections:
            self._detection_index[detection.detection_id] = detection
    
    def _generate_suggestions(self, detections: list[HallucinationDetection]) -> list[str]:
        """Generate improvement suggestions (ç”Ÿæˆæ”¹é€²å»ºè­°).

        Aggregates detections by severity and type to create concise,
        prioritized remediation hints that can be shown to engineers or fed
        into automated workflows.

        Args:
            detections: Detections to summarize.

        Returns:
            Ordered list of human-readable suggestions.
        """
        suggestions = []
        
        # æŒ‰åš´é‡ç¨‹åº¦åˆ†çµ„
        critical = [d for d in detections if d.severity == SeverityLevel.CRITICAL]
        high = [d for d in detections if d.severity == SeverityLevel.HIGH]
        
        if critical:
            suggestions.append(f"âš ï¸ {len(critical)} critical issues require immediate attention")
        if high:
            suggestions.append(f"ğŸ”´ {len(high)} high-priority issues should be addressed soon")
        
        # æŒ‰é¡å‹æä¾›å»ºè­°
        security_issues = [d for d in detections if d.hallucination_type == HallucinationType.SECURITY_FLAW]
        if security_issues:
            suggestions.append("ğŸ”’ Security review recommended before deployment")
        
        incomplete_issues = [d for d in detections if d.hallucination_type == HallucinationType.INCOMPLETE]
        if incomplete_issues:
            suggestions.append("ğŸ“ Complete all TODO items and placeholder implementations")
        
        return suggestions
    
    def _update_stats(self, detections: list[HallucinationDetection]) -> None:
        """Update detection statistics (æ›´æ–°æª¢æ¸¬çµ±è¨ˆ).

        Maintains aggregate counts across validations to support reporting and
        trend analysis without exposing mutable internal state to callers.

        Args:
            detections: New detections from the current validation pass.
        """
        self._stats["total_validations"] += 1
        self._stats["total_hallucinations"] += len(detections)

        for d in detections:
            type_key = d.hallucination_type.value
            self._stats["by_type"][type_key] = self._stats["by_type"].get(type_key, 0) + 1

            severity_key = d.severity.value
            self._stats["by_severity"][severity_key] = self._stats["by_severity"].get(severity_key, 0) + 1

            pattern_key = d.metadata.get("pattern") if isinstance(d.metadata, dict) else None
            if pattern_key:
                self._stats["by_pattern"][pattern_key] = self._stats["by_pattern"].get(pattern_key, 0) + 1
    
    def register_custom_validator(
        self, 
        validator: Callable[[str], list[HallucinationDetection]]
    ) -> None:
        """Register a custom validation function (è¨»å†Šè‡ªå®šç¾©é©—è­‰å™¨).

        Custom validators allow domain teams to enforce additional rules (for
        example, framework conventions) while reusing the detector's lifecycle
        management and reporting pipeline.

        Args:
            validator: Callable returning detections for the provided code.
        """
        self._custom_validators.append(validator)
    
    def get_statistics(self) -> dict[str, Any]:
        """Get detection statistics (ç²å–æª¢æ¸¬çµ±è¨ˆ).

        Returns a shallow copy to prevent external mutation of the collector's
        internal counters.
        """
        return self._stats.copy()
    
    def get_detection_history(self) -> list[HallucinationDetection]:
        """Get detection history (ç²å–æª¢æ¸¬æ­·å²).

        Provides an immutable snapshot of all detections seen so far to support
        audit logging or user-visible history views.
        """
        return self._detection_history.copy()

    def ingest_feedback(
        self,
        code: str,
        detection_id: str,
        verdict: str,
        notes: Optional[str] = None,
    ) -> None:
        """Capture human feedback to evolve detector behavior (æ¥æ”¶äººé¡å›é¥‹ä¸¦è‡ªå‹•æ¼”åŒ–).

        This method enables a lightweight reinforcement loop:

        - ``false_positive`` verdicts silence the referenced detection for the
          provided code hash and increment a suppression counter.
        - ``true_positive`` verdicts are logged to help prioritize heuristics
          that commonly surface real issues.
        - ``improvement_applied`` verdicts note that remediation actions were
          taken, allowing downstream dashboards to track closure rates.

        Args:
            code: Exact code sample that produced the detection.
            detection_id: Identifier of the detection being reviewed.
            verdict: One of ``"false_positive"``, ``"true_positive"``, or
                ``"improvement_applied"``.
            notes: Optional analyst notes or remediation details.
        """
        timestamp = datetime.now().isoformat()
        code_hash = hashlib.sha256(code.encode()).hexdigest()

        if verdict == "false_positive":
            self._false_positive_hashes.add(f"{code_hash}:{detection_id}")
            self._stats["false_positive_marks"] += 1

        detection = self._find_detection(detection_id)
        pattern_key = detection.metadata.get("pattern") if detection and isinstance(detection.metadata, dict) else None
        type_key = detection.hallucination_type.value if detection else None

        self._feedback_stats["total"] += 1
        self._feedback_stats["by_verdict"][verdict] = self._feedback_stats["by_verdict"].get(verdict, 0) + 1

        if pattern_key:
            bucket = self._feedback_stats["by_pattern"].setdefault(pattern_key, {"false_positive": 0, "true_positive": 0, "improvement_applied": 0})
            if verdict in bucket:
                bucket[verdict] += 1

        if type_key:
            bucket = self._feedback_stats["by_type"].setdefault(type_key, {"false_positive": 0, "true_positive": 0, "improvement_applied": 0})
            if verdict in bucket:
                bucket[verdict] += 1

        self._feedback_log.append(
            {
                "code_hash": code_hash,
                "detection_id": detection_id,
                "verdict": verdict,
                "notes": notes,
                "timestamp": timestamp,
            }
        )

    def get_feedback_log(self) -> list[dict[str, Any]]:
        """Return accumulated feedback entries (å›å‚³ç´¯ç©å›é¥‹è¨˜éŒ„)."""
        return self._feedback_log.copy()

    def get_feedback_summary(self) -> dict[str, Any]:
        """Aggregate feedback signals for dashboards (å½™ç¸½å›é¥‹è¨Šè™Ÿä¾›çœ‹æ¿ä½¿ç”¨)."""

        return {
            "total": self._feedback_stats.get("total", 0),
            "by_verdict": self._feedback_stats.get("by_verdict", {}).copy(),
            "by_pattern": {k: v.copy() for k, v in self._feedback_stats.get("by_pattern", {}).items()},
            "by_type": {k: v.copy() for k, v in self._feedback_stats.get("by_type", {}).items()},
        }

    def get_adaptive_recommendations(self, min_occurrences: int = 3) -> list[str]:
        """Suggest evolutions to detection rules (å»ºè­°è‡ªå‹•æ¼”åŒ–æ–¹å‘).

        Reviews aggregated statistics and human feedback to highlight areas
        where heuristics should be strengthened or relaxed.

        Args:
            min_occurrences: Minimum pattern count to be considered "trending".

        Returns:
            A list of human-readable recommendations suitable for backlog items
            or rule updates.
        """
        recommendations: list[str] = []

        for pattern, count in self._stats.get("by_pattern", {}).items():
            if count >= min_occurrences:
                recommendations.append(
                    f"Pattern '{pattern}' triggered {count} times; consider a dedicated rule or test."
                )

        for pattern, feedback in self._feedback_stats.get("by_pattern", {}).items():
            false_pos = feedback.get("false_positive", 0)
            true_pos = feedback.get("true_positive", 0)
            improvements = feedback.get("improvement_applied", 0)

            if false_pos >= min_occurrences and false_pos > true_pos:
                recommendations.append(
                    f"Pattern '{pattern}' draws frequent false positives ({false_pos}); relax regex or downgrade severity."
                )
            if true_pos >= min_occurrences and true_pos > false_pos:
                recommendations.append(
                    f"Pattern '{pattern}' confirmed {true_pos} times; strengthen detection with tests or linters."
                )
            if improvements > 0:
                recommendations.append(
                    f"Pattern '{pattern}' prompted {improvements} remediations; add regression checks to prevent recurrence."
                )

        for type_key, feedback in self._feedback_stats.get("by_type", {}).items():
            if feedback.get("false_positive", 0) >= min_occurrences and feedback.get("false_positive", 0) > feedback.get("true_positive", 0):
                recommendations.append(
                    f"Hallucination type '{type_key}' has elevated false positives; recalibrate detection thresholds."
                )
            if feedback.get("true_positive", 0) >= min_occurrences and feedback.get("true_positive", 0) > feedback.get("false_positive", 0):
                recommendations.append(
                    f"Hallucination type '{type_key}' consistently surfaces real issues; prioritize rule hardening."
                )

        if self._stats.get("false_positive_marks", 0) >= min_occurrences:
            recommendations.append(
                "High false-positive volume detected; review regexes or lower severities for noisy rules."
            )

        if not recommendations:
            recommendations.append("No adaptive actions required; keep monitoring usage signals.")

        return recommendations

    def _find_detection(self, detection_id: str) -> Optional[HallucinationDetection]:
        """Retrieve a detection by id from history (å¾æ­·å²ä¸­æŸ¥æ‰¾æª¢æ¸¬è¨˜éŒ„)."""

        return self._detection_index.get(detection_id)
